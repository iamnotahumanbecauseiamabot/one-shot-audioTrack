{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SiameseVGGish1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "jiZbNWpg-iZX",
        "outputId": "716ffa94-c039-4ef6-c653-f683c32e3458",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3b8e316d-4ab3-4bc6-b506-4cb5896bad8c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3b8e316d-4ab3-4bc6-b506-4cb5896bad8c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving speaker1.mp3 to speaker1.mp3\n",
            "Saving speaker2.mp3 to speaker2.mp3\n",
            "Saving speaker3.mp3 to speaker3.mp3\n",
            "Saving speaker4.mp3 to speaker4.mp3\n",
            "Saving speaker5.mp3 to speaker5.mp3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-Yh25c-w-yZO",
        "outputId": "8eaf21fc-0986-43b9-b7cd-11cf19d6bfe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!pip install soundfile"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.6/dist-packages (0.23.1)\n",
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/68/64/1191352221e2ec90db7492b4bf0c04fd9d2508de67b3f39cbf093cd6bd86/SoundFile-0.10.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.19)\n",
            "Installing collected packages: soundfile\n",
            "Successfully installed soundfile-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pboGkUhm-fqU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import subprocess\n",
        "from pydub import silence\n",
        "from pydub import AudioSegment\n",
        "from wave import open as open_wave\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sparse\n",
        "import librosa\n",
        "from librosa import display\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "import time\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout\n",
        "from keras.models import Model\n",
        "import seaborn as sns\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ctiGkuzaI84",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ec3KJsBql5h9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "audio_length_min = 10\n",
        "classes = 5\n",
        "total_length = audio_length_min * 60000\n",
        "segment_length= 5000\n",
        "segments = total_length/segment_length\n",
        "segments = int(segments)\n",
        "n_mels = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RjDvKordAsvZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!mkdir wavspeaker\n",
        "!mkdir dataset\n",
        "for i in range(0,classes):\n",
        "  os.mkdir(\"dataset/s{}\".format(i+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UX76CF9i-fqd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(0,classes):\n",
        "  subprocess.call(['ffmpeg', '-i', './speaker{}.mp3'.format(i+1),\n",
        "                   './wavspeaker/speaker{}.wav'.format(i+1)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4N4U8aBH-fqv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(0,classes):\n",
        "    t1 = 0\n",
        "    newAudio = AudioSegment.from_wav('./wavspeaker/speaker{}.wav'.format(i+1))\n",
        "    for j in range(0,segments): \n",
        "        newAudio1 = newAudio[t1:(t1+segment_length)]\n",
        "        newAudio1.export('./dataset/s{}/{}.wav'.format(i+1, j), format=\"wav\")\n",
        "        t1 = t1 + segment_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SeeIY8lq4lCy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_audio_mel(audio, sample_rate=16000, window_size=25,step_size=10, eps=1e-10):\n",
        "  \n",
        "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels= n_mels)\n",
        "    mel_db = (librosa.power_to_db(mel_spec, ref=np.max))\n",
        "    return mel_db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RROdLgjO-fqy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_make(file_path):\n",
        "    dataset = glob.glob(file_path)\n",
        "    data1 = []\n",
        "    for i in range(0,segments):\n",
        "        y, sr = librosa.load(dataset[i], sr=16000)\n",
        "        #y=y.astype(np.float64)\n",
        "        #y=y/np.abs(np.max(y))\n",
        "        y = preprocess_audio_mel(audio = y)\n",
        "        #y=np.reshape(y, (250,320))\n",
        "        data1.append(y)\n",
        "    return data1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4jvCW5vpjN0N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data1 = data_make('./dataset/s1/*.wav')\n",
        "data2 = data_make('./dataset/s2/*.wav')\n",
        "data3 = data_make('./dataset/s3/*.wav')\n",
        "data4 = data_make('./dataset/s4/*.wav')\n",
        "data5 = data_make('./dataset/s5/*.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZYyfc5A0QZkA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data1 = np.array(data1)\n",
        "data2 = np.array(data2)\n",
        "data3 = np.array(data3)\n",
        "data4 = np.array(data4)\n",
        "data5 = np.array(data5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JOqAe6FS7jB1",
        "colab_type": "code",
        "outputId": "c6fe651a-0300-46c6-ff4a-b34284f304ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "display.specshow(data4[1],y_axis='mel', fmax=8000,x_axis='time')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Mel spectrogram')\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApkAAAEYCAYAAAAXq+2yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_Pkp7jETopG0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "odo = AudioSegment.from_wav('./dataset/s1/0.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNKXcKh0plct",
        "colab_type": "code",
        "outputId": "55e1c9e1-08d1-46df-ca31-ecb5a97beb08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "p = silence.detect_silence(odo, min_silence_len=100)\n",
        "p"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 3000]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "urCP0s38QvDp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data1 = np.expand_dims(data1, axis=3)\n",
        "data2 = np.expand_dims(data2, axis=3)\n",
        "data3 = np.expand_dims(data3, axis=3)\n",
        "data4 = np.expand_dims(data4, axis=3)\n",
        "data5 = np.expand_dims(data5, axis=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DmI2b2WuRJ0S",
        "outputId": "4f9c33e8-7eeb-4a4a-de89-6ff109723a1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data1.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 300, 300, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 255
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1YFMry_gRJ4f",
        "outputId": "14554e10-42a4-4526-8640-23886430bab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data1[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 94, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1b51ay-IHYg_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pair_make(item, datacat1, datacat2):\n",
        "    pair = []\n",
        "    pair1 = []\n",
        "    for i in range(item):\n",
        "        indx = np.random.randint(0, 100)\n",
        "        indx1 = np.random.randint(0, 100)\n",
        "        pair.append(datacat1[indx])\n",
        "        pair1.append(datacat2[indx1])\n",
        "    return pair, pair1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7x2QFqDxxyYQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = [data1, data2, data3, data4, data5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "InlxSejLrX2t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iWsnMxRxsZWg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pair = []\n",
        "cor_pair = []\n",
        "for i in range(0, classes):\n",
        "  pair1 = []\n",
        "  pair11 = []\n",
        "  pair1, pair11 = pair_make(50, data[i],data[i])\n",
        "  pair = pair + pair1\n",
        "  cor_pair = cor_pair + pair11"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srL-eAFquqUw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "paird = []\n",
        "diff_paird = []\n",
        "for i in range(0,classes):\n",
        "    for j in range(0, classes):\n",
        "        pair1 = []\n",
        "        pair11 = []\n",
        "        if(j<i):\n",
        "            pair1, pair11 = pair_make(50, data[i],data[j])\n",
        "            paird = paird + pair1\n",
        "            diff_paird = diff_paird + pair11"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SbqWfZzz2jSx",
        "colab_type": "code",
        "outputId": "9d516220-51ce-4c21-b01d-cb0c7b27f130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(pair)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "metadata": {
        "id": "tssCMLiF0WYE",
        "colab_type": "code",
        "outputId": "55ffd4f7-dffb-466b-c040-1335f68e8b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(paird)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "O1VQWkShHYl3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label1 = [1]*150\n",
        "label0 = [0]*150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VInHr5AZTQBD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lside = pair + paird\n",
        "rside = cor_pair + diff_paird\n",
        "labels = label1 + label0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EOuYfpZU1KNj",
        "colab_type": "code",
        "outputId": "63611e97-f4bf-4d2f-cb36-17f1872451d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(rside)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "64MzvOxEVNAN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ji38BQIYY85J",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_l = np.array(lside)\n",
        "X_r = np.array(rside)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aYc1oAuga0pB",
        "outputId": "cee2d12c-8fb1-4c08-82f6-40fdc022cb32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_r.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 128, 94, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7CyG0WC_-fro",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_siamese_model(input_shape):\n",
        "    \n",
        "    left_input = Input(input_shape)\n",
        "    right_input = Input(input_shape)\n",
        "    model = Sequential()\n",
        "    BATCH_NORM = 1\n",
        "    model.add(Conv2D(8, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', input_shape=(128,94,1), name='block1_conv1'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(8, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block1_conv2'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block2_conv1'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block2_conv2'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block3_conv1'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block3_conv2'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block3_conv3'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block3_conv4'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block4_conv1'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block4_conv2'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block4_conv3'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block4_conv4'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block5_conv1'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', name='block5_conv2'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', name='block5_conv3'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(256, (3, 3), padding='same', name='block5_conv4'))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(2048))\n",
        "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(2048, name='fc2'))\n",
        "    \n",
        "    encoded_l = model(left_input)\n",
        "    encoded_r = model(right_input)\n",
        "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
        "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "    return siamese_net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7EgTK8z3-frr",
        "outputId": "cf24dfdf-14f1-4169-920a-39831355e84d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "model = get_siamese_model((128,94,1))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_15 (InputLayer)           (None, 128, 94, 1)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_16 (InputLayer)           (None, 128, 94, 1)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_8 (Sequential)       (None, 2048)         30173528    input_15[0][0]                   \n",
            "                                                                 input_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 2048)         0           sequential_8[1][0]               \n",
            "                                                                 sequential_8[2][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 1)            2049        lambda_8[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 30,175,577\n",
            "Trainable params: 30,166,073\n",
            "Non-trainable params: 9,504\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DLjubBvJ-frw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = Adam(lr = 0.00006)\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "g0b2arIe-fry",
        "outputId": "8491a1ec-0ae2-4768-991f-befa36823849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88073
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the discriminator\n",
        "d_loss = model.fit([X_l, X_r],labels,validation_split=0.33,nb_epoch=3000,verbose=1,shuffle=True)\n",
        "\n",
        "print( d_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 200 samples, validate on 100 samples\n",
            "Epoch 1/3000\n",
            "200/200 [==============================] - 24s 122ms/step - loss: 18.2190 - acc: 0.6000 - val_loss: 18.8661 - val_acc: 0.1000\n",
            "Epoch 2/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 18.0041 - acc: 0.7750 - val_loss: 18.9795 - val_acc: 0.0800\n",
            "Epoch 3/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.8743 - acc: 0.8300 - val_loss: 19.1528 - val_acc: 0.0300\n",
            "Epoch 4/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.7820 - acc: 0.8850 - val_loss: 19.2091 - val_acc: 0.0600\n",
            "Epoch 5/3000\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 17.7363 - acc: 0.9150 - val_loss: 19.3111 - val_acc: 0.0900\n",
            "Epoch 6/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.6577 - acc: 0.9600 - val_loss: 19.3488 - val_acc: 0.0600\n",
            "Epoch 7/3000\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 17.6186 - acc: 0.9800 - val_loss: 19.3478 - val_acc: 0.0600\n",
            "Epoch 8/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.5982 - acc: 0.9800 - val_loss: 19.3670 - val_acc: 0.0900\n",
            "Epoch 9/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.5897 - acc: 0.9700 - val_loss: 19.3508 - val_acc: 0.0500\n",
            "Epoch 10/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.5922 - acc: 0.9650 - val_loss: 19.3258 - val_acc: 0.1000\n",
            "Epoch 11/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.5577 - acc: 0.9700 - val_loss: 19.2288 - val_acc: 0.1200\n",
            "Epoch 12/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.5183 - acc: 0.9900 - val_loss: 19.2904 - val_acc: 0.1300\n",
            "Epoch 13/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.5238 - acc: 0.9850 - val_loss: 19.3808 - val_acc: 0.0900\n",
            "Epoch 14/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.5146 - acc: 0.9750 - val_loss: 19.4760 - val_acc: 0.0800\n",
            "Epoch 15/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.4624 - acc: 1.0000 - val_loss: 19.3964 - val_acc: 0.0900\n",
            "Epoch 16/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.4589 - acc: 0.9950 - val_loss: 19.2460 - val_acc: 0.1200\n",
            "Epoch 17/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.4622 - acc: 0.9950 - val_loss: 19.3357 - val_acc: 0.1200\n",
            "Epoch 18/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.4456 - acc: 0.9950 - val_loss: 19.4199 - val_acc: 0.0700\n",
            "Epoch 19/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.4154 - acc: 0.9900 - val_loss: 19.4292 - val_acc: 0.0800\n",
            "Epoch 20/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.4098 - acc: 0.9950 - val_loss: 19.4698 - val_acc: 0.1000\n",
            "Epoch 21/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.3998 - acc: 0.9900 - val_loss: 19.5680 - val_acc: 0.0800\n",
            "Epoch 22/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.3806 - acc: 0.9850 - val_loss: 19.3740 - val_acc: 0.0800\n",
            "Epoch 23/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.3698 - acc: 0.9850 - val_loss: 19.2853 - val_acc: 0.0700\n",
            "Epoch 24/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.3583 - acc: 0.9900 - val_loss: 19.4294 - val_acc: 0.0700\n",
            "Epoch 25/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.3396 - acc: 0.9950 - val_loss: 19.4495 - val_acc: 0.0500\n",
            "Epoch 26/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.3485 - acc: 0.9800 - val_loss: 19.3674 - val_acc: 0.0800\n",
            "Epoch 27/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.3046 - acc: 0.9950 - val_loss: 19.2353 - val_acc: 0.0800\n",
            "Epoch 28/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.2898 - acc: 1.0000 - val_loss: 19.2392 - val_acc: 0.0800\n",
            "Epoch 29/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.2744 - acc: 1.0000 - val_loss: 19.3512 - val_acc: 0.0900\n",
            "Epoch 30/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.2609 - acc: 0.9900 - val_loss: 19.4542 - val_acc: 0.1000\n",
            "Epoch 31/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.2479 - acc: 0.9950 - val_loss: 19.5176 - val_acc: 0.0700\n",
            "Epoch 32/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.2254 - acc: 1.0000 - val_loss: 19.5177 - val_acc: 0.0900\n",
            "Epoch 33/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.2162 - acc: 0.9950 - val_loss: 19.6013 - val_acc: 0.0800\n",
            "Epoch 34/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.2135 - acc: 0.9850 - val_loss: 19.4575 - val_acc: 0.1000\n",
            "Epoch 35/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.1802 - acc: 1.0000 - val_loss: 19.1599 - val_acc: 0.1200\n",
            "Epoch 36/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.1795 - acc: 0.9950 - val_loss: 19.1143 - val_acc: 0.1400\n",
            "Epoch 37/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.1862 - acc: 0.9950 - val_loss: 19.0312 - val_acc: 0.1300\n",
            "Epoch 38/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.1601 - acc: 1.0000 - val_loss: 18.8783 - val_acc: 0.1300\n",
            "Epoch 39/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.1466 - acc: 0.9950 - val_loss: 18.9414 - val_acc: 0.1300\n",
            "Epoch 40/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.1410 - acc: 0.9950 - val_loss: 19.0513 - val_acc: 0.0800\n",
            "Epoch 41/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.1059 - acc: 1.0000 - val_loss: 19.0798 - val_acc: 0.1100\n",
            "Epoch 42/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.0800 - acc: 1.0000 - val_loss: 19.1762 - val_acc: 0.1300\n",
            "Epoch 43/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.0741 - acc: 1.0000 - val_loss: 19.2276 - val_acc: 0.1200\n",
            "Epoch 44/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.0486 - acc: 1.0000 - val_loss: 19.3127 - val_acc: 0.1100\n",
            "Epoch 45/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.0371 - acc: 1.0000 - val_loss: 19.2993 - val_acc: 0.1100\n",
            "Epoch 46/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 17.0110 - acc: 1.0000 - val_loss: 19.2328 - val_acc: 0.1400\n",
            "Epoch 47/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.9930 - acc: 1.0000 - val_loss: 19.3494 - val_acc: 0.1300\n",
            "Epoch 48/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.9763 - acc: 1.0000 - val_loss: 19.4274 - val_acc: 0.1300\n",
            "Epoch 49/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.9632 - acc: 0.9950 - val_loss: 19.4563 - val_acc: 0.1500\n",
            "Epoch 50/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.9390 - acc: 1.0000 - val_loss: 19.4740 - val_acc: 0.1400\n",
            "Epoch 51/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.9335 - acc: 1.0000 - val_loss: 19.3994 - val_acc: 0.1500\n",
            "Epoch 52/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.9079 - acc: 1.0000 - val_loss: 19.3899 - val_acc: 0.1500\n",
            "Epoch 53/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.8943 - acc: 1.0000 - val_loss: 19.2749 - val_acc: 0.1600\n",
            "Epoch 54/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.8736 - acc: 1.0000 - val_loss: 19.3323 - val_acc: 0.1700\n",
            "Epoch 55/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.8577 - acc: 1.0000 - val_loss: 19.3509 - val_acc: 0.2000\n",
            "Epoch 56/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.8386 - acc: 1.0000 - val_loss: 19.4866 - val_acc: 0.1900\n",
            "Epoch 57/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.8194 - acc: 1.0000 - val_loss: 19.4341 - val_acc: 0.1500\n",
            "Epoch 58/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.8084 - acc: 1.0000 - val_loss: 19.4644 - val_acc: 0.1600\n",
            "Epoch 59/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.7843 - acc: 1.0000 - val_loss: 19.2913 - val_acc: 0.1800\n",
            "Epoch 60/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.7686 - acc: 1.0000 - val_loss: 19.2820 - val_acc: 0.1700\n",
            "Epoch 61/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.7542 - acc: 0.9950 - val_loss: 19.3391 - val_acc: 0.1700\n",
            "Epoch 62/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.7369 - acc: 0.9950 - val_loss: 19.3565 - val_acc: 0.1600\n",
            "Epoch 63/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.7244 - acc: 0.9900 - val_loss: 19.1448 - val_acc: 0.1400\n",
            "Epoch 64/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.7081 - acc: 1.0000 - val_loss: 18.7116 - val_acc: 0.1600\n",
            "Epoch 65/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.6910 - acc: 1.0000 - val_loss: 18.5575 - val_acc: 0.1200\n",
            "Epoch 66/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.6882 - acc: 0.9950 - val_loss: 18.7811 - val_acc: 0.1100\n",
            "Epoch 67/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.6592 - acc: 1.0000 - val_loss: 18.8844 - val_acc: 0.1300\n",
            "Epoch 68/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.6408 - acc: 0.9950 - val_loss: 18.8848 - val_acc: 0.1500\n",
            "Epoch 69/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.6162 - acc: 1.0000 - val_loss: 18.8191 - val_acc: 0.1900\n",
            "Epoch 70/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.6026 - acc: 1.0000 - val_loss: 18.6516 - val_acc: 0.2000\n",
            "Epoch 71/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.5817 - acc: 1.0000 - val_loss: 18.6959 - val_acc: 0.1800\n",
            "Epoch 72/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.5594 - acc: 1.0000 - val_loss: 18.7272 - val_acc: 0.1900\n",
            "Epoch 73/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.5451 - acc: 1.0000 - val_loss: 18.7875 - val_acc: 0.1500\n",
            "Epoch 74/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.5261 - acc: 0.9950 - val_loss: 18.8002 - val_acc: 0.1700\n",
            "Epoch 75/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.5093 - acc: 1.0000 - val_loss: 18.8438 - val_acc: 0.1600\n",
            "Epoch 76/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.4919 - acc: 0.9950 - val_loss: 18.9577 - val_acc: 0.1600\n",
            "Epoch 77/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.4662 - acc: 1.0000 - val_loss: 19.0853 - val_acc: 0.1200\n",
            "Epoch 78/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.4654 - acc: 0.9950 - val_loss: 18.9740 - val_acc: 0.1000\n",
            "Epoch 79/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.4314 - acc: 1.0000 - val_loss: 18.6845 - val_acc: 0.1300\n",
            "Epoch 80/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.4145 - acc: 1.0000 - val_loss: 18.5790 - val_acc: 0.1400\n",
            "Epoch 81/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.4014 - acc: 1.0000 - val_loss: 18.6946 - val_acc: 0.1400\n",
            "Epoch 82/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.3725 - acc: 1.0000 - val_loss: 18.6954 - val_acc: 0.1400\n",
            "Epoch 83/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.3499 - acc: 1.0000 - val_loss: 18.8662 - val_acc: 0.1500\n",
            "Epoch 84/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.3348 - acc: 0.9950 - val_loss: 18.8440 - val_acc: 0.1200\n",
            "Epoch 85/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.3132 - acc: 0.9950 - val_loss: 18.8484 - val_acc: 0.1400\n",
            "Epoch 86/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.2929 - acc: 1.0000 - val_loss: 18.7637 - val_acc: 0.1500\n",
            "Epoch 87/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.2703 - acc: 1.0000 - val_loss: 18.6790 - val_acc: 0.1600\n",
            "Epoch 88/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.2535 - acc: 1.0000 - val_loss: 18.5945 - val_acc: 0.1700\n",
            "Epoch 89/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.2308 - acc: 1.0000 - val_loss: 18.6231 - val_acc: 0.1600\n",
            "Epoch 90/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.2099 - acc: 1.0000 - val_loss: 18.6417 - val_acc: 0.1600\n",
            "Epoch 91/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.1881 - acc: 1.0000 - val_loss: 18.7002 - val_acc: 0.1800\n",
            "Epoch 92/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.1697 - acc: 1.0000 - val_loss: 18.6477 - val_acc: 0.1800\n",
            "Epoch 93/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.1468 - acc: 1.0000 - val_loss: 18.6308 - val_acc: 0.1900\n",
            "Epoch 94/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.1294 - acc: 0.9950 - val_loss: 18.5843 - val_acc: 0.2000\n",
            "Epoch 95/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.1055 - acc: 1.0000 - val_loss: 18.4516 - val_acc: 0.1900\n",
            "Epoch 96/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.0865 - acc: 1.0000 - val_loss: 18.3506 - val_acc: 0.1900\n",
            "Epoch 97/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.0661 - acc: 1.0000 - val_loss: 18.4846 - val_acc: 0.1800\n",
            "Epoch 98/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.0438 - acc: 1.0000 - val_loss: 18.5276 - val_acc: 0.1900\n",
            "Epoch 99/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.0242 - acc: 1.0000 - val_loss: 18.5491 - val_acc: 0.1700\n",
            "Epoch 100/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 16.0101 - acc: 0.9950 - val_loss: 18.5953 - val_acc: 0.1800\n",
            "Epoch 101/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.9799 - acc: 1.0000 - val_loss: 18.4300 - val_acc: 0.2100\n",
            "Epoch 102/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.9682 - acc: 1.0000 - val_loss: 18.2881 - val_acc: 0.2400\n",
            "Epoch 103/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.9418 - acc: 1.0000 - val_loss: 18.3099 - val_acc: 0.1700\n",
            "Epoch 104/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.9262 - acc: 0.9950 - val_loss: 18.5048 - val_acc: 0.1400\n",
            "Epoch 105/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.9047 - acc: 0.9950 - val_loss: 18.5416 - val_acc: 0.1200\n",
            "Epoch 106/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.8766 - acc: 1.0000 - val_loss: 18.5136 - val_acc: 0.1200\n",
            "Epoch 107/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.8554 - acc: 1.0000 - val_loss: 18.4801 - val_acc: 0.1400\n",
            "Epoch 108/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.8351 - acc: 1.0000 - val_loss: 18.5596 - val_acc: 0.1500\n",
            "Epoch 109/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.8121 - acc: 1.0000 - val_loss: 18.6203 - val_acc: 0.1500\n",
            "Epoch 110/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.7916 - acc: 1.0000 - val_loss: 18.5681 - val_acc: 0.1600\n",
            "Epoch 111/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.7715 - acc: 1.0000 - val_loss: 18.4524 - val_acc: 0.1600\n",
            "Epoch 112/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.7437 - acc: 1.0000 - val_loss: 18.4383 - val_acc: 0.1700\n",
            "Epoch 113/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.7222 - acc: 1.0000 - val_loss: 18.4938 - val_acc: 0.1500\n",
            "Epoch 114/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.7119 - acc: 0.9950 - val_loss: 18.3995 - val_acc: 0.1600\n",
            "Epoch 115/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.6864 - acc: 0.9950 - val_loss: 18.1462 - val_acc: 0.1300\n",
            "Epoch 116/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.6649 - acc: 1.0000 - val_loss: 17.9636 - val_acc: 0.1800\n",
            "Epoch 117/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.6454 - acc: 1.0000 - val_loss: 17.8004 - val_acc: 0.1700\n",
            "Epoch 118/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.6411 - acc: 0.9950 - val_loss: 18.0329 - val_acc: 0.1400\n",
            "Epoch 119/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.6033 - acc: 1.0000 - val_loss: 18.0526 - val_acc: 0.1300\n",
            "Epoch 120/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.5953 - acc: 0.9950 - val_loss: 18.2072 - val_acc: 0.1400\n",
            "Epoch 121/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.5659 - acc: 0.9950 - val_loss: 18.2306 - val_acc: 0.1600\n",
            "Epoch 122/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.5404 - acc: 1.0000 - val_loss: 18.1778 - val_acc: 0.1500\n",
            "Epoch 123/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.5148 - acc: 1.0000 - val_loss: 18.0570 - val_acc: 0.1200\n",
            "Epoch 124/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.4947 - acc: 1.0000 - val_loss: 18.1327 - val_acc: 0.1200\n",
            "Epoch 125/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.4697 - acc: 1.0000 - val_loss: 18.2137 - val_acc: 0.1300\n",
            "Epoch 126/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.4489 - acc: 1.0000 - val_loss: 18.1843 - val_acc: 0.1400\n",
            "Epoch 127/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.4230 - acc: 1.0000 - val_loss: 17.9014 - val_acc: 0.1600\n",
            "Epoch 128/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.4005 - acc: 1.0000 - val_loss: 17.7207 - val_acc: 0.1700\n",
            "Epoch 129/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.3777 - acc: 1.0000 - val_loss: 17.6652 - val_acc: 0.1700\n",
            "Epoch 130/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.3559 - acc: 1.0000 - val_loss: 17.7861 - val_acc: 0.1400\n",
            "Epoch 131/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.3333 - acc: 1.0000 - val_loss: 17.8169 - val_acc: 0.1500\n",
            "Epoch 132/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.3093 - acc: 1.0000 - val_loss: 18.1696 - val_acc: 0.1300\n",
            "Epoch 133/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.2843 - acc: 1.0000 - val_loss: 17.9972 - val_acc: 0.1500\n",
            "Epoch 134/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.2612 - acc: 1.0000 - val_loss: 18.2171 - val_acc: 0.1300\n",
            "Epoch 135/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.2381 - acc: 1.0000 - val_loss: 18.1177 - val_acc: 0.1300\n",
            "Epoch 136/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.2152 - acc: 1.0000 - val_loss: 18.1673 - val_acc: 0.1200\n",
            "Epoch 137/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.1918 - acc: 1.0000 - val_loss: 18.1236 - val_acc: 0.1400\n",
            "Epoch 138/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.1671 - acc: 1.0000 - val_loss: 18.0581 - val_acc: 0.1500\n",
            "Epoch 139/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.1436 - acc: 1.0000 - val_loss: 17.9574 - val_acc: 0.1500\n",
            "Epoch 140/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.1223 - acc: 1.0000 - val_loss: 17.9196 - val_acc: 0.1600\n",
            "Epoch 141/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.0966 - acc: 1.0000 - val_loss: 17.9759 - val_acc: 0.1500\n",
            "Epoch 142/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.0752 - acc: 1.0000 - val_loss: 17.9539 - val_acc: 0.1500\n",
            "Epoch 143/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.0494 - acc: 1.0000 - val_loss: 18.0105 - val_acc: 0.1500\n",
            "Epoch 144/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.0263 - acc: 1.0000 - val_loss: 17.9944 - val_acc: 0.1400\n",
            "Epoch 145/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 15.0016 - acc: 1.0000 - val_loss: 17.9339 - val_acc: 0.1600\n",
            "Epoch 146/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.9783 - acc: 1.0000 - val_loss: 17.9706 - val_acc: 0.1500\n",
            "Epoch 147/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.9553 - acc: 1.0000 - val_loss: 17.9798 - val_acc: 0.1400\n",
            "Epoch 148/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.9315 - acc: 1.0000 - val_loss: 17.8616 - val_acc: 0.1700\n",
            "Epoch 149/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.9063 - acc: 1.0000 - val_loss: 17.7429 - val_acc: 0.1600\n",
            "Epoch 150/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.8866 - acc: 1.0000 - val_loss: 17.6553 - val_acc: 0.1700\n",
            "Epoch 151/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.8585 - acc: 1.0000 - val_loss: 17.4919 - val_acc: 0.1700\n",
            "Epoch 152/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.8365 - acc: 1.0000 - val_loss: 17.4226 - val_acc: 0.1800\n",
            "Epoch 153/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.8169 - acc: 1.0000 - val_loss: 17.6042 - val_acc: 0.1800\n",
            "Epoch 154/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.7896 - acc: 1.0000 - val_loss: 17.8088 - val_acc: 0.1200\n",
            "Epoch 155/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.7657 - acc: 1.0000 - val_loss: 18.0326 - val_acc: 0.1000\n",
            "Epoch 156/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.7389 - acc: 1.0000 - val_loss: 17.8665 - val_acc: 0.1000\n",
            "Epoch 157/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.7166 - acc: 1.0000 - val_loss: 17.7314 - val_acc: 0.1100\n",
            "Epoch 158/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.6939 - acc: 1.0000 - val_loss: 17.3810 - val_acc: 0.1400\n",
            "Epoch 159/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.6829 - acc: 0.9900 - val_loss: 17.3179 - val_acc: 0.1800\n",
            "Epoch 160/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.6464 - acc: 1.0000 - val_loss: 17.0473 - val_acc: 0.1800\n",
            "Epoch 161/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.6244 - acc: 1.0000 - val_loss: 16.9248 - val_acc: 0.1500\n",
            "Epoch 162/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.6036 - acc: 1.0000 - val_loss: 17.0022 - val_acc: 0.1300\n",
            "Epoch 163/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.5805 - acc: 1.0000 - val_loss: 17.1513 - val_acc: 0.1400\n",
            "Epoch 164/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.5630 - acc: 0.9950 - val_loss: 17.1273 - val_acc: 0.1300\n",
            "Epoch 165/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.5496 - acc: 0.9900 - val_loss: 17.1965 - val_acc: 0.1200\n",
            "Epoch 166/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.5148 - acc: 1.0000 - val_loss: 16.9013 - val_acc: 0.1200\n",
            "Epoch 167/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.4981 - acc: 1.0000 - val_loss: 17.1185 - val_acc: 0.1000\n",
            "Epoch 168/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.4767 - acc: 1.0000 - val_loss: 17.2031 - val_acc: 0.1100\n",
            "Epoch 169/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.4464 - acc: 1.0000 - val_loss: 17.3545 - val_acc: 0.0900\n",
            "Epoch 170/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.4268 - acc: 1.0000 - val_loss: 17.4446 - val_acc: 0.1200\n",
            "Epoch 171/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.3973 - acc: 1.0000 - val_loss: 17.2201 - val_acc: 0.1500\n",
            "Epoch 172/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.3697 - acc: 1.0000 - val_loss: 17.2386 - val_acc: 0.1400\n",
            "Epoch 173/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.3448 - acc: 1.0000 - val_loss: 17.5552 - val_acc: 0.1100\n",
            "Epoch 174/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.3172 - acc: 1.0000 - val_loss: 17.5879 - val_acc: 0.1000\n",
            "Epoch 175/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.2940 - acc: 1.0000 - val_loss: 17.6984 - val_acc: 0.0900\n",
            "Epoch 176/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.2672 - acc: 1.0000 - val_loss: 18.0087 - val_acc: 0.0800\n",
            "Epoch 177/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.2455 - acc: 1.0000 - val_loss: 17.7636 - val_acc: 0.0800\n",
            "Epoch 178/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.2263 - acc: 0.9950 - val_loss: 17.6435 - val_acc: 0.1100\n",
            "Epoch 179/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.1966 - acc: 1.0000 - val_loss: 17.2343 - val_acc: 0.1900\n",
            "Epoch 180/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.1910 - acc: 0.9900 - val_loss: 17.2807 - val_acc: 0.1800\n",
            "Epoch 181/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.1553 - acc: 0.9950 - val_loss: 17.1163 - val_acc: 0.1600\n",
            "Epoch 182/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.1280 - acc: 1.0000 - val_loss: 16.7447 - val_acc: 0.1500\n",
            "Epoch 183/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.1069 - acc: 1.0000 - val_loss: 16.6174 - val_acc: 0.1500\n",
            "Epoch 184/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.0813 - acc: 1.0000 - val_loss: 16.5983 - val_acc: 0.1600\n",
            "Epoch 185/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.0545 - acc: 1.0000 - val_loss: 16.5974 - val_acc: 0.1400\n",
            "Epoch 186/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.0367 - acc: 0.9950 - val_loss: 16.4968 - val_acc: 0.1900\n",
            "Epoch 187/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 14.0179 - acc: 0.9950 - val_loss: 16.5293 - val_acc: 0.1500\n",
            "Epoch 188/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.9881 - acc: 0.9950 - val_loss: 16.7967 - val_acc: 0.1300\n",
            "Epoch 189/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.9599 - acc: 1.0000 - val_loss: 16.6620 - val_acc: 0.1400\n",
            "Epoch 190/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.9373 - acc: 1.0000 - val_loss: 16.5095 - val_acc: 0.1600\n",
            "Epoch 191/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.9263 - acc: 1.0000 - val_loss: 16.3062 - val_acc: 0.1600\n",
            "Epoch 192/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.8984 - acc: 1.0000 - val_loss: 15.9171 - val_acc: 0.2200\n",
            "Epoch 193/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.9022 - acc: 0.9900 - val_loss: 16.1748 - val_acc: 0.1500\n",
            "Epoch 194/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.8766 - acc: 0.9850 - val_loss: 16.3001 - val_acc: 0.1300\n",
            "Epoch 195/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.8378 - acc: 1.0000 - val_loss: 15.9265 - val_acc: 0.2100\n",
            "Epoch 196/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.8192 - acc: 0.9950 - val_loss: 15.8673 - val_acc: 0.2100\n",
            "Epoch 197/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.7925 - acc: 1.0000 - val_loss: 16.1699 - val_acc: 0.1500\n",
            "Epoch 198/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.7748 - acc: 0.9900 - val_loss: 16.1756 - val_acc: 0.1600\n",
            "Epoch 199/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.7456 - acc: 1.0000 - val_loss: 15.6202 - val_acc: 0.2700\n",
            "Epoch 200/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.7200 - acc: 1.0000 - val_loss: 15.5737 - val_acc: 0.2800\n",
            "Epoch 201/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.7034 - acc: 0.9950 - val_loss: 15.9014 - val_acc: 0.2000\n",
            "Epoch 202/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.6797 - acc: 0.9950 - val_loss: 16.0952 - val_acc: 0.2000\n",
            "Epoch 203/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.6528 - acc: 1.0000 - val_loss: 16.1898 - val_acc: 0.1900\n",
            "Epoch 204/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.6428 - acc: 0.9950 - val_loss: 15.9704 - val_acc: 0.1900\n",
            "Epoch 205/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.6045 - acc: 1.0000 - val_loss: 15.9122 - val_acc: 0.2200\n",
            "Epoch 206/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.5778 - acc: 1.0000 - val_loss: 16.1990 - val_acc: 0.2200\n",
            "Epoch 207/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.5556 - acc: 1.0000 - val_loss: 16.4950 - val_acc: 0.1800\n",
            "Epoch 208/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.5314 - acc: 1.0000 - val_loss: 16.5279 - val_acc: 0.1800\n",
            "Epoch 209/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.5113 - acc: 0.9950 - val_loss: 16.4399 - val_acc: 0.1800\n",
            "Epoch 210/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.4825 - acc: 1.0000 - val_loss: 16.3479 - val_acc: 0.1800\n",
            "Epoch 211/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.4600 - acc: 1.0000 - val_loss: 16.1202 - val_acc: 0.2300\n",
            "Epoch 212/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.4399 - acc: 0.9950 - val_loss: 16.0415 - val_acc: 0.2300\n",
            "Epoch 213/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.4153 - acc: 0.9950 - val_loss: 16.2241 - val_acc: 0.2200\n",
            "Epoch 214/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.3917 - acc: 1.0000 - val_loss: 16.2411 - val_acc: 0.2100\n",
            "Epoch 215/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.3644 - acc: 1.0000 - val_loss: 16.5720 - val_acc: 0.2000\n",
            "Epoch 216/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.3449 - acc: 1.0000 - val_loss: 16.4889 - val_acc: 0.1800\n",
            "Epoch 217/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.3188 - acc: 1.0000 - val_loss: 16.5012 - val_acc: 0.2200\n",
            "Epoch 218/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.3002 - acc: 0.9950 - val_loss: 16.2827 - val_acc: 0.2200\n",
            "Epoch 219/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.2746 - acc: 1.0000 - val_loss: 16.2252 - val_acc: 0.2200\n",
            "Epoch 220/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.2531 - acc: 1.0000 - val_loss: 16.1857 - val_acc: 0.2500\n",
            "Epoch 221/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.2245 - acc: 1.0000 - val_loss: 15.7734 - val_acc: 0.2400\n",
            "Epoch 222/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.2028 - acc: 1.0000 - val_loss: 15.7710 - val_acc: 0.2500\n",
            "Epoch 223/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.1773 - acc: 1.0000 - val_loss: 15.9595 - val_acc: 0.2600\n",
            "Epoch 224/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.1621 - acc: 0.9950 - val_loss: 16.1203 - val_acc: 0.2300\n",
            "Epoch 225/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.1295 - acc: 1.0000 - val_loss: 16.4341 - val_acc: 0.2100\n",
            "Epoch 226/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.1065 - acc: 1.0000 - val_loss: 16.3834 - val_acc: 0.2200\n",
            "Epoch 227/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.0833 - acc: 1.0000 - val_loss: 16.3386 - val_acc: 0.2300\n",
            "Epoch 228/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.0577 - acc: 1.0000 - val_loss: 16.0568 - val_acc: 0.2100\n",
            "Epoch 229/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.0362 - acc: 1.0000 - val_loss: 16.0815 - val_acc: 0.2200\n",
            "Epoch 230/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 13.0119 - acc: 1.0000 - val_loss: 16.1153 - val_acc: 0.2000\n",
            "Epoch 231/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.9862 - acc: 1.0000 - val_loss: 16.2213 - val_acc: 0.1900\n",
            "Epoch 232/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.9617 - acc: 1.0000 - val_loss: 16.2532 - val_acc: 0.2000\n",
            "Epoch 233/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.9382 - acc: 1.0000 - val_loss: 16.2202 - val_acc: 0.1900\n",
            "Epoch 234/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.9143 - acc: 1.0000 - val_loss: 16.2293 - val_acc: 0.2100\n",
            "Epoch 235/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.8911 - acc: 1.0000 - val_loss: 16.4009 - val_acc: 0.2200\n",
            "Epoch 236/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.8685 - acc: 1.0000 - val_loss: 16.2840 - val_acc: 0.2300\n",
            "Epoch 237/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.8423 - acc: 1.0000 - val_loss: 16.6135 - val_acc: 0.1800\n",
            "Epoch 238/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.8196 - acc: 1.0000 - val_loss: 16.5476 - val_acc: 0.1800\n",
            "Epoch 239/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.7986 - acc: 1.0000 - val_loss: 16.5193 - val_acc: 0.1900\n",
            "Epoch 240/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.7758 - acc: 0.9950 - val_loss: 16.2976 - val_acc: 0.2200\n",
            "Epoch 241/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.7472 - acc: 1.0000 - val_loss: 15.5192 - val_acc: 0.2500\n",
            "Epoch 242/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.7323 - acc: 0.9950 - val_loss: 15.4163 - val_acc: 0.2300\n",
            "Epoch 243/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.7089 - acc: 0.9950 - val_loss: 15.3453 - val_acc: 0.2200\n",
            "Epoch 244/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.6823 - acc: 1.0000 - val_loss: 15.5834 - val_acc: 0.1900\n",
            "Epoch 245/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.6808 - acc: 0.9950 - val_loss: 15.6691 - val_acc: 0.1800\n",
            "Epoch 246/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.6381 - acc: 0.9950 - val_loss: 15.7379 - val_acc: 0.1700\n",
            "Epoch 247/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.6234 - acc: 0.9950 - val_loss: 15.7737 - val_acc: 0.1400\n",
            "Epoch 248/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.6055 - acc: 0.9950 - val_loss: 15.9603 - val_acc: 0.1500\n",
            "Epoch 249/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.5817 - acc: 0.9950 - val_loss: 16.1455 - val_acc: 0.1500\n",
            "Epoch 250/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.5512 - acc: 1.0000 - val_loss: 15.7450 - val_acc: 0.2100\n",
            "Epoch 251/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.5273 - acc: 1.0000 - val_loss: 15.8637 - val_acc: 0.2100\n",
            "Epoch 252/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.5009 - acc: 1.0000 - val_loss: 16.1299 - val_acc: 0.1700\n",
            "Epoch 253/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.4779 - acc: 1.0000 - val_loss: 16.2350 - val_acc: 0.1600\n",
            "Epoch 254/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.4503 - acc: 1.0000 - val_loss: 16.2547 - val_acc: 0.1800\n",
            "Epoch 255/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.4318 - acc: 1.0000 - val_loss: 16.0477 - val_acc: 0.2000\n",
            "Epoch 256/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.4048 - acc: 1.0000 - val_loss: 15.8328 - val_acc: 0.2300\n",
            "Epoch 257/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.3850 - acc: 1.0000 - val_loss: 15.6653 - val_acc: 0.2100\n",
            "Epoch 258/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.3568 - acc: 1.0000 - val_loss: 15.9181 - val_acc: 0.1900\n",
            "Epoch 259/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.3320 - acc: 1.0000 - val_loss: 15.9729 - val_acc: 0.1900\n",
            "Epoch 260/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.3081 - acc: 1.0000 - val_loss: 15.9871 - val_acc: 0.1600\n",
            "Epoch 261/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.2940 - acc: 0.9950 - val_loss: 15.9678 - val_acc: 0.1600\n",
            "Epoch 262/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.2617 - acc: 1.0000 - val_loss: 15.6632 - val_acc: 0.2000\n",
            "Epoch 263/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.2415 - acc: 1.0000 - val_loss: 15.4406 - val_acc: 0.2200\n",
            "Epoch 264/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.2216 - acc: 1.0000 - val_loss: 15.5141 - val_acc: 0.1500\n",
            "Epoch 265/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.1977 - acc: 1.0000 - val_loss: 15.4301 - val_acc: 0.1700\n",
            "Epoch 266/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.1764 - acc: 1.0000 - val_loss: 15.3425 - val_acc: 0.1600\n",
            "Epoch 267/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.1527 - acc: 1.0000 - val_loss: 15.2423 - val_acc: 0.1500\n",
            "Epoch 268/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.1323 - acc: 1.0000 - val_loss: 15.3115 - val_acc: 0.1500\n",
            "Epoch 269/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.1032 - acc: 1.0000 - val_loss: 15.4476 - val_acc: 0.1600\n",
            "Epoch 270/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.0802 - acc: 1.0000 - val_loss: 15.5869 - val_acc: 0.1600\n",
            "Epoch 271/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.0582 - acc: 1.0000 - val_loss: 15.5833 - val_acc: 0.1300\n",
            "Epoch 272/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.0350 - acc: 1.0000 - val_loss: 15.5390 - val_acc: 0.1800\n",
            "Epoch 273/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 12.0099 - acc: 1.0000 - val_loss: 15.1371 - val_acc: 0.2500\n",
            "Epoch 274/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.9979 - acc: 0.9950 - val_loss: 15.1608 - val_acc: 0.2300\n",
            "Epoch 275/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.9691 - acc: 0.9950 - val_loss: 15.3556 - val_acc: 0.2100\n",
            "Epoch 276/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.9418 - acc: 1.0000 - val_loss: 15.3896 - val_acc: 0.1900\n",
            "Epoch 277/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.9174 - acc: 1.0000 - val_loss: 15.2270 - val_acc: 0.1900\n",
            "Epoch 278/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.8947 - acc: 1.0000 - val_loss: 15.1879 - val_acc: 0.1800\n",
            "Epoch 279/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.8692 - acc: 1.0000 - val_loss: 15.2565 - val_acc: 0.1800\n",
            "Epoch 280/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.8480 - acc: 1.0000 - val_loss: 15.3063 - val_acc: 0.1800\n",
            "Epoch 281/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.8222 - acc: 1.0000 - val_loss: 15.4220 - val_acc: 0.1900\n",
            "Epoch 282/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.8004 - acc: 1.0000 - val_loss: 15.5091 - val_acc: 0.1800\n",
            "Epoch 283/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.7764 - acc: 1.0000 - val_loss: 15.2198 - val_acc: 0.1800\n",
            "Epoch 284/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.7518 - acc: 1.0000 - val_loss: 15.0931 - val_acc: 0.1800\n",
            "Epoch 285/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.7323 - acc: 1.0000 - val_loss: 15.0267 - val_acc: 0.1700\n",
            "Epoch 286/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.7047 - acc: 1.0000 - val_loss: 14.9812 - val_acc: 0.2000\n",
            "Epoch 287/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.6815 - acc: 1.0000 - val_loss: 14.9993 - val_acc: 0.2000\n",
            "Epoch 288/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.6571 - acc: 1.0000 - val_loss: 14.9018 - val_acc: 0.1900\n",
            "Epoch 289/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.6370 - acc: 1.0000 - val_loss: 14.8807 - val_acc: 0.2200\n",
            "Epoch 290/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.6141 - acc: 0.9950 - val_loss: 14.3890 - val_acc: 0.2400\n",
            "Epoch 291/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.5882 - acc: 1.0000 - val_loss: 14.3639 - val_acc: 0.2500\n",
            "Epoch 292/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.5643 - acc: 1.0000 - val_loss: 14.5235 - val_acc: 0.2600\n",
            "Epoch 293/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.5400 - acc: 1.0000 - val_loss: 14.5516 - val_acc: 0.2400\n",
            "Epoch 294/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.5169 - acc: 1.0000 - val_loss: 14.6430 - val_acc: 0.2300\n",
            "Epoch 295/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.4922 - acc: 1.0000 - val_loss: 14.8178 - val_acc: 0.2000\n",
            "Epoch 296/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.4681 - acc: 1.0000 - val_loss: 14.9749 - val_acc: 0.2400\n",
            "Epoch 297/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.4447 - acc: 1.0000 - val_loss: 14.8502 - val_acc: 0.2100\n",
            "Epoch 298/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.4211 - acc: 1.0000 - val_loss: 14.9013 - val_acc: 0.2200\n",
            "Epoch 299/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.3970 - acc: 1.0000 - val_loss: 14.7535 - val_acc: 0.2100\n",
            "Epoch 300/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.3731 - acc: 1.0000 - val_loss: 14.7972 - val_acc: 0.2200\n",
            "Epoch 301/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.3495 - acc: 1.0000 - val_loss: 14.7822 - val_acc: 0.2400\n",
            "Epoch 302/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.3253 - acc: 1.0000 - val_loss: 14.7937 - val_acc: 0.2300\n",
            "Epoch 303/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.3011 - acc: 1.0000 - val_loss: 14.8649 - val_acc: 0.2600\n",
            "Epoch 304/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.2770 - acc: 1.0000 - val_loss: 14.9199 - val_acc: 0.2600\n",
            "Epoch 305/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.2536 - acc: 1.0000 - val_loss: 14.9678 - val_acc: 0.2500\n",
            "Epoch 306/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.2290 - acc: 1.0000 - val_loss: 14.8193 - val_acc: 0.2700\n",
            "Epoch 307/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.2059 - acc: 1.0000 - val_loss: 14.6420 - val_acc: 0.2800\n",
            "Epoch 308/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.1813 - acc: 1.0000 - val_loss: 14.6199 - val_acc: 0.2600\n",
            "Epoch 309/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.1567 - acc: 1.0000 - val_loss: 14.7256 - val_acc: 0.2700\n",
            "Epoch 310/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.1325 - acc: 1.0000 - val_loss: 14.6753 - val_acc: 0.2400\n",
            "Epoch 311/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.1087 - acc: 1.0000 - val_loss: 14.6947 - val_acc: 0.2400\n",
            "Epoch 312/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.0841 - acc: 1.0000 - val_loss: 14.9715 - val_acc: 0.2400\n",
            "Epoch 313/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.0602 - acc: 1.0000 - val_loss: 14.8224 - val_acc: 0.2400\n",
            "Epoch 314/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.0355 - acc: 1.0000 - val_loss: 14.6963 - val_acc: 0.2300\n",
            "Epoch 315/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 11.0113 - acc: 1.0000 - val_loss: 14.7972 - val_acc: 0.2400\n",
            "Epoch 316/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.9871 - acc: 1.0000 - val_loss: 14.5877 - val_acc: 0.2400\n",
            "Epoch 317/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.9627 - acc: 1.0000 - val_loss: 14.6122 - val_acc: 0.2500\n",
            "Epoch 318/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.9382 - acc: 1.0000 - val_loss: 14.6286 - val_acc: 0.2400\n",
            "Epoch 319/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.9140 - acc: 1.0000 - val_loss: 14.6507 - val_acc: 0.2500\n",
            "Epoch 320/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.8895 - acc: 1.0000 - val_loss: 14.8130 - val_acc: 0.2300\n",
            "Epoch 321/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.8651 - acc: 1.0000 - val_loss: 14.5694 - val_acc: 0.2500\n",
            "Epoch 322/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.8407 - acc: 1.0000 - val_loss: 14.4944 - val_acc: 0.2500\n",
            "Epoch 323/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.8161 - acc: 1.0000 - val_loss: 14.6922 - val_acc: 0.2300\n",
            "Epoch 324/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.7918 - acc: 1.0000 - val_loss: 14.5503 - val_acc: 0.2300\n",
            "Epoch 325/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.7674 - acc: 1.0000 - val_loss: 14.4944 - val_acc: 0.2500\n",
            "Epoch 326/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.7429 - acc: 1.0000 - val_loss: 14.7314 - val_acc: 0.2300\n",
            "Epoch 327/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.7181 - acc: 1.0000 - val_loss: 14.7327 - val_acc: 0.2300\n",
            "Epoch 328/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.6935 - acc: 1.0000 - val_loss: 14.4657 - val_acc: 0.2400\n",
            "Epoch 329/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.6690 - acc: 1.0000 - val_loss: 14.4963 - val_acc: 0.2400\n",
            "Epoch 330/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.6449 - acc: 1.0000 - val_loss: 14.5970 - val_acc: 0.2400\n",
            "Epoch 331/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.6199 - acc: 1.0000 - val_loss: 14.4551 - val_acc: 0.2500\n",
            "Epoch 332/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.5954 - acc: 1.0000 - val_loss: 14.3692 - val_acc: 0.2600\n",
            "Epoch 333/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.5707 - acc: 1.0000 - val_loss: 14.3428 - val_acc: 0.2500\n",
            "Epoch 334/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.5463 - acc: 1.0000 - val_loss: 14.1222 - val_acc: 0.2400\n",
            "Epoch 335/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.5218 - acc: 1.0000 - val_loss: 14.1443 - val_acc: 0.2100\n",
            "Epoch 336/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.4968 - acc: 1.0000 - val_loss: 14.1646 - val_acc: 0.2300\n",
            "Epoch 337/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.4723 - acc: 1.0000 - val_loss: 14.1804 - val_acc: 0.2300\n",
            "Epoch 338/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.4473 - acc: 1.0000 - val_loss: 14.0435 - val_acc: 0.2100\n",
            "Epoch 339/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.4226 - acc: 1.0000 - val_loss: 13.9014 - val_acc: 0.2300\n",
            "Epoch 340/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.4003 - acc: 1.0000 - val_loss: 13.8783 - val_acc: 0.2700\n",
            "Epoch 341/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.3737 - acc: 1.0000 - val_loss: 13.5406 - val_acc: 0.2800\n",
            "Epoch 342/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.3519 - acc: 1.0000 - val_loss: 13.5235 - val_acc: 0.2800\n",
            "Epoch 343/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.3304 - acc: 1.0000 - val_loss: 13.6321 - val_acc: 0.2600\n",
            "Epoch 344/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.3026 - acc: 1.0000 - val_loss: 13.5560 - val_acc: 0.2300\n",
            "Epoch 345/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.2775 - acc: 1.0000 - val_loss: 13.9036 - val_acc: 0.2200\n",
            "Epoch 346/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.2535 - acc: 1.0000 - val_loss: 13.8190 - val_acc: 0.2200\n",
            "Epoch 347/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.2282 - acc: 1.0000 - val_loss: 13.8745 - val_acc: 0.1800\n",
            "Epoch 348/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.2035 - acc: 1.0000 - val_loss: 13.7153 - val_acc: 0.1600\n",
            "Epoch 349/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.1794 - acc: 1.0000 - val_loss: 13.8206 - val_acc: 0.1600\n",
            "Epoch 350/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.1541 - acc: 1.0000 - val_loss: 13.7279 - val_acc: 0.2000\n",
            "Epoch 351/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.1308 - acc: 1.0000 - val_loss: 13.6102 - val_acc: 0.2000\n",
            "Epoch 352/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.1062 - acc: 1.0000 - val_loss: 13.7110 - val_acc: 0.1900\n",
            "Epoch 353/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.0807 - acc: 1.0000 - val_loss: 13.6864 - val_acc: 0.2000\n",
            "Epoch 354/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.0570 - acc: 1.0000 - val_loss: 13.7964 - val_acc: 0.1900\n",
            "Epoch 355/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.0314 - acc: 1.0000 - val_loss: 13.5653 - val_acc: 0.2000\n",
            "Epoch 356/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 10.0068 - acc: 1.0000 - val_loss: 13.6398 - val_acc: 0.2100\n",
            "Epoch 357/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.9825 - acc: 1.0000 - val_loss: 13.5524 - val_acc: 0.2000\n",
            "Epoch 358/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.9573 - acc: 1.0000 - val_loss: 13.4673 - val_acc: 0.2000\n",
            "Epoch 359/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.9334 - acc: 1.0000 - val_loss: 13.6100 - val_acc: 0.2000\n",
            "Epoch 360/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.9079 - acc: 1.0000 - val_loss: 13.3139 - val_acc: 0.2500\n",
            "Epoch 361/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.8832 - acc: 1.0000 - val_loss: 13.2493 - val_acc: 0.2600\n",
            "Epoch 362/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.8590 - acc: 1.0000 - val_loss: 13.2365 - val_acc: 0.2400\n",
            "Epoch 363/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.8340 - acc: 1.0000 - val_loss: 13.3538 - val_acc: 0.2200\n",
            "Epoch 364/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.8094 - acc: 1.0000 - val_loss: 13.4339 - val_acc: 0.2300\n",
            "Epoch 365/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.7846 - acc: 1.0000 - val_loss: 13.3640 - val_acc: 0.2200\n",
            "Epoch 366/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.7600 - acc: 1.0000 - val_loss: 13.4808 - val_acc: 0.2400\n",
            "Epoch 367/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.7351 - acc: 1.0000 - val_loss: 13.4577 - val_acc: 0.2500\n",
            "Epoch 368/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.7102 - acc: 1.0000 - val_loss: 13.4165 - val_acc: 0.2600\n",
            "Epoch 369/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.6867 - acc: 1.0000 - val_loss: 13.4305 - val_acc: 0.2500\n",
            "Epoch 370/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.6607 - acc: 1.0000 - val_loss: 13.4197 - val_acc: 0.2500\n",
            "Epoch 371/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.6357 - acc: 1.0000 - val_loss: 13.3949 - val_acc: 0.2600\n",
            "Epoch 372/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.6114 - acc: 1.0000 - val_loss: 13.4224 - val_acc: 0.2400\n",
            "Epoch 373/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.5862 - acc: 1.0000 - val_loss: 13.6434 - val_acc: 0.2400\n",
            "Epoch 374/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.5668 - acc: 0.9950 - val_loss: 13.3381 - val_acc: 0.2400\n",
            "Epoch 375/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.5427 - acc: 1.0000 - val_loss: 13.0004 - val_acc: 0.3500\n",
            "Epoch 376/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.5216 - acc: 1.0000 - val_loss: 13.4945 - val_acc: 0.2700\n",
            "Epoch 377/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.5347 - acc: 0.9900 - val_loss: 14.2726 - val_acc: 0.1300\n",
            "Epoch 378/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.4719 - acc: 1.0000 - val_loss: 14.1995 - val_acc: 0.1600\n",
            "Epoch 379/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.4509 - acc: 1.0000 - val_loss: 13.4929 - val_acc: 0.1700\n",
            "Epoch 380/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.4225 - acc: 1.0000 - val_loss: 13.3592 - val_acc: 0.2100\n",
            "Epoch 381/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.3993 - acc: 1.0000 - val_loss: 13.2996 - val_acc: 0.1600\n",
            "Epoch 382/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.3746 - acc: 1.0000 - val_loss: 13.4986 - val_acc: 0.1500\n",
            "Epoch 383/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.3519 - acc: 1.0000 - val_loss: 13.6358 - val_acc: 0.1400\n",
            "Epoch 384/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.3268 - acc: 1.0000 - val_loss: 13.1438 - val_acc: 0.1400\n",
            "Epoch 385/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.3029 - acc: 1.0000 - val_loss: 12.5447 - val_acc: 0.2300\n",
            "Epoch 386/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.2782 - acc: 1.0000 - val_loss: 13.2483 - val_acc: 0.1700\n",
            "Epoch 387/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.2575 - acc: 1.0000 - val_loss: 13.5224 - val_acc: 0.1600\n",
            "Epoch 388/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.2407 - acc: 0.9950 - val_loss: 11.8202 - val_acc: 0.3000\n",
            "Epoch 389/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.2105 - acc: 1.0000 - val_loss: 11.6012 - val_acc: 0.3400\n",
            "Epoch 390/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.1905 - acc: 0.9950 - val_loss: 11.9581 - val_acc: 0.2700\n",
            "Epoch 391/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.1611 - acc: 1.0000 - val_loss: 12.1882 - val_acc: 0.2600\n",
            "Epoch 392/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.1374 - acc: 1.0000 - val_loss: 12.0201 - val_acc: 0.2800\n",
            "Epoch 393/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.1121 - acc: 1.0000 - val_loss: 12.1474 - val_acc: 0.2600\n",
            "Epoch 394/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.0900 - acc: 1.0000 - val_loss: 12.2817 - val_acc: 0.2600\n",
            "Epoch 395/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.0646 - acc: 1.0000 - val_loss: 12.2919 - val_acc: 0.2300\n",
            "Epoch 396/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.0402 - acc: 1.0000 - val_loss: 12.3407 - val_acc: 0.2300\n",
            "Epoch 397/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 9.0168 - acc: 1.0000 - val_loss: 12.3560 - val_acc: 0.2400\n",
            "Epoch 398/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.9933 - acc: 1.0000 - val_loss: 12.3681 - val_acc: 0.2800\n",
            "Epoch 399/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.9698 - acc: 1.0000 - val_loss: 11.7217 - val_acc: 0.3200\n",
            "Epoch 400/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.9449 - acc: 1.0000 - val_loss: 11.5507 - val_acc: 0.3400\n",
            "Epoch 401/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.9329 - acc: 0.9950 - val_loss: 12.1313 - val_acc: 0.2200\n",
            "Epoch 402/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.9006 - acc: 1.0000 - val_loss: 12.2545 - val_acc: 0.2200\n",
            "Epoch 403/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.8744 - acc: 1.0000 - val_loss: 12.1299 - val_acc: 0.1500\n",
            "Epoch 404/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.8511 - acc: 1.0000 - val_loss: 12.0695 - val_acc: 0.1200\n",
            "Epoch 405/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.8283 - acc: 1.0000 - val_loss: 12.0688 - val_acc: 0.1900\n",
            "Epoch 406/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.8170 - acc: 0.9950 - val_loss: 12.0559 - val_acc: 0.1800\n",
            "Epoch 407/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.7808 - acc: 1.0000 - val_loss: 12.0194 - val_acc: 0.1800\n",
            "Epoch 408/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.7587 - acc: 1.0000 - val_loss: 12.2821 - val_acc: 0.2000\n",
            "Epoch 409/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.7350 - acc: 1.0000 - val_loss: 12.1174 - val_acc: 0.1800\n",
            "Epoch 410/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.7114 - acc: 1.0000 - val_loss: 12.4047 - val_acc: 0.2200\n",
            "Epoch 411/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.6880 - acc: 1.0000 - val_loss: 12.2053 - val_acc: 0.1700\n",
            "Epoch 412/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.6638 - acc: 1.0000 - val_loss: 12.1832 - val_acc: 0.2000\n",
            "Epoch 413/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.6404 - acc: 1.0000 - val_loss: 12.2091 - val_acc: 0.1800\n",
            "Epoch 414/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.6164 - acc: 1.0000 - val_loss: 12.2843 - val_acc: 0.1900\n",
            "Epoch 415/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.5928 - acc: 1.0000 - val_loss: 12.2908 - val_acc: 0.1900\n",
            "Epoch 416/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.5699 - acc: 1.0000 - val_loss: 12.1062 - val_acc: 0.2000\n",
            "Epoch 417/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.5456 - acc: 1.0000 - val_loss: 12.1468 - val_acc: 0.2000\n",
            "Epoch 418/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.5331 - acc: 0.9950 - val_loss: 11.5914 - val_acc: 0.2200\n",
            "Epoch 419/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.5024 - acc: 1.0000 - val_loss: 10.6845 - val_acc: 0.3400\n",
            "Epoch 420/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.5423 - acc: 0.9800 - val_loss: 10.8941 - val_acc: 0.2800\n",
            "Epoch 421/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.5064 - acc: 0.9900 - val_loss: 11.4123 - val_acc: 0.2200\n",
            "Epoch 422/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.4795 - acc: 0.9850 - val_loss: 11.2452 - val_acc: 0.2500\n",
            "Epoch 423/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.4595 - acc: 0.9950 - val_loss: 10.7178 - val_acc: 0.3600\n",
            "Epoch 424/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.4233 - acc: 1.0000 - val_loss: 11.2706 - val_acc: 0.3000\n",
            "Epoch 425/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.4066 - acc: 0.9950 - val_loss: 11.6985 - val_acc: 0.2200\n",
            "Epoch 426/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.4016 - acc: 0.9850 - val_loss: 11.0918 - val_acc: 0.3300\n",
            "Epoch 427/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.3821 - acc: 0.9900 - val_loss: 11.4261 - val_acc: 0.2400\n",
            "Epoch 428/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.3658 - acc: 0.9900 - val_loss: 10.6100 - val_acc: 0.3500\n",
            "Epoch 429/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.3564 - acc: 0.9900 - val_loss: 11.8044 - val_acc: 0.2300\n",
            "Epoch 430/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.3453 - acc: 0.9900 - val_loss: 11.1667 - val_acc: 0.2900\n",
            "Epoch 431/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.3503 - acc: 0.9900 - val_loss: 11.8296 - val_acc: 0.1700\n",
            "Epoch 432/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.2686 - acc: 1.0000 - val_loss: 12.1273 - val_acc: 0.1800\n",
            "Epoch 433/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.2506 - acc: 0.9950 - val_loss: 12.1810 - val_acc: 0.1900\n",
            "Epoch 434/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.2281 - acc: 1.0000 - val_loss: 12.4659 - val_acc: 0.1800\n",
            "Epoch 435/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.2088 - acc: 1.0000 - val_loss: 11.9260 - val_acc: 0.2200\n",
            "Epoch 436/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.1869 - acc: 1.0000 - val_loss: 11.6668 - val_acc: 0.2400\n",
            "Epoch 437/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.1657 - acc: 1.0000 - val_loss: 11.7735 - val_acc: 0.2500\n",
            "Epoch 438/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.1454 - acc: 1.0000 - val_loss: 11.9111 - val_acc: 0.2400\n",
            "Epoch 439/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.1291 - acc: 1.0000 - val_loss: 11.8083 - val_acc: 0.2400\n",
            "Epoch 440/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.1346 - acc: 0.9900 - val_loss: 11.9682 - val_acc: 0.2200\n",
            "Epoch 441/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.0894 - acc: 1.0000 - val_loss: 10.9862 - val_acc: 0.3300\n",
            "Epoch 442/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.0696 - acc: 1.0000 - val_loss: 11.0458 - val_acc: 0.3000\n",
            "Epoch 443/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.0528 - acc: 1.0000 - val_loss: 11.2099 - val_acc: 0.2200\n",
            "Epoch 444/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.0399 - acc: 0.9950 - val_loss: 11.6200 - val_acc: 0.2200\n",
            "Epoch 445/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 8.0220 - acc: 0.9900 - val_loss: 11.5123 - val_acc: 0.2200\n",
            "Epoch 446/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.9898 - acc: 1.0000 - val_loss: 11.5760 - val_acc: 0.2300\n",
            "Epoch 447/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.9741 - acc: 1.0000 - val_loss: 11.8086 - val_acc: 0.2300\n",
            "Epoch 448/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.9509 - acc: 1.0000 - val_loss: 11.8115 - val_acc: 0.2300\n",
            "Epoch 449/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.9310 - acc: 1.0000 - val_loss: 11.7116 - val_acc: 0.2200\n",
            "Epoch 450/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.9112 - acc: 1.0000 - val_loss: 11.6087 - val_acc: 0.2300\n",
            "Epoch 451/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.8919 - acc: 1.0000 - val_loss: 11.5214 - val_acc: 0.2400\n",
            "Epoch 452/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.8708 - acc: 1.0000 - val_loss: 11.6638 - val_acc: 0.2500\n",
            "Epoch 453/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.8509 - acc: 1.0000 - val_loss: 11.6452 - val_acc: 0.2400\n",
            "Epoch 454/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.8312 - acc: 1.0000 - val_loss: 11.5984 - val_acc: 0.2400\n",
            "Epoch 455/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.8119 - acc: 1.0000 - val_loss: 11.6186 - val_acc: 0.2500\n",
            "Epoch 456/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.7917 - acc: 1.0000 - val_loss: 11.4979 - val_acc: 0.2700\n",
            "Epoch 457/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.7717 - acc: 1.0000 - val_loss: 11.4819 - val_acc: 0.2700\n",
            "Epoch 458/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.7520 - acc: 1.0000 - val_loss: 11.3236 - val_acc: 0.2800\n",
            "Epoch 459/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.7318 - acc: 1.0000 - val_loss: 11.2549 - val_acc: 0.2900\n",
            "Epoch 460/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.7121 - acc: 1.0000 - val_loss: 11.2804 - val_acc: 0.3000\n",
            "Epoch 461/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.6929 - acc: 1.0000 - val_loss: 11.2390 - val_acc: 0.2900\n",
            "Epoch 462/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.6730 - acc: 1.0000 - val_loss: 10.8862 - val_acc: 0.3000\n",
            "Epoch 463/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.6524 - acc: 1.0000 - val_loss: 10.9149 - val_acc: 0.3000\n",
            "Epoch 464/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.6327 - acc: 1.0000 - val_loss: 10.6634 - val_acc: 0.3000\n",
            "Epoch 465/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.6129 - acc: 1.0000 - val_loss: 10.7232 - val_acc: 0.2700\n",
            "Epoch 466/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.5928 - acc: 1.0000 - val_loss: 10.6332 - val_acc: 0.2800\n",
            "Epoch 467/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.5746 - acc: 1.0000 - val_loss: 10.7499 - val_acc: 0.2800\n",
            "Epoch 468/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.5532 - acc: 1.0000 - val_loss: 10.8750 - val_acc: 0.2600\n",
            "Epoch 469/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.5333 - acc: 1.0000 - val_loss: 10.8350 - val_acc: 0.2700\n",
            "Epoch 470/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.5135 - acc: 1.0000 - val_loss: 10.8925 - val_acc: 0.2500\n",
            "Epoch 471/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4934 - acc: 1.0000 - val_loss: 10.9455 - val_acc: 0.2600\n",
            "Epoch 472/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4737 - acc: 1.0000 - val_loss: 10.9232 - val_acc: 0.2500\n",
            "Epoch 473/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4541 - acc: 1.0000 - val_loss: 10.8243 - val_acc: 0.2500\n",
            "Epoch 474/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4342 - acc: 1.0000 - val_loss: 10.8851 - val_acc: 0.2700\n",
            "Epoch 475/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4183 - acc: 0.9950 - val_loss: 10.6693 - val_acc: 0.2700\n",
            "Epoch 476/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4061 - acc: 0.9950 - val_loss: 10.6265 - val_acc: 0.3000\n",
            "Epoch 477/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4248 - acc: 0.9900 - val_loss: 10.9052 - val_acc: 0.2600\n",
            "Epoch 478/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4198 - acc: 0.9850 - val_loss: 11.4011 - val_acc: 0.2600\n",
            "Epoch 479/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4344 - acc: 0.9750 - val_loss: 10.8578 - val_acc: 0.3100\n",
            "Epoch 480/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4630 - acc: 0.9550 - val_loss: 13.2576 - val_acc: 0.2100\n",
            "Epoch 481/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.6150 - acc: 0.9550 - val_loss: 14.3346 - val_acc: 0.2000\n",
            "Epoch 482/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4526 - acc: 0.9500 - val_loss: 12.4112 - val_acc: 0.2700\n",
            "Epoch 483/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4594 - acc: 0.9600 - val_loss: 13.9308 - val_acc: 0.1600\n",
            "Epoch 484/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.4063 - acc: 0.9600 - val_loss: 11.8244 - val_acc: 0.2000\n",
            "Epoch 485/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.3400 - acc: 0.9800 - val_loss: 11.5345 - val_acc: 0.2200\n",
            "Epoch 486/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.3184 - acc: 0.9850 - val_loss: 11.2353 - val_acc: 0.1500\n",
            "Epoch 487/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.2809 - acc: 0.9850 - val_loss: 10.6135 - val_acc: 0.1700\n",
            "Epoch 488/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.2475 - acc: 0.9900 - val_loss: 11.0148 - val_acc: 0.1600\n",
            "Epoch 489/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.2434 - acc: 0.9800 - val_loss: 11.5952 - val_acc: 0.2000\n",
            "Epoch 490/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.2336 - acc: 0.9850 - val_loss: 11.8061 - val_acc: 0.1300\n",
            "Epoch 491/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.2393 - acc: 0.9800 - val_loss: 11.7416 - val_acc: 0.1800\n",
            "Epoch 492/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.1822 - acc: 0.9950 - val_loss: 11.6632 - val_acc: 0.2500\n",
            "Epoch 493/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.1823 - acc: 0.9850 - val_loss: 11.9910 - val_acc: 0.2400\n",
            "Epoch 494/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.1768 - acc: 0.9950 - val_loss: 11.3175 - val_acc: 0.2300\n",
            "Epoch 495/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.1698 - acc: 0.9900 - val_loss: 10.2544 - val_acc: 0.2900\n",
            "Epoch 496/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.1335 - acc: 0.9900 - val_loss: 10.7048 - val_acc: 0.2200\n",
            "Epoch 497/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.1105 - acc: 0.9950 - val_loss: 10.6643 - val_acc: 0.2800\n",
            "Epoch 498/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.0931 - acc: 0.9900 - val_loss: 10.7398 - val_acc: 0.2400\n",
            "Epoch 499/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.0830 - acc: 0.9900 - val_loss: 10.7428 - val_acc: 0.2800\n",
            "Epoch 500/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.0463 - acc: 1.0000 - val_loss: 10.9739 - val_acc: 0.2200\n",
            "Epoch 501/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.0370 - acc: 0.9950 - val_loss: 10.8649 - val_acc: 0.1900\n",
            "Epoch 502/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.0200 - acc: 0.9950 - val_loss: 10.8866 - val_acc: 0.2000\n",
            "Epoch 503/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.9987 - acc: 1.0000 - val_loss: 10.1193 - val_acc: 0.3400\n",
            "Epoch 504/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 7.0183 - acc: 0.9850 - val_loss: 10.9869 - val_acc: 0.2100\n",
            "Epoch 505/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.9910 - acc: 0.9950 - val_loss: 12.3363 - val_acc: 0.1500\n",
            "Epoch 506/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.9761 - acc: 0.9850 - val_loss: 11.9219 - val_acc: 0.2200\n",
            "Epoch 507/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.9474 - acc: 1.0000 - val_loss: 11.8728 - val_acc: 0.2400\n",
            "Epoch 508/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.9384 - acc: 0.9950 - val_loss: 11.4855 - val_acc: 0.2400\n",
            "Epoch 509/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.9167 - acc: 0.9950 - val_loss: 10.2953 - val_acc: 0.2600\n",
            "Epoch 510/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.9663 - acc: 0.9800 - val_loss: 9.8975 - val_acc: 0.3400\n",
            "Epoch 511/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.9956 - acc: 0.9750 - val_loss: 10.3827 - val_acc: 0.3600\n",
            "Epoch 512/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.8730 - acc: 1.0000 - val_loss: 11.8493 - val_acc: 0.2300\n",
            "Epoch 513/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.9078 - acc: 0.9800 - val_loss: 11.2116 - val_acc: 0.2700\n",
            "Epoch 514/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.8534 - acc: 0.9900 - val_loss: 10.6608 - val_acc: 0.3000\n",
            "Epoch 515/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.8810 - acc: 0.9850 - val_loss: 10.4240 - val_acc: 0.3600\n",
            "Epoch 516/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.8341 - acc: 0.9900 - val_loss: 11.0811 - val_acc: 0.3100\n",
            "Epoch 517/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.8223 - acc: 0.9900 - val_loss: 10.0913 - val_acc: 0.3900\n",
            "Epoch 518/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.7930 - acc: 1.0000 - val_loss: 9.2347 - val_acc: 0.5200\n",
            "Epoch 519/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.7916 - acc: 0.9900 - val_loss: 9.6999 - val_acc: 0.4200\n",
            "Epoch 520/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.7857 - acc: 0.9950 - val_loss: 10.1084 - val_acc: 0.3300\n",
            "Epoch 521/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.8023 - acc: 0.9800 - val_loss: 10.0100 - val_acc: 0.3800\n",
            "Epoch 522/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.7280 - acc: 1.0000 - val_loss: 10.6047 - val_acc: 0.3200\n",
            "Epoch 523/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.7176 - acc: 0.9950 - val_loss: 10.7566 - val_acc: 0.3100\n",
            "Epoch 524/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.6980 - acc: 1.0000 - val_loss: 10.9982 - val_acc: 0.2900\n",
            "Epoch 525/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.6837 - acc: 1.0000 - val_loss: 11.0650 - val_acc: 0.2900\n",
            "Epoch 526/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.6653 - acc: 1.0000 - val_loss: 11.0248 - val_acc: 0.3000\n",
            "Epoch 527/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.6547 - acc: 1.0000 - val_loss: 11.0513 - val_acc: 0.2500\n",
            "Epoch 528/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.6379 - acc: 1.0000 - val_loss: 10.8173 - val_acc: 0.3600\n",
            "Epoch 529/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.6205 - acc: 1.0000 - val_loss: 10.7897 - val_acc: 0.3400\n",
            "Epoch 530/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.6082 - acc: 1.0000 - val_loss: 10.7279 - val_acc: 0.3500\n",
            "Epoch 531/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.5914 - acc: 1.0000 - val_loss: 10.8648 - val_acc: 0.3600\n",
            "Epoch 532/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.5753 - acc: 1.0000 - val_loss: 11.0966 - val_acc: 0.3100\n",
            "Epoch 533/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.5609 - acc: 1.0000 - val_loss: 11.2424 - val_acc: 0.3000\n",
            "Epoch 534/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.5452 - acc: 1.0000 - val_loss: 11.2606 - val_acc: 0.2900\n",
            "Epoch 535/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.5302 - acc: 1.0000 - val_loss: 11.1131 - val_acc: 0.2900\n",
            "Epoch 536/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.5153 - acc: 1.0000 - val_loss: 11.2257 - val_acc: 0.2800\n",
            "Epoch 537/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.5003 - acc: 1.0000 - val_loss: 11.2524 - val_acc: 0.2700\n",
            "Epoch 538/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.4858 - acc: 1.0000 - val_loss: 11.2149 - val_acc: 0.2700\n",
            "Epoch 539/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.4700 - acc: 1.0000 - val_loss: 11.1330 - val_acc: 0.2600\n",
            "Epoch 540/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.4549 - acc: 1.0000 - val_loss: 10.9526 - val_acc: 0.2800\n",
            "Epoch 541/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.4403 - acc: 1.0000 - val_loss: 11.2183 - val_acc: 0.2500\n",
            "Epoch 542/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.4246 - acc: 1.0000 - val_loss: 10.9713 - val_acc: 0.2700\n",
            "Epoch 543/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.4111 - acc: 1.0000 - val_loss: 10.9650 - val_acc: 0.2800\n",
            "Epoch 544/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.3944 - acc: 1.0000 - val_loss: 11.0436 - val_acc: 0.2700\n",
            "Epoch 545/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.3796 - acc: 1.0000 - val_loss: 11.1703 - val_acc: 0.2700\n",
            "Epoch 546/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.3642 - acc: 1.0000 - val_loss: 11.2189 - val_acc: 0.2700\n",
            "Epoch 547/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.3496 - acc: 1.0000 - val_loss: 11.3990 - val_acc: 0.2300\n",
            "Epoch 548/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.3357 - acc: 1.0000 - val_loss: 11.1370 - val_acc: 0.2400\n",
            "Epoch 549/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.3188 - acc: 1.0000 - val_loss: 10.8895 - val_acc: 0.2400\n",
            "Epoch 550/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.3047 - acc: 1.0000 - val_loss: 10.6908 - val_acc: 0.2500\n",
            "Epoch 551/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.2888 - acc: 1.0000 - val_loss: 10.7707 - val_acc: 0.2500\n",
            "Epoch 552/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.2735 - acc: 1.0000 - val_loss: 10.5967 - val_acc: 0.3000\n",
            "Epoch 553/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.2597 - acc: 1.0000 - val_loss: 10.6565 - val_acc: 0.2200\n",
            "Epoch 554/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.2437 - acc: 1.0000 - val_loss: 10.9859 - val_acc: 0.2000\n",
            "Epoch 555/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.2311 - acc: 1.0000 - val_loss: 11.1654 - val_acc: 0.2100\n",
            "Epoch 556/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.2126 - acc: 1.0000 - val_loss: 11.4076 - val_acc: 0.2300\n",
            "Epoch 557/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.1976 - acc: 1.0000 - val_loss: 11.2441 - val_acc: 0.2200\n",
            "Epoch 558/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.1825 - acc: 1.0000 - val_loss: 11.5916 - val_acc: 0.2000\n",
            "Epoch 559/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.1673 - acc: 1.0000 - val_loss: 11.3428 - val_acc: 0.2400\n",
            "Epoch 560/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.1520 - acc: 1.0000 - val_loss: 11.3372 - val_acc: 0.2400\n",
            "Epoch 561/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.1382 - acc: 1.0000 - val_loss: 11.1684 - val_acc: 0.2600\n",
            "Epoch 562/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.1235 - acc: 1.0000 - val_loss: 10.5936 - val_acc: 0.2800\n",
            "Epoch 563/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.1067 - acc: 1.0000 - val_loss: 10.3984 - val_acc: 0.2700\n",
            "Epoch 564/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.0913 - acc: 1.0000 - val_loss: 10.2502 - val_acc: 0.3100\n",
            "Epoch 565/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.0767 - acc: 1.0000 - val_loss: 9.7751 - val_acc: 0.3400\n",
            "Epoch 566/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.0631 - acc: 1.0000 - val_loss: 10.0611 - val_acc: 0.2900\n",
            "Epoch 567/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.0489 - acc: 1.0000 - val_loss: 10.2248 - val_acc: 0.2800\n",
            "Epoch 568/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.0314 - acc: 1.0000 - val_loss: 10.3211 - val_acc: 0.2800\n",
            "Epoch 569/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 6.0150 - acc: 1.0000 - val_loss: 10.6396 - val_acc: 0.2700\n",
            "Epoch 570/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.9997 - acc: 1.0000 - val_loss: 10.6131 - val_acc: 0.2600\n",
            "Epoch 571/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.9849 - acc: 1.0000 - val_loss: 10.7039 - val_acc: 0.2600\n",
            "Epoch 572/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.9694 - acc: 1.0000 - val_loss: 10.7461 - val_acc: 0.2500\n",
            "Epoch 573/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.9540 - acc: 1.0000 - val_loss: 10.7365 - val_acc: 0.2600\n",
            "Epoch 574/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.9386 - acc: 1.0000 - val_loss: 10.5235 - val_acc: 0.2600\n",
            "Epoch 575/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.9234 - acc: 1.0000 - val_loss: 10.6763 - val_acc: 0.2500\n",
            "Epoch 576/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.9082 - acc: 1.0000 - val_loss: 10.7149 - val_acc: 0.2500\n",
            "Epoch 577/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.8929 - acc: 1.0000 - val_loss: 10.6632 - val_acc: 0.2600\n",
            "Epoch 578/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.8773 - acc: 1.0000 - val_loss: 10.5823 - val_acc: 0.2600\n",
            "Epoch 579/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.8621 - acc: 1.0000 - val_loss: 10.6168 - val_acc: 0.2600\n",
            "Epoch 580/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.8473 - acc: 1.0000 - val_loss: 10.5396 - val_acc: 0.2600\n",
            "Epoch 581/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.8318 - acc: 1.0000 - val_loss: 10.6655 - val_acc: 0.2400\n",
            "Epoch 582/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.8161 - acc: 1.0000 - val_loss: 10.8351 - val_acc: 0.2500\n",
            "Epoch 583/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.8008 - acc: 1.0000 - val_loss: 10.6989 - val_acc: 0.2600\n",
            "Epoch 584/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.7855 - acc: 1.0000 - val_loss: 10.7546 - val_acc: 0.2500\n",
            "Epoch 585/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.8417 - acc: 0.9850 - val_loss: 10.3493 - val_acc: 0.2800\n",
            "Epoch 586/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.8320 - acc: 0.9800 - val_loss: 9.2244 - val_acc: 0.2900\n",
            "Epoch 587/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.8611 - acc: 0.9800 - val_loss: 8.9789 - val_acc: 0.3000\n",
            "Epoch 588/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.8063 - acc: 0.9800 - val_loss: 8.5347 - val_acc: 0.3400\n",
            "Epoch 589/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.7562 - acc: 0.9900 - val_loss: 8.2491 - val_acc: 0.4300\n",
            "Epoch 590/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.7200 - acc: 0.9950 - val_loss: 8.7283 - val_acc: 0.4000\n",
            "Epoch 591/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.7123 - acc: 0.9900 - val_loss: 8.6602 - val_acc: 0.4400\n",
            "Epoch 592/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.6971 - acc: 0.9950 - val_loss: 8.1197 - val_acc: 0.4900\n",
            "Epoch 593/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.6751 - acc: 1.0000 - val_loss: 8.0853 - val_acc: 0.5200\n",
            "Epoch 594/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.6603 - acc: 1.0000 - val_loss: 8.2682 - val_acc: 0.5100\n",
            "Epoch 595/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.6661 - acc: 0.9950 - val_loss: 8.1548 - val_acc: 0.4700\n",
            "Epoch 596/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.6295 - acc: 1.0000 - val_loss: 8.4337 - val_acc: 0.4200\n",
            "Epoch 597/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.6173 - acc: 1.0000 - val_loss: 8.4070 - val_acc: 0.4300\n",
            "Epoch 598/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.6073 - acc: 1.0000 - val_loss: 8.6205 - val_acc: 0.3700\n",
            "Epoch 599/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.5863 - acc: 1.0000 - val_loss: 8.5526 - val_acc: 0.4300\n",
            "Epoch 600/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.5818 - acc: 0.9950 - val_loss: 8.7947 - val_acc: 0.4200\n",
            "Epoch 601/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.5654 - acc: 0.9950 - val_loss: 8.8914 - val_acc: 0.4200\n",
            "Epoch 602/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.5589 - acc: 0.9900 - val_loss: 9.7918 - val_acc: 0.3300\n",
            "Epoch 603/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.5379 - acc: 0.9950 - val_loss: 10.4614 - val_acc: 0.2800\n",
            "Epoch 604/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.5168 - acc: 1.0000 - val_loss: 11.0006 - val_acc: 0.2400\n",
            "Epoch 605/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.5023 - acc: 1.0000 - val_loss: 11.0755 - val_acc: 0.2300\n",
            "Epoch 606/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.4900 - acc: 1.0000 - val_loss: 11.0585 - val_acc: 0.2100\n",
            "Epoch 607/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.4743 - acc: 1.0000 - val_loss: 10.8637 - val_acc: 0.2100\n",
            "Epoch 608/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.4631 - acc: 1.0000 - val_loss: 10.5889 - val_acc: 0.2400\n",
            "Epoch 609/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.4507 - acc: 1.0000 - val_loss: 9.7262 - val_acc: 0.2900\n",
            "Epoch 610/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.4351 - acc: 1.0000 - val_loss: 9.6766 - val_acc: 0.3100\n",
            "Epoch 611/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.4187 - acc: 1.0000 - val_loss: 9.7750 - val_acc: 0.3100\n",
            "Epoch 612/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.4064 - acc: 1.0000 - val_loss: 10.0025 - val_acc: 0.3000\n",
            "Epoch 613/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.3913 - acc: 1.0000 - val_loss: 9.8572 - val_acc: 0.3000\n",
            "Epoch 614/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.3799 - acc: 1.0000 - val_loss: 9.4225 - val_acc: 0.4000\n",
            "Epoch 615/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.3911 - acc: 0.9950 - val_loss: 9.2531 - val_acc: 0.4000\n",
            "Epoch 616/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.3513 - acc: 1.0000 - val_loss: 9.5984 - val_acc: 0.3700\n",
            "Epoch 617/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.3546 - acc: 0.9950 - val_loss: 9.4978 - val_acc: 0.3400\n",
            "Epoch 618/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.3233 - acc: 1.0000 - val_loss: 9.7227 - val_acc: 0.3500\n",
            "Epoch 619/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.3123 - acc: 0.9950 - val_loss: 9.4245 - val_acc: 0.3800\n",
            "Epoch 620/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.2931 - acc: 1.0000 - val_loss: 8.7876 - val_acc: 0.4100\n",
            "Epoch 621/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.2791 - acc: 1.0000 - val_loss: 8.9399 - val_acc: 0.4200\n",
            "Epoch 622/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.2648 - acc: 1.0000 - val_loss: 8.8788 - val_acc: 0.4100\n",
            "Epoch 623/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.2509 - acc: 1.0000 - val_loss: 8.7521 - val_acc: 0.4200\n",
            "Epoch 624/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.2370 - acc: 1.0000 - val_loss: 8.8319 - val_acc: 0.4100\n",
            "Epoch 625/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.2226 - acc: 1.0000 - val_loss: 8.9566 - val_acc: 0.4300\n",
            "Epoch 626/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.2080 - acc: 1.0000 - val_loss: 9.0553 - val_acc: 0.4200\n",
            "Epoch 627/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.1949 - acc: 1.0000 - val_loss: 8.8391 - val_acc: 0.4200\n",
            "Epoch 628/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.1797 - acc: 1.0000 - val_loss: 8.9431 - val_acc: 0.4200\n",
            "Epoch 629/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.1653 - acc: 1.0000 - val_loss: 8.8295 - val_acc: 0.4200\n",
            "Epoch 630/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.1514 - acc: 1.0000 - val_loss: 8.9303 - val_acc: 0.4100\n",
            "Epoch 631/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.1398 - acc: 1.0000 - val_loss: 8.6846 - val_acc: 0.4300\n",
            "Epoch 632/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.1261 - acc: 1.0000 - val_loss: 7.8361 - val_acc: 0.4800\n",
            "Epoch 633/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.1161 - acc: 1.0000 - val_loss: 8.0154 - val_acc: 0.4800\n",
            "Epoch 634/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.1407 - acc: 0.9800 - val_loss: 8.5630 - val_acc: 0.4000\n",
            "Epoch 635/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.0931 - acc: 0.9950 - val_loss: 8.3966 - val_acc: 0.4200\n",
            "Epoch 636/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.0734 - acc: 1.0000 - val_loss: 8.5101 - val_acc: 0.3700\n",
            "Epoch 637/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.0586 - acc: 1.0000 - val_loss: 8.6398 - val_acc: 0.3700\n",
            "Epoch 638/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.0458 - acc: 1.0000 - val_loss: 8.5369 - val_acc: 0.3700\n",
            "Epoch 639/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.0294 - acc: 1.0000 - val_loss: 8.3163 - val_acc: 0.3900\n",
            "Epoch 640/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.0158 - acc: 1.0000 - val_loss: 8.4506 - val_acc: 0.3900\n",
            "Epoch 641/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 5.0049 - acc: 1.0000 - val_loss: 8.5448 - val_acc: 0.3800\n",
            "Epoch 642/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.9963 - acc: 0.9950 - val_loss: 8.7412 - val_acc: 0.3700\n",
            "Epoch 643/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.9796 - acc: 1.0000 - val_loss: 8.1543 - val_acc: 0.4600\n",
            "Epoch 644/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.9686 - acc: 0.9950 - val_loss: 8.8345 - val_acc: 0.3700\n",
            "Epoch 645/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.9600 - acc: 0.9950 - val_loss: 8.8319 - val_acc: 0.3200\n",
            "Epoch 646/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.9385 - acc: 1.0000 - val_loss: 8.8841 - val_acc: 0.3100\n",
            "Epoch 647/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.9245 - acc: 1.0000 - val_loss: 9.0753 - val_acc: 0.3000\n",
            "Epoch 648/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.9091 - acc: 1.0000 - val_loss: 9.0660 - val_acc: 0.3000\n",
            "Epoch 649/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.8978 - acc: 1.0000 - val_loss: 8.8646 - val_acc: 0.3300\n",
            "Epoch 650/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.8815 - acc: 1.0000 - val_loss: 8.3879 - val_acc: 0.3600\n",
            "Epoch 651/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.8694 - acc: 1.0000 - val_loss: 8.5519 - val_acc: 0.3500\n",
            "Epoch 652/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.8578 - acc: 0.9950 - val_loss: 8.5895 - val_acc: 0.3300\n",
            "Epoch 653/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.8400 - acc: 1.0000 - val_loss: 8.5608 - val_acc: 0.3300\n",
            "Epoch 654/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.8359 - acc: 1.0000 - val_loss: 8.1971 - val_acc: 0.3600\n",
            "Epoch 655/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.8181 - acc: 0.9950 - val_loss: 7.8659 - val_acc: 0.3700\n",
            "Epoch 656/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.8227 - acc: 0.9900 - val_loss: 7.9184 - val_acc: 0.3700\n",
            "Epoch 657/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.7962 - acc: 1.0000 - val_loss: 8.8316 - val_acc: 0.2800\n",
            "Epoch 658/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.8475 - acc: 0.9700 - val_loss: 7.7532 - val_acc: 0.3900\n",
            "Epoch 659/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.7746 - acc: 1.0000 - val_loss: 7.4771 - val_acc: 0.4900\n",
            "Epoch 660/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.7872 - acc: 0.9850 - val_loss: 7.3503 - val_acc: 0.5100\n",
            "Epoch 661/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.7481 - acc: 1.0000 - val_loss: 7.7697 - val_acc: 0.4200\n",
            "Epoch 662/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.7942 - acc: 0.9750 - val_loss: 7.7273 - val_acc: 0.4300\n",
            "Epoch 663/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.7426 - acc: 0.9950 - val_loss: 6.7976 - val_acc: 0.4900\n",
            "Epoch 664/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.7639 - acc: 0.9750 - val_loss: 6.9723 - val_acc: 0.5200\n",
            "Epoch 665/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.7580 - acc: 0.9800 - val_loss: 6.7200 - val_acc: 0.6000\n",
            "Epoch 666/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.7463 - acc: 0.9700 - val_loss: 6.6541 - val_acc: 0.6300\n",
            "Epoch 667/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.7705 - acc: 0.9750 - val_loss: 7.6685 - val_acc: 0.4700\n",
            "Epoch 668/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.6870 - acc: 0.9900 - val_loss: 7.9252 - val_acc: 0.4000\n",
            "Epoch 669/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.6781 - acc: 0.9900 - val_loss: 8.1390 - val_acc: 0.4200\n",
            "Epoch 670/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.6803 - acc: 0.9850 - val_loss: 7.3502 - val_acc: 0.5000\n",
            "Epoch 671/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.6427 - acc: 0.9950 - val_loss: 7.0621 - val_acc: 0.5200\n",
            "Epoch 672/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.6236 - acc: 1.0000 - val_loss: 6.8925 - val_acc: 0.5300\n",
            "Epoch 673/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.6102 - acc: 1.0000 - val_loss: 7.2136 - val_acc: 0.5100\n",
            "Epoch 674/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.5967 - acc: 1.0000 - val_loss: 7.4783 - val_acc: 0.4900\n",
            "Epoch 675/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.5957 - acc: 0.9950 - val_loss: 7.4676 - val_acc: 0.4900\n",
            "Epoch 676/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.5735 - acc: 1.0000 - val_loss: 7.6811 - val_acc: 0.5300\n",
            "Epoch 677/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.5921 - acc: 0.9800 - val_loss: 7.8017 - val_acc: 0.4900\n",
            "Epoch 678/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.5634 - acc: 0.9950 - val_loss: 6.9605 - val_acc: 0.5100\n",
            "Epoch 679/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.5603 - acc: 0.9950 - val_loss: 7.9225 - val_acc: 0.2800\n",
            "Epoch 680/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.5377 - acc: 1.0000 - val_loss: 8.2932 - val_acc: 0.2300\n",
            "Epoch 681/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.5254 - acc: 0.9950 - val_loss: 7.9897 - val_acc: 0.2700\n",
            "Epoch 682/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.5042 - acc: 1.0000 - val_loss: 7.5233 - val_acc: 0.4200\n",
            "Epoch 683/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.4914 - acc: 1.0000 - val_loss: 7.7998 - val_acc: 0.4100\n",
            "Epoch 684/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.4835 - acc: 1.0000 - val_loss: 7.4909 - val_acc: 0.4600\n",
            "Epoch 685/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.4672 - acc: 1.0000 - val_loss: 7.8239 - val_acc: 0.4000\n",
            "Epoch 686/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.4565 - acc: 1.0000 - val_loss: 7.7599 - val_acc: 0.4400\n",
            "Epoch 687/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.4610 - acc: 0.9950 - val_loss: 7.6250 - val_acc: 0.4300\n",
            "Epoch 688/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.4373 - acc: 1.0000 - val_loss: 7.2925 - val_acc: 0.4800\n",
            "Epoch 689/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.4613 - acc: 0.9950 - val_loss: 7.8609 - val_acc: 0.3900\n",
            "Epoch 690/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.4104 - acc: 1.0000 - val_loss: 8.3211 - val_acc: 0.3400\n",
            "Epoch 691/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.4044 - acc: 0.9950 - val_loss: 8.1289 - val_acc: 0.4100\n",
            "Epoch 692/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.3930 - acc: 0.9950 - val_loss: 8.2202 - val_acc: 0.3900\n",
            "Epoch 693/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.3874 - acc: 0.9900 - val_loss: 7.8841 - val_acc: 0.3900\n",
            "Epoch 694/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.3875 - acc: 0.9900 - val_loss: 7.0797 - val_acc: 0.4400\n",
            "Epoch 695/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.3825 - acc: 0.9950 - val_loss: 6.7781 - val_acc: 0.5200\n",
            "Epoch 696/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.3695 - acc: 0.9850 - val_loss: 5.5696 - val_acc: 0.7100\n",
            "Epoch 697/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.4179 - acc: 0.9700 - val_loss: 6.5422 - val_acc: 0.5300\n",
            "Epoch 698/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.3614 - acc: 0.9900 - val_loss: 8.2442 - val_acc: 0.3100\n",
            "Epoch 699/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.3585 - acc: 0.9750 - val_loss: 7.5445 - val_acc: 0.4000\n",
            "Epoch 700/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.3169 - acc: 0.9950 - val_loss: 6.7285 - val_acc: 0.5300\n",
            "Epoch 701/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.2942 - acc: 1.0000 - val_loss: 6.5000 - val_acc: 0.5900\n",
            "Epoch 702/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.2795 - acc: 1.0000 - val_loss: 6.5781 - val_acc: 0.5900\n",
            "Epoch 703/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.2710 - acc: 1.0000 - val_loss: 6.6547 - val_acc: 0.5800\n",
            "Epoch 704/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.2572 - acc: 1.0000 - val_loss: 6.6964 - val_acc: 0.5400\n",
            "Epoch 705/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.2450 - acc: 1.0000 - val_loss: 6.8649 - val_acc: 0.5500\n",
            "Epoch 706/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.2349 - acc: 1.0000 - val_loss: 6.9632 - val_acc: 0.5500\n",
            "Epoch 707/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.2223 - acc: 1.0000 - val_loss: 6.6718 - val_acc: 0.5900\n",
            "Epoch 708/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.2102 - acc: 1.0000 - val_loss: 6.6329 - val_acc: 0.5900\n",
            "Epoch 709/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.1993 - acc: 1.0000 - val_loss: 6.5277 - val_acc: 0.6100\n",
            "Epoch 710/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.1875 - acc: 1.0000 - val_loss: 6.3709 - val_acc: 0.6100\n",
            "Epoch 711/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.1791 - acc: 1.0000 - val_loss: 6.4573 - val_acc: 0.6100\n",
            "Epoch 712/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.1676 - acc: 1.0000 - val_loss: 6.3984 - val_acc: 0.6200\n",
            "Epoch 713/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.2060 - acc: 0.9900 - val_loss: 6.5496 - val_acc: 0.5800\n",
            "Epoch 714/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.1553 - acc: 0.9950 - val_loss: 6.7499 - val_acc: 0.5500\n",
            "Epoch 715/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.1374 - acc: 0.9950 - val_loss: 6.9317 - val_acc: 0.5400\n",
            "Epoch 716/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.1210 - acc: 1.0000 - val_loss: 7.1265 - val_acc: 0.5000\n",
            "Epoch 717/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.1094 - acc: 1.0000 - val_loss: 6.9720 - val_acc: 0.5400\n",
            "Epoch 718/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.0976 - acc: 1.0000 - val_loss: 7.1118 - val_acc: 0.4800\n",
            "Epoch 719/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.0880 - acc: 1.0000 - val_loss: 6.8836 - val_acc: 0.5400\n",
            "Epoch 720/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.0740 - acc: 1.0000 - val_loss: 7.0512 - val_acc: 0.5300\n",
            "Epoch 721/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.0625 - acc: 1.0000 - val_loss: 6.7567 - val_acc: 0.5300\n",
            "Epoch 722/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.0573 - acc: 0.9950 - val_loss: 6.8276 - val_acc: 0.5400\n",
            "Epoch 723/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.0397 - acc: 1.0000 - val_loss: 6.7625 - val_acc: 0.5900\n",
            "Epoch 724/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.0318 - acc: 1.0000 - val_loss: 6.9324 - val_acc: 0.6000\n",
            "Epoch 725/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.0379 - acc: 0.9900 - val_loss: 6.9261 - val_acc: 0.5700\n",
            "Epoch 726/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.0143 - acc: 1.0000 - val_loss: 7.0822 - val_acc: 0.4600\n",
            "Epoch 727/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.0375 - acc: 0.9850 - val_loss: 6.6361 - val_acc: 0.5000\n",
            "Epoch 728/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9914 - acc: 1.0000 - val_loss: 7.8151 - val_acc: 0.3700\n",
            "Epoch 729/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 4.0033 - acc: 0.9900 - val_loss: 8.1907 - val_acc: 0.3600\n",
            "Epoch 730/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9940 - acc: 0.9900 - val_loss: 8.0462 - val_acc: 0.3900\n",
            "Epoch 731/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9599 - acc: 1.0000 - val_loss: 8.8518 - val_acc: 0.3500\n",
            "Epoch 732/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9613 - acc: 0.9950 - val_loss: 8.2279 - val_acc: 0.3600\n",
            "Epoch 733/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9508 - acc: 0.9900 - val_loss: 9.3381 - val_acc: 0.2600\n",
            "Epoch 734/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9475 - acc: 0.9850 - val_loss: 7.9259 - val_acc: 0.3400\n",
            "Epoch 735/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9288 - acc: 0.9900 - val_loss: 7.6594 - val_acc: 0.3000\n",
            "Epoch 736/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9269 - acc: 0.9950 - val_loss: 7.8537 - val_acc: 0.2600\n",
            "Epoch 737/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9103 - acc: 0.9950 - val_loss: 7.6418 - val_acc: 0.3200\n",
            "Epoch 738/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.8913 - acc: 1.0000 - val_loss: 7.1755 - val_acc: 0.4100\n",
            "Epoch 739/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.8792 - acc: 0.9950 - val_loss: 6.7519 - val_acc: 0.4800\n",
            "Epoch 740/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.8780 - acc: 0.9950 - val_loss: 6.5383 - val_acc: 0.5400\n",
            "Epoch 741/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.8575 - acc: 0.9950 - val_loss: 6.6650 - val_acc: 0.4800\n",
            "Epoch 742/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9166 - acc: 0.9750 - val_loss: 7.0498 - val_acc: 0.3500\n",
            "Epoch 743/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.8694 - acc: 0.9900 - val_loss: 7.1704 - val_acc: 0.2200\n",
            "Epoch 744/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9256 - acc: 0.9800 - val_loss: 8.8188 - val_acc: 0.0300\n",
            "Epoch 745/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.8595 - acc: 0.9900 - val_loss: 7.6515 - val_acc: 0.1700\n",
            "Epoch 746/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.9295 - acc: 0.9600 - val_loss: 6.8085 - val_acc: 0.3300\n",
            "Epoch 747/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.8927 - acc: 0.9800 - val_loss: 6.2342 - val_acc: 0.4000\n",
            "Epoch 748/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.8069 - acc: 1.0000 - val_loss: 5.9048 - val_acc: 0.5300\n",
            "Epoch 749/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.8068 - acc: 0.9950 - val_loss: 6.5489 - val_acc: 0.4700\n",
            "Epoch 750/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.8117 - acc: 0.9900 - val_loss: 7.0366 - val_acc: 0.4200\n",
            "Epoch 751/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.7776 - acc: 1.0000 - val_loss: 7.5954 - val_acc: 0.3600\n",
            "Epoch 752/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.8014 - acc: 0.9900 - val_loss: 6.6158 - val_acc: 0.4600\n",
            "Epoch 753/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.7706 - acc: 0.9900 - val_loss: 6.3248 - val_acc: 0.5100\n",
            "Epoch 754/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.7564 - acc: 0.9900 - val_loss: 6.3366 - val_acc: 0.5200\n",
            "Epoch 755/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.7344 - acc: 1.0000 - val_loss: 6.8867 - val_acc: 0.4200\n",
            "Epoch 756/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.7460 - acc: 0.9950 - val_loss: 6.8678 - val_acc: 0.4300\n",
            "Epoch 757/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.7468 - acc: 0.9950 - val_loss: 6.2604 - val_acc: 0.4900\n",
            "Epoch 758/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.7176 - acc: 0.9950 - val_loss: 5.8551 - val_acc: 0.6000\n",
            "Epoch 759/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.7082 - acc: 0.9950 - val_loss: 5.7055 - val_acc: 0.6300\n",
            "Epoch 760/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.7275 - acc: 0.9800 - val_loss: 5.4541 - val_acc: 0.6400\n",
            "Epoch 761/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.6995 - acc: 0.9900 - val_loss: 5.4565 - val_acc: 0.6700\n",
            "Epoch 762/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.6702 - acc: 1.0000 - val_loss: 5.3892 - val_acc: 0.6400\n",
            "Epoch 763/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.6951 - acc: 0.9900 - val_loss: 5.5991 - val_acc: 0.6500\n",
            "Epoch 764/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.6505 - acc: 1.0000 - val_loss: 5.7674 - val_acc: 0.6700\n",
            "Epoch 765/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.6538 - acc: 1.0000 - val_loss: 5.7721 - val_acc: 0.5900\n",
            "Epoch 766/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.6434 - acc: 0.9950 - val_loss: 6.0866 - val_acc: 0.5300\n",
            "Epoch 767/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.6221 - acc: 1.0000 - val_loss: 6.4941 - val_acc: 0.5300\n",
            "Epoch 768/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.6296 - acc: 0.9900 - val_loss: 6.4078 - val_acc: 0.5100\n",
            "Epoch 769/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.6044 - acc: 1.0000 - val_loss: 6.6452 - val_acc: 0.4400\n",
            "Epoch 770/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.6081 - acc: 0.9950 - val_loss: 6.9227 - val_acc: 0.3600\n",
            "Epoch 771/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.5871 - acc: 1.0000 - val_loss: 7.0203 - val_acc: 0.2900\n",
            "Epoch 772/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.5871 - acc: 0.9950 - val_loss: 6.6256 - val_acc: 0.3100\n",
            "Epoch 773/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.5709 - acc: 0.9950 - val_loss: 6.2204 - val_acc: 0.3900\n",
            "Epoch 774/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.5613 - acc: 1.0000 - val_loss: 6.2354 - val_acc: 0.3900\n",
            "Epoch 775/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.5509 - acc: 1.0000 - val_loss: 6.0871 - val_acc: 0.4800\n",
            "Epoch 776/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.5417 - acc: 1.0000 - val_loss: 6.2468 - val_acc: 0.4600\n",
            "Epoch 777/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.5346 - acc: 1.0000 - val_loss: 6.3007 - val_acc: 0.4300\n",
            "Epoch 778/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.5214 - acc: 1.0000 - val_loss: 6.1293 - val_acc: 0.4500\n",
            "Epoch 779/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.5120 - acc: 1.0000 - val_loss: 5.9223 - val_acc: 0.5000\n",
            "Epoch 780/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.4998 - acc: 1.0000 - val_loss: 6.0943 - val_acc: 0.4600\n",
            "Epoch 781/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.4907 - acc: 1.0000 - val_loss: 5.9155 - val_acc: 0.5100\n",
            "Epoch 782/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.4796 - acc: 1.0000 - val_loss: 6.1652 - val_acc: 0.4600\n",
            "Epoch 783/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.4714 - acc: 1.0000 - val_loss: 5.9333 - val_acc: 0.5300\n",
            "Epoch 784/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.4613 - acc: 1.0000 - val_loss: 5.9111 - val_acc: 0.4900\n",
            "Epoch 785/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.4537 - acc: 1.0000 - val_loss: 6.2462 - val_acc: 0.4600\n",
            "Epoch 786/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.4410 - acc: 1.0000 - val_loss: 6.3848 - val_acc: 0.4400\n",
            "Epoch 787/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.4329 - acc: 1.0000 - val_loss: 6.3973 - val_acc: 0.4900\n",
            "Epoch 788/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.4234 - acc: 1.0000 - val_loss: 6.6382 - val_acc: 0.4100\n",
            "Epoch 789/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.4123 - acc: 1.0000 - val_loss: 6.5247 - val_acc: 0.4300\n",
            "Epoch 790/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.4039 - acc: 1.0000 - val_loss: 6.3997 - val_acc: 0.4500\n",
            "Epoch 791/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.3941 - acc: 1.0000 - val_loss: 6.3711 - val_acc: 0.4600\n",
            "Epoch 792/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.3838 - acc: 1.0000 - val_loss: 6.3158 - val_acc: 0.4700\n",
            "Epoch 793/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.3742 - acc: 1.0000 - val_loss: 6.2562 - val_acc: 0.4700\n",
            "Epoch 794/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.3644 - acc: 1.0000 - val_loss: 6.0071 - val_acc: 0.4700\n",
            "Epoch 795/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.3546 - acc: 1.0000 - val_loss: 5.8629 - val_acc: 0.4900\n",
            "Epoch 796/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.3450 - acc: 1.0000 - val_loss: 5.9142 - val_acc: 0.4700\n",
            "Epoch 797/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.3353 - acc: 1.0000 - val_loss: 5.9344 - val_acc: 0.5000\n",
            "Epoch 798/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.3270 - acc: 1.0000 - val_loss: 5.9470 - val_acc: 0.4900\n",
            "Epoch 799/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.3163 - acc: 1.0000 - val_loss: 5.8750 - val_acc: 0.5100\n",
            "Epoch 800/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.3067 - acc: 1.0000 - val_loss: 5.7685 - val_acc: 0.5100\n",
            "Epoch 801/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.2969 - acc: 1.0000 - val_loss: 5.7135 - val_acc: 0.5200\n",
            "Epoch 802/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.2876 - acc: 1.0000 - val_loss: 5.7934 - val_acc: 0.5100\n",
            "Epoch 803/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.2777 - acc: 1.0000 - val_loss: 6.0053 - val_acc: 0.5100\n",
            "Epoch 804/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.2680 - acc: 1.0000 - val_loss: 5.7396 - val_acc: 0.5400\n",
            "Epoch 805/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.2588 - acc: 1.0000 - val_loss: 5.8387 - val_acc: 0.5100\n",
            "Epoch 806/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.2488 - acc: 1.0000 - val_loss: 5.9623 - val_acc: 0.5200\n",
            "Epoch 807/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.2409 - acc: 1.0000 - val_loss: 5.9304 - val_acc: 0.5100\n",
            "Epoch 808/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.2297 - acc: 1.0000 - val_loss: 5.7962 - val_acc: 0.4800\n",
            "Epoch 809/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.2209 - acc: 1.0000 - val_loss: 5.9199 - val_acc: 0.4800\n",
            "Epoch 810/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.2105 - acc: 1.0000 - val_loss: 5.8741 - val_acc: 0.5100\n",
            "Epoch 811/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.2010 - acc: 1.0000 - val_loss: 5.8651 - val_acc: 0.5000\n",
            "Epoch 812/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.1914 - acc: 1.0000 - val_loss: 5.9890 - val_acc: 0.4800\n",
            "Epoch 813/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.1820 - acc: 1.0000 - val_loss: 5.9424 - val_acc: 0.4900\n",
            "Epoch 814/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.1722 - acc: 1.0000 - val_loss: 5.9515 - val_acc: 0.4700\n",
            "Epoch 815/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.1629 - acc: 1.0000 - val_loss: 5.7868 - val_acc: 0.4800\n",
            "Epoch 816/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.1532 - acc: 1.0000 - val_loss: 5.9279 - val_acc: 0.4700\n",
            "Epoch 817/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.1436 - acc: 1.0000 - val_loss: 5.8281 - val_acc: 0.4800\n",
            "Epoch 818/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.1341 - acc: 1.0000 - val_loss: 5.8442 - val_acc: 0.4700\n",
            "Epoch 819/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.1245 - acc: 1.0000 - val_loss: 6.0163 - val_acc: 0.4700\n",
            "Epoch 820/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.1156 - acc: 1.0000 - val_loss: 5.7370 - val_acc: 0.4800\n",
            "Epoch 821/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.1054 - acc: 1.0000 - val_loss: 5.9757 - val_acc: 0.4600\n",
            "Epoch 822/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.0961 - acc: 1.0000 - val_loss: 5.8018 - val_acc: 0.4600\n",
            "Epoch 823/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.0892 - acc: 1.0000 - val_loss: 6.0280 - val_acc: 0.4800\n",
            "Epoch 824/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.0771 - acc: 1.0000 - val_loss: 6.2060 - val_acc: 0.4900\n",
            "Epoch 825/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.0703 - acc: 1.0000 - val_loss: 6.4947 - val_acc: 0.4100\n",
            "Epoch 826/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.0654 - acc: 0.9950 - val_loss: 6.0556 - val_acc: 0.5000\n",
            "Epoch 827/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.0540 - acc: 0.9950 - val_loss: 5.5898 - val_acc: 0.5300\n",
            "Epoch 828/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.0425 - acc: 1.0000 - val_loss: 5.6815 - val_acc: 0.5400\n",
            "Epoch 829/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.0308 - acc: 1.0000 - val_loss: 5.7062 - val_acc: 0.5500\n",
            "Epoch 830/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.0214 - acc: 1.0000 - val_loss: 5.6207 - val_acc: 0.5600\n",
            "Epoch 831/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.0151 - acc: 1.0000 - val_loss: 6.0577 - val_acc: 0.5100\n",
            "Epoch 832/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 3.0026 - acc: 1.0000 - val_loss: 5.9375 - val_acc: 0.5500\n",
            "Epoch 833/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.9927 - acc: 1.0000 - val_loss: 5.8951 - val_acc: 0.5500\n",
            "Epoch 834/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.9837 - acc: 1.0000 - val_loss: 5.9155 - val_acc: 0.5600\n",
            "Epoch 835/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.9742 - acc: 1.0000 - val_loss: 5.9273 - val_acc: 0.5600\n",
            "Epoch 836/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.9647 - acc: 1.0000 - val_loss: 6.1912 - val_acc: 0.5300\n",
            "Epoch 837/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.9560 - acc: 1.0000 - val_loss: 6.0187 - val_acc: 0.5500\n",
            "Epoch 838/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.9458 - acc: 1.0000 - val_loss: 6.0288 - val_acc: 0.4900\n",
            "Epoch 839/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.9368 - acc: 1.0000 - val_loss: 5.9395 - val_acc: 0.5300\n",
            "Epoch 840/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.9270 - acc: 1.0000 - val_loss: 6.0786 - val_acc: 0.5100\n",
            "Epoch 841/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.9195 - acc: 1.0000 - val_loss: 5.8328 - val_acc: 0.5000\n",
            "Epoch 842/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.9086 - acc: 1.0000 - val_loss: 7.0680 - val_acc: 0.4000\n",
            "Epoch 843/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.8998 - acc: 1.0000 - val_loss: 7.0724 - val_acc: 0.3900\n",
            "Epoch 844/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.8932 - acc: 1.0000 - val_loss: 6.9336 - val_acc: 0.4000\n",
            "Epoch 845/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.8839 - acc: 1.0000 - val_loss: 6.2882 - val_acc: 0.4400\n",
            "Epoch 846/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.8715 - acc: 1.0000 - val_loss: 5.3496 - val_acc: 0.5500\n",
            "Epoch 847/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.8621 - acc: 1.0000 - val_loss: 5.1054 - val_acc: 0.5900\n",
            "Epoch 848/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.8531 - acc: 1.0000 - val_loss: 5.1221 - val_acc: 0.5800\n",
            "Epoch 849/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.8446 - acc: 1.0000 - val_loss: 5.2457 - val_acc: 0.5800\n",
            "Epoch 850/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.8354 - acc: 1.0000 - val_loss: 5.2112 - val_acc: 0.5400\n",
            "Epoch 851/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.8254 - acc: 1.0000 - val_loss: 5.1523 - val_acc: 0.5400\n",
            "Epoch 852/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.8159 - acc: 1.0000 - val_loss: 5.2092 - val_acc: 0.5400\n",
            "Epoch 853/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.8077 - acc: 1.0000 - val_loss: 5.1690 - val_acc: 0.5200\n",
            "Epoch 854/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7973 - acc: 1.0000 - val_loss: 5.5278 - val_acc: 0.4900\n",
            "Epoch 855/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7884 - acc: 1.0000 - val_loss: 5.6911 - val_acc: 0.4800\n",
            "Epoch 856/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7795 - acc: 1.0000 - val_loss: 5.4382 - val_acc: 0.4900\n",
            "Epoch 857/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7694 - acc: 1.0000 - val_loss: 5.6655 - val_acc: 0.4600\n",
            "Epoch 858/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7650 - acc: 0.9950 - val_loss: 5.8496 - val_acc: 0.4700\n",
            "Epoch 859/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7523 - acc: 1.0000 - val_loss: 5.8196 - val_acc: 0.4700\n",
            "Epoch 860/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7420 - acc: 1.0000 - val_loss: 4.9865 - val_acc: 0.6000\n",
            "Epoch 861/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7335 - acc: 1.0000 - val_loss: 4.6658 - val_acc: 0.6500\n",
            "Epoch 862/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7256 - acc: 1.0000 - val_loss: 4.6355 - val_acc: 0.6300\n",
            "Epoch 863/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7154 - acc: 1.0000 - val_loss: 4.6511 - val_acc: 0.6500\n",
            "Epoch 864/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7057 - acc: 1.0000 - val_loss: 4.7884 - val_acc: 0.6200\n",
            "Epoch 865/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7023 - acc: 0.9950 - val_loss: 4.7027 - val_acc: 0.6100\n",
            "Epoch 866/3000\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.6895 - acc: 1.0000 - val_loss: 4.6781 - val_acc: 0.5500\n",
            "Epoch 867/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.7080 - acc: 0.9950 - val_loss: 4.5133 - val_acc: 0.5900\n",
            "Epoch 868/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.6813 - acc: 0.9950 - val_loss: 4.5926 - val_acc: 0.5300\n",
            "Epoch 869/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.6666 - acc: 1.0000 - val_loss: 5.0118 - val_acc: 0.5000\n",
            "Epoch 870/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.6563 - acc: 1.0000 - val_loss: 4.9878 - val_acc: 0.5200\n",
            "Epoch 871/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.6474 - acc: 1.0000 - val_loss: 5.0705 - val_acc: 0.5300\n",
            "Epoch 872/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.6402 - acc: 1.0000 - val_loss: 4.9861 - val_acc: 0.5500\n",
            "Epoch 873/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.6300 - acc: 1.0000 - val_loss: 5.1788 - val_acc: 0.5000\n",
            "Epoch 874/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.6254 - acc: 1.0000 - val_loss: 4.8014 - val_acc: 0.5700\n",
            "Epoch 875/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.6187 - acc: 0.9950 - val_loss: 4.0858 - val_acc: 0.6800\n",
            "Epoch 876/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.6124 - acc: 0.9950 - val_loss: 4.2874 - val_acc: 0.6700\n",
            "Epoch 877/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.6182 - acc: 0.9950 - val_loss: 4.3955 - val_acc: 0.6500\n",
            "Epoch 878/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5900 - acc: 1.0000 - val_loss: 4.4458 - val_acc: 0.6500\n",
            "Epoch 879/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5826 - acc: 1.0000 - val_loss: 4.6911 - val_acc: 0.5500\n",
            "Epoch 880/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5726 - acc: 1.0000 - val_loss: 4.3051 - val_acc: 0.6400\n",
            "Epoch 881/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5774 - acc: 0.9950 - val_loss: 4.3013 - val_acc: 0.6200\n",
            "Epoch 882/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5546 - acc: 1.0000 - val_loss: 3.9923 - val_acc: 0.6200\n",
            "Epoch 883/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5472 - acc: 1.0000 - val_loss: 3.7775 - val_acc: 0.6600\n",
            "Epoch 884/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5432 - acc: 1.0000 - val_loss: 4.0670 - val_acc: 0.6300\n",
            "Epoch 885/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5289 - acc: 1.0000 - val_loss: 4.0305 - val_acc: 0.6600\n",
            "Epoch 886/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5216 - acc: 1.0000 - val_loss: 4.3052 - val_acc: 0.6300\n",
            "Epoch 887/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5138 - acc: 1.0000 - val_loss: 4.1713 - val_acc: 0.6000\n",
            "Epoch 888/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5043 - acc: 1.0000 - val_loss: 3.5418 - val_acc: 0.7300\n",
            "Epoch 889/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5027 - acc: 0.9950 - val_loss: 3.2481 - val_acc: 0.8200\n",
            "Epoch 890/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4928 - acc: 0.9950 - val_loss: 3.6238 - val_acc: 0.7300\n",
            "Epoch 891/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4786 - acc: 1.0000 - val_loss: 3.8280 - val_acc: 0.6900\n",
            "Epoch 892/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4697 - acc: 1.0000 - val_loss: 3.9239 - val_acc: 0.6900\n",
            "Epoch 893/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4614 - acc: 1.0000 - val_loss: 3.9406 - val_acc: 0.6900\n",
            "Epoch 894/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4601 - acc: 0.9950 - val_loss: 4.0182 - val_acc: 0.6800\n",
            "Epoch 895/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4610 - acc: 0.9900 - val_loss: 4.0394 - val_acc: 0.6800\n",
            "Epoch 896/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5033 - acc: 0.9850 - val_loss: 5.5421 - val_acc: 0.4800\n",
            "Epoch 897/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.6210 - acc: 0.9450 - val_loss: 7.9602 - val_acc: 0.3000\n",
            "Epoch 898/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5441 - acc: 0.9550 - val_loss: 6.7033 - val_acc: 0.3300\n",
            "Epoch 899/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5388 - acc: 0.9700 - val_loss: 5.9416 - val_acc: 0.3400\n",
            "Epoch 900/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4437 - acc: 0.9950 - val_loss: 3.8793 - val_acc: 0.6100\n",
            "Epoch 901/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.5136 - acc: 0.9900 - val_loss: 4.1577 - val_acc: 0.6200\n",
            "Epoch 902/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4695 - acc: 0.9900 - val_loss: 4.0857 - val_acc: 0.6400\n",
            "Epoch 903/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4628 - acc: 0.9850 - val_loss: 4.5455 - val_acc: 0.5700\n",
            "Epoch 904/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4249 - acc: 0.9950 - val_loss: 5.1826 - val_acc: 0.4300\n",
            "Epoch 905/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4370 - acc: 0.9900 - val_loss: 4.5815 - val_acc: 0.5600\n",
            "Epoch 906/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.4116 - acc: 0.9900 - val_loss: 3.8256 - val_acc: 0.6200\n",
            "Epoch 907/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3866 - acc: 1.0000 - val_loss: 3.9189 - val_acc: 0.5700\n",
            "Epoch 908/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3831 - acc: 1.0000 - val_loss: 4.1112 - val_acc: 0.5300\n",
            "Epoch 909/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3943 - acc: 0.9900 - val_loss: 4.1414 - val_acc: 0.5700\n",
            "Epoch 910/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3774 - acc: 0.9900 - val_loss: 3.8334 - val_acc: 0.6000\n",
            "Epoch 911/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3887 - acc: 0.9850 - val_loss: 3.8608 - val_acc: 0.6500\n",
            "Epoch 912/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3712 - acc: 0.9950 - val_loss: 3.8903 - val_acc: 0.6300\n",
            "Epoch 913/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3778 - acc: 0.9900 - val_loss: 3.5946 - val_acc: 0.6900\n",
            "Epoch 914/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3433 - acc: 1.0000 - val_loss: 4.2769 - val_acc: 0.6100\n",
            "Epoch 915/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3386 - acc: 0.9950 - val_loss: 4.8636 - val_acc: 0.5300\n",
            "Epoch 916/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3337 - acc: 0.9950 - val_loss: 4.9427 - val_acc: 0.5300\n",
            "Epoch 917/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3198 - acc: 1.0000 - val_loss: 4.8596 - val_acc: 0.5300\n",
            "Epoch 918/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3111 - acc: 1.0000 - val_loss: 4.5586 - val_acc: 0.5600\n",
            "Epoch 919/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3037 - acc: 1.0000 - val_loss: 4.5080 - val_acc: 0.5600\n",
            "Epoch 920/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2974 - acc: 1.0000 - val_loss: 4.7646 - val_acc: 0.5600\n",
            "Epoch 921/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2903 - acc: 1.0000 - val_loss: 4.5661 - val_acc: 0.5700\n",
            "Epoch 922/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2824 - acc: 1.0000 - val_loss: 4.7144 - val_acc: 0.5500\n",
            "Epoch 923/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2963 - acc: 0.9950 - val_loss: 5.0117 - val_acc: 0.5400\n",
            "Epoch 924/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2990 - acc: 0.9850 - val_loss: 6.0921 - val_acc: 0.4700\n",
            "Epoch 925/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2924 - acc: 0.9850 - val_loss: 5.5189 - val_acc: 0.5100\n",
            "Epoch 926/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2692 - acc: 0.9950 - val_loss: 5.3322 - val_acc: 0.4500\n",
            "Epoch 927/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2808 - acc: 0.9950 - val_loss: 5.5399 - val_acc: 0.4200\n",
            "Epoch 928/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3104 - acc: 0.9800 - val_loss: 5.1413 - val_acc: 0.4500\n",
            "Epoch 929/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3317 - acc: 0.9750 - val_loss: 4.4994 - val_acc: 0.5700\n",
            "Epoch 930/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2935 - acc: 0.9900 - val_loss: 4.7719 - val_acc: 0.5100\n",
            "Epoch 931/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2961 - acc: 0.9700 - val_loss: 5.4121 - val_acc: 0.4800\n",
            "Epoch 932/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2718 - acc: 0.9750 - val_loss: 5.0702 - val_acc: 0.5800\n",
            "Epoch 933/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2473 - acc: 0.9900 - val_loss: 4.1516 - val_acc: 0.7100\n",
            "Epoch 934/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2557 - acc: 0.9900 - val_loss: 3.4530 - val_acc: 0.7700\n",
            "Epoch 935/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2221 - acc: 1.0000 - val_loss: 3.2822 - val_acc: 0.7500\n",
            "Epoch 936/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2128 - acc: 1.0000 - val_loss: 3.4546 - val_acc: 0.6900\n",
            "Epoch 937/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2178 - acc: 0.9950 - val_loss: 3.3437 - val_acc: 0.6900\n",
            "Epoch 938/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2004 - acc: 1.0000 - val_loss: 3.7131 - val_acc: 0.6700\n",
            "Epoch 939/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2009 - acc: 1.0000 - val_loss: 3.9447 - val_acc: 0.6500\n",
            "Epoch 940/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1962 - acc: 0.9950 - val_loss: 4.3145 - val_acc: 0.5800\n",
            "Epoch 941/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1821 - acc: 1.0000 - val_loss: 4.8034 - val_acc: 0.5500\n",
            "Epoch 942/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1782 - acc: 1.0000 - val_loss: 4.9088 - val_acc: 0.5500\n",
            "Epoch 943/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1798 - acc: 0.9950 - val_loss: 4.6494 - val_acc: 0.5500\n",
            "Epoch 944/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1626 - acc: 1.0000 - val_loss: 4.5025 - val_acc: 0.5700\n",
            "Epoch 945/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1583 - acc: 1.0000 - val_loss: 4.7020 - val_acc: 0.5700\n",
            "Epoch 946/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1493 - acc: 1.0000 - val_loss: 4.4678 - val_acc: 0.5800\n",
            "Epoch 947/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1433 - acc: 1.0000 - val_loss: 4.3884 - val_acc: 0.5800\n",
            "Epoch 948/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1362 - acc: 1.0000 - val_loss: 4.8789 - val_acc: 0.5500\n",
            "Epoch 949/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1327 - acc: 1.0000 - val_loss: 4.5878 - val_acc: 0.5500\n",
            "Epoch 950/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1276 - acc: 1.0000 - val_loss: 4.7387 - val_acc: 0.5500\n",
            "Epoch 951/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1205 - acc: 1.0000 - val_loss: 5.0682 - val_acc: 0.5400\n",
            "Epoch 952/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1138 - acc: 1.0000 - val_loss: 4.5778 - val_acc: 0.5600\n",
            "Epoch 953/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1050 - acc: 1.0000 - val_loss: 4.8933 - val_acc: 0.5500\n",
            "Epoch 954/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1015 - acc: 1.0000 - val_loss: 4.9382 - val_acc: 0.5600\n",
            "Epoch 955/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0973 - acc: 1.0000 - val_loss: 4.9288 - val_acc: 0.5500\n",
            "Epoch 956/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1111 - acc: 0.9900 - val_loss: 4.8390 - val_acc: 0.5600\n",
            "Epoch 957/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1568 - acc: 0.9950 - val_loss: 4.1748 - val_acc: 0.6200\n",
            "Epoch 958/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.3374 - acc: 0.9350 - val_loss: 5.3598 - val_acc: 0.4800\n",
            "Epoch 959/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.2404 - acc: 0.9350 - val_loss: 4.7824 - val_acc: 0.5200\n",
            "Epoch 960/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.1292 - acc: 0.9800 - val_loss: 3.9611 - val_acc: 0.6100\n",
            "Epoch 961/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0662 - acc: 1.0000 - val_loss: 3.8389 - val_acc: 0.6400\n",
            "Epoch 962/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0644 - acc: 1.0000 - val_loss: 3.7821 - val_acc: 0.6400\n",
            "Epoch 963/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0645 - acc: 0.9950 - val_loss: 4.0025 - val_acc: 0.5900\n",
            "Epoch 964/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0527 - acc: 1.0000 - val_loss: 4.4901 - val_acc: 0.5300\n",
            "Epoch 965/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0468 - acc: 1.0000 - val_loss: 4.3813 - val_acc: 0.5500\n",
            "Epoch 966/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0407 - acc: 1.0000 - val_loss: 4.3769 - val_acc: 0.5400\n",
            "Epoch 967/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0314 - acc: 1.0000 - val_loss: 4.2465 - val_acc: 0.5600\n",
            "Epoch 968/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0246 - acc: 1.0000 - val_loss: 4.4572 - val_acc: 0.5300\n",
            "Epoch 969/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0183 - acc: 1.0000 - val_loss: 4.0812 - val_acc: 0.5700\n",
            "Epoch 970/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0139 - acc: 1.0000 - val_loss: 4.1787 - val_acc: 0.5500\n",
            "Epoch 971/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0083 - acc: 1.0000 - val_loss: 4.1103 - val_acc: 0.5500\n",
            "Epoch 972/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 2.0002 - acc: 1.0000 - val_loss: 4.0711 - val_acc: 0.5500\n",
            "Epoch 973/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9945 - acc: 1.0000 - val_loss: 4.1752 - val_acc: 0.5100\n",
            "Epoch 974/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9884 - acc: 1.0000 - val_loss: 3.9315 - val_acc: 0.5500\n",
            "Epoch 975/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9825 - acc: 1.0000 - val_loss: 4.0398 - val_acc: 0.5500\n",
            "Epoch 976/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9775 - acc: 1.0000 - val_loss: 4.0674 - val_acc: 0.5300\n",
            "Epoch 977/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9707 - acc: 1.0000 - val_loss: 4.1976 - val_acc: 0.5300\n",
            "Epoch 978/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9670 - acc: 1.0000 - val_loss: 4.0364 - val_acc: 0.6000\n",
            "Epoch 979/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9593 - acc: 1.0000 - val_loss: 4.0410 - val_acc: 0.5900\n",
            "Epoch 980/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9536 - acc: 1.0000 - val_loss: 3.9668 - val_acc: 0.6000\n",
            "Epoch 981/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9470 - acc: 1.0000 - val_loss: 4.1929 - val_acc: 0.5600\n",
            "Epoch 982/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9417 - acc: 1.0000 - val_loss: 4.0780 - val_acc: 0.5600\n",
            "Epoch 983/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9361 - acc: 1.0000 - val_loss: 3.9903 - val_acc: 0.5900\n",
            "Epoch 984/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9295 - acc: 1.0000 - val_loss: 4.2794 - val_acc: 0.5400\n",
            "Epoch 985/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9234 - acc: 1.0000 - val_loss: 4.3700 - val_acc: 0.5600\n",
            "Epoch 986/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9174 - acc: 1.0000 - val_loss: 4.2288 - val_acc: 0.5600\n",
            "Epoch 987/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9115 - acc: 1.0000 - val_loss: 4.2407 - val_acc: 0.5500\n",
            "Epoch 988/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.9061 - acc: 1.0000 - val_loss: 4.3398 - val_acc: 0.5300\n",
            "Epoch 989/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8999 - acc: 1.0000 - val_loss: 4.2868 - val_acc: 0.5400\n",
            "Epoch 990/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8946 - acc: 1.0000 - val_loss: 4.0660 - val_acc: 0.5900\n",
            "Epoch 991/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8880 - acc: 1.0000 - val_loss: 4.0861 - val_acc: 0.5700\n",
            "Epoch 992/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8825 - acc: 1.0000 - val_loss: 4.0757 - val_acc: 0.5600\n",
            "Epoch 993/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8764 - acc: 1.0000 - val_loss: 4.0649 - val_acc: 0.5400\n",
            "Epoch 994/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8711 - acc: 1.0000 - val_loss: 4.0492 - val_acc: 0.5400\n",
            "Epoch 995/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8649 - acc: 1.0000 - val_loss: 4.0396 - val_acc: 0.5600\n",
            "Epoch 996/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8592 - acc: 1.0000 - val_loss: 4.0978 - val_acc: 0.5300\n",
            "Epoch 997/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8531 - acc: 1.0000 - val_loss: 3.9866 - val_acc: 0.5500\n",
            "Epoch 998/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8471 - acc: 1.0000 - val_loss: 3.9009 - val_acc: 0.5600\n",
            "Epoch 999/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8418 - acc: 1.0000 - val_loss: 4.1810 - val_acc: 0.5200\n",
            "Epoch 1000/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8356 - acc: 1.0000 - val_loss: 3.9800 - val_acc: 0.5600\n",
            "Epoch 1001/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8298 - acc: 1.0000 - val_loss: 3.9958 - val_acc: 0.5600\n",
            "Epoch 1002/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8238 - acc: 1.0000 - val_loss: 3.8881 - val_acc: 0.5600\n",
            "Epoch 1003/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8182 - acc: 1.0000 - val_loss: 4.0218 - val_acc: 0.5600\n",
            "Epoch 1004/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8319 - acc: 0.9950 - val_loss: 3.6473 - val_acc: 0.5900\n",
            "Epoch 1005/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8104 - acc: 1.0000 - val_loss: 2.3526 - val_acc: 0.8100\n",
            "Epoch 1006/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8139 - acc: 0.9950 - val_loss: 2.2915 - val_acc: 0.8100\n",
            "Epoch 1007/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8259 - acc: 0.9950 - val_loss: 2.3894 - val_acc: 0.8200\n",
            "Epoch 1008/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8065 - acc: 0.9900 - val_loss: 2.8704 - val_acc: 0.6600\n",
            "Epoch 1009/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7980 - acc: 0.9950 - val_loss: 3.2175 - val_acc: 0.6200\n",
            "Epoch 1010/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7910 - acc: 0.9950 - val_loss: 3.2654 - val_acc: 0.6500\n",
            "Epoch 1011/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7836 - acc: 1.0000 - val_loss: 3.3235 - val_acc: 0.6500\n",
            "Epoch 1012/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8010 - acc: 0.9950 - val_loss: 3.1391 - val_acc: 0.7000\n",
            "Epoch 1013/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7901 - acc: 0.9900 - val_loss: 2.8899 - val_acc: 0.7800\n",
            "Epoch 1014/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7663 - acc: 1.0000 - val_loss: 3.3230 - val_acc: 0.6900\n",
            "Epoch 1015/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7598 - acc: 1.0000 - val_loss: 3.3678 - val_acc: 0.6800\n",
            "Epoch 1016/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7574 - acc: 1.0000 - val_loss: 3.2075 - val_acc: 0.6900\n",
            "Epoch 1017/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7527 - acc: 1.0000 - val_loss: 3.3149 - val_acc: 0.6100\n",
            "Epoch 1018/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7458 - acc: 1.0000 - val_loss: 3.3191 - val_acc: 0.6400\n",
            "Epoch 1019/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7461 - acc: 0.9950 - val_loss: 3.3533 - val_acc: 0.6300\n",
            "Epoch 1020/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7512 - acc: 0.9900 - val_loss: 3.6184 - val_acc: 0.5900\n",
            "Epoch 1021/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7711 - acc: 0.9850 - val_loss: 3.5000 - val_acc: 0.6200\n",
            "Epoch 1022/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7622 - acc: 0.9900 - val_loss: 4.1879 - val_acc: 0.5800\n",
            "Epoch 1023/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7997 - acc: 0.9650 - val_loss: 2.5504 - val_acc: 0.6700\n",
            "Epoch 1024/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7399 - acc: 0.9950 - val_loss: 3.1786 - val_acc: 0.5700\n",
            "Epoch 1025/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7399 - acc: 0.9950 - val_loss: 3.6729 - val_acc: 0.5500\n",
            "Epoch 1026/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7180 - acc: 1.0000 - val_loss: 3.9923 - val_acc: 0.5300\n",
            "Epoch 1027/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7122 - acc: 1.0000 - val_loss: 3.8264 - val_acc: 0.5300\n",
            "Epoch 1028/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7142 - acc: 0.9950 - val_loss: 4.0635 - val_acc: 0.5200\n",
            "Epoch 1029/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7395 - acc: 0.9900 - val_loss: 3.5887 - val_acc: 0.5300\n",
            "Epoch 1030/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6993 - acc: 1.0000 - val_loss: 3.6159 - val_acc: 0.5400\n",
            "Epoch 1031/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7912 - acc: 0.9700 - val_loss: 3.8649 - val_acc: 0.5200\n",
            "Epoch 1032/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6893 - acc: 1.0000 - val_loss: 3.8759 - val_acc: 0.5300\n",
            "Epoch 1033/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6833 - acc: 1.0000 - val_loss: 3.4085 - val_acc: 0.5700\n",
            "Epoch 1034/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6807 - acc: 1.0000 - val_loss: 3.8211 - val_acc: 0.5300\n",
            "Epoch 1035/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6854 - acc: 0.9950 - val_loss: 3.5479 - val_acc: 0.5600\n",
            "Epoch 1036/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6725 - acc: 1.0000 - val_loss: 3.3465 - val_acc: 0.5600\n",
            "Epoch 1037/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6659 - acc: 1.0000 - val_loss: 3.6230 - val_acc: 0.5400\n",
            "Epoch 1038/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6574 - acc: 1.0000 - val_loss: 3.4572 - val_acc: 0.5400\n",
            "Epoch 1039/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6551 - acc: 1.0000 - val_loss: 3.3110 - val_acc: 0.5400\n",
            "Epoch 1040/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6477 - acc: 1.0000 - val_loss: 3.1909 - val_acc: 0.5400\n",
            "Epoch 1041/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6419 - acc: 1.0000 - val_loss: 3.5058 - val_acc: 0.5200\n",
            "Epoch 1042/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6371 - acc: 1.0000 - val_loss: 3.6105 - val_acc: 0.5200\n",
            "Epoch 1043/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6318 - acc: 1.0000 - val_loss: 3.6511 - val_acc: 0.5100\n",
            "Epoch 1044/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6279 - acc: 1.0000 - val_loss: 3.6274 - val_acc: 0.5200\n",
            "Epoch 1045/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6217 - acc: 1.0000 - val_loss: 3.4669 - val_acc: 0.5100\n",
            "Epoch 1046/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6237 - acc: 0.9950 - val_loss: 3.5978 - val_acc: 0.5000\n",
            "Epoch 1047/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6117 - acc: 1.0000 - val_loss: 3.4519 - val_acc: 0.5100\n",
            "Epoch 1048/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6086 - acc: 1.0000 - val_loss: 3.2365 - val_acc: 0.5300\n",
            "Epoch 1049/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6034 - acc: 1.0000 - val_loss: 3.4872 - val_acc: 0.5100\n",
            "Epoch 1050/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6026 - acc: 0.9950 - val_loss: 3.5123 - val_acc: 0.5200\n",
            "Epoch 1051/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5934 - acc: 1.0000 - val_loss: 3.4337 - val_acc: 0.5200\n",
            "Epoch 1052/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5878 - acc: 1.0000 - val_loss: 3.5325 - val_acc: 0.5200\n",
            "Epoch 1053/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.7006 - acc: 0.9850 - val_loss: 3.4898 - val_acc: 0.5200\n",
            "Epoch 1054/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6504 - acc: 0.9900 - val_loss: 2.8336 - val_acc: 0.6500\n",
            "Epoch 1055/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6917 - acc: 0.9800 - val_loss: 2.9328 - val_acc: 0.6500\n",
            "Epoch 1056/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5973 - acc: 0.9900 - val_loss: 3.1820 - val_acc: 0.6200\n",
            "Epoch 1057/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5692 - acc: 1.0000 - val_loss: 2.8618 - val_acc: 0.6700\n",
            "Epoch 1058/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5659 - acc: 1.0000 - val_loss: 2.8881 - val_acc: 0.6700\n",
            "Epoch 1059/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5609 - acc: 1.0000 - val_loss: 3.4722 - val_acc: 0.6100\n",
            "Epoch 1060/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5819 - acc: 0.9950 - val_loss: 3.8706 - val_acc: 0.5800\n",
            "Epoch 1061/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5866 - acc: 0.9850 - val_loss: 4.2152 - val_acc: 0.5400\n",
            "Epoch 1062/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5655 - acc: 0.9950 - val_loss: 4.2398 - val_acc: 0.5100\n",
            "Epoch 1063/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5432 - acc: 1.0000 - val_loss: 3.7865 - val_acc: 0.5300\n",
            "Epoch 1064/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5426 - acc: 1.0000 - val_loss: 4.2782 - val_acc: 0.5000\n",
            "Epoch 1065/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5504 - acc: 0.9950 - val_loss: 4.0527 - val_acc: 0.5100\n",
            "Epoch 1066/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5292 - acc: 1.0000 - val_loss: 4.0699 - val_acc: 0.5300\n",
            "Epoch 1067/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5450 - acc: 0.9950 - val_loss: 3.7352 - val_acc: 0.5300\n",
            "Epoch 1068/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5216 - acc: 1.0000 - val_loss: 2.2552 - val_acc: 0.6500\n",
            "Epoch 1069/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5173 - acc: 1.0000 - val_loss: 2.2409 - val_acc: 0.6300\n",
            "Epoch 1070/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5139 - acc: 1.0000 - val_loss: 2.4289 - val_acc: 0.6000\n",
            "Epoch 1071/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5160 - acc: 0.9950 - val_loss: 2.4885 - val_acc: 0.5900\n",
            "Epoch 1072/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5024 - acc: 1.0000 - val_loss: 2.8491 - val_acc: 0.5500\n",
            "Epoch 1073/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4987 - acc: 1.0000 - val_loss: 2.9154 - val_acc: 0.5400\n",
            "Epoch 1074/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4937 - acc: 1.0000 - val_loss: 3.0521 - val_acc: 0.5400\n",
            "Epoch 1075/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4944 - acc: 1.0000 - val_loss: 2.9585 - val_acc: 0.5300\n",
            "Epoch 1076/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4844 - acc: 1.0000 - val_loss: 3.2611 - val_acc: 0.5000\n",
            "Epoch 1077/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4795 - acc: 1.0000 - val_loss: 3.1212 - val_acc: 0.5200\n",
            "Epoch 1078/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4752 - acc: 1.0000 - val_loss: 3.4747 - val_acc: 0.5200\n",
            "Epoch 1079/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5177 - acc: 0.9950 - val_loss: 2.8186 - val_acc: 0.5100\n",
            "Epoch 1080/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8816 - acc: 0.8900 - val_loss: 11.7666 - val_acc: 0.2800\n",
            "Epoch 1081/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8851 - acc: 0.8950 - val_loss: 13.3681 - val_acc: 0.2100\n",
            "Epoch 1082/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.8727 - acc: 0.8800 - val_loss: 15.4044 - val_acc: 0.1200\n",
            "Epoch 1083/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.6009 - acc: 0.9550 - val_loss: 17.0791 - val_acc: 0.0100\n",
            "Epoch 1084/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5618 - acc: 0.9700 - val_loss: 16.9518 - val_acc: 0.0000e+00\n",
            "Epoch 1085/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5037 - acc: 0.9900 - val_loss: 15.3499 - val_acc: 0.0500\n",
            "Epoch 1086/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4841 - acc: 1.0000 - val_loss: 14.3301 - val_acc: 0.1200\n",
            "Epoch 1087/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4852 - acc: 0.9950 - val_loss: 14.0648 - val_acc: 0.1100\n",
            "Epoch 1088/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4890 - acc: 0.9950 - val_loss: 13.8807 - val_acc: 0.1100\n",
            "Epoch 1089/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4773 - acc: 0.9950 - val_loss: 13.7248 - val_acc: 0.1400\n",
            "Epoch 1090/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4752 - acc: 0.9950 - val_loss: 13.6605 - val_acc: 0.1200\n",
            "Epoch 1091/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4703 - acc: 1.0000 - val_loss: 13.5598 - val_acc: 0.1100\n",
            "Epoch 1092/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5073 - acc: 0.9950 - val_loss: 13.5521 - val_acc: 0.1200\n",
            "Epoch 1093/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4688 - acc: 0.9950 - val_loss: 13.6750 - val_acc: 0.0900\n",
            "Epoch 1094/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4955 - acc: 0.9800 - val_loss: 13.0653 - val_acc: 0.1200\n",
            "Epoch 1095/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4640 - acc: 0.9900 - val_loss: 12.3520 - val_acc: 0.0900\n",
            "Epoch 1096/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4885 - acc: 0.9900 - val_loss: 12.2494 - val_acc: 0.1000\n",
            "Epoch 1097/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4619 - acc: 0.9950 - val_loss: 11.3423 - val_acc: 0.1100\n",
            "Epoch 1098/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4584 - acc: 0.9900 - val_loss: 10.6261 - val_acc: 0.1400\n",
            "Epoch 1099/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4584 - acc: 0.9950 - val_loss: 9.2596 - val_acc: 0.1600\n",
            "Epoch 1100/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4497 - acc: 0.9950 - val_loss: 8.4072 - val_acc: 0.1800\n",
            "Epoch 1101/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4346 - acc: 1.0000 - val_loss: 6.2669 - val_acc: 0.2800\n",
            "Epoch 1102/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4967 - acc: 0.9900 - val_loss: 6.9856 - val_acc: 0.1500\n",
            "Epoch 1103/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4731 - acc: 0.9700 - val_loss: 7.2203 - val_acc: 0.1500\n",
            "Epoch 1104/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4705 - acc: 0.9950 - val_loss: 4.3720 - val_acc: 0.5000\n",
            "Epoch 1105/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4394 - acc: 0.9900 - val_loss: 4.1565 - val_acc: 0.5000\n",
            "Epoch 1106/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4387 - acc: 0.9950 - val_loss: 4.2806 - val_acc: 0.5000\n",
            "Epoch 1107/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4163 - acc: 1.0000 - val_loss: 4.3873 - val_acc: 0.5000\n",
            "Epoch 1108/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4393 - acc: 0.9950 - val_loss: 4.1412 - val_acc: 0.5000\n",
            "Epoch 1109/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4466 - acc: 0.9900 - val_loss: 4.9894 - val_acc: 0.4900\n",
            "Epoch 1110/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.5395 - acc: 0.9650 - val_loss: 4.6632 - val_acc: 0.5700\n",
            "Epoch 1111/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4298 - acc: 0.9950 - val_loss: 6.1043 - val_acc: 0.4600\n",
            "Epoch 1112/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4096 - acc: 1.0000 - val_loss: 6.5417 - val_acc: 0.4500\n",
            "Epoch 1113/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4108 - acc: 0.9900 - val_loss: 7.1313 - val_acc: 0.3900\n",
            "Epoch 1114/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3998 - acc: 1.0000 - val_loss: 6.2140 - val_acc: 0.4400\n",
            "Epoch 1115/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3984 - acc: 0.9950 - val_loss: 6.2637 - val_acc: 0.4500\n",
            "Epoch 1116/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3979 - acc: 0.9950 - val_loss: 5.3552 - val_acc: 0.4600\n",
            "Epoch 1117/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4082 - acc: 0.9850 - val_loss: 4.7530 - val_acc: 0.5100\n",
            "Epoch 1118/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4861 - acc: 0.9850 - val_loss: 3.9471 - val_acc: 0.5300\n",
            "Epoch 1119/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3861 - acc: 0.9950 - val_loss: 4.0477 - val_acc: 0.5300\n",
            "Epoch 1120/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4145 - acc: 0.9950 - val_loss: 4.2808 - val_acc: 0.5300\n",
            "Epoch 1121/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4095 - acc: 0.9900 - val_loss: 3.3027 - val_acc: 0.5400\n",
            "Epoch 1122/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4074 - acc: 0.9900 - val_loss: 4.0122 - val_acc: 0.5100\n",
            "Epoch 1123/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3759 - acc: 0.9950 - val_loss: 4.0619 - val_acc: 0.4900\n",
            "Epoch 1124/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3760 - acc: 0.9950 - val_loss: 4.1167 - val_acc: 0.5000\n",
            "Epoch 1125/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3651 - acc: 1.0000 - val_loss: 4.1764 - val_acc: 0.5300\n",
            "Epoch 1126/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3640 - acc: 1.0000 - val_loss: 3.8639 - val_acc: 0.5400\n",
            "Epoch 1127/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3607 - acc: 1.0000 - val_loss: 3.2936 - val_acc: 0.5400\n",
            "Epoch 1128/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3611 - acc: 0.9950 - val_loss: 3.1734 - val_acc: 0.5400\n",
            "Epoch 1129/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3876 - acc: 0.9950 - val_loss: 3.1932 - val_acc: 0.5500\n",
            "Epoch 1130/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4063 - acc: 0.9900 - val_loss: 3.0056 - val_acc: 0.5400\n",
            "Epoch 1131/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4050 - acc: 0.9750 - val_loss: 3.4896 - val_acc: 0.5200\n",
            "Epoch 1132/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3944 - acc: 0.9900 - val_loss: 4.0913 - val_acc: 0.5000\n",
            "Epoch 1133/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3640 - acc: 0.9950 - val_loss: 4.0246 - val_acc: 0.5000\n",
            "Epoch 1134/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3447 - acc: 1.0000 - val_loss: 3.9677 - val_acc: 0.5000\n",
            "Epoch 1135/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3382 - acc: 1.0000 - val_loss: 3.7674 - val_acc: 0.5000\n",
            "Epoch 1136/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3332 - acc: 1.0000 - val_loss: 3.7570 - val_acc: 0.5100\n",
            "Epoch 1137/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3325 - acc: 1.0000 - val_loss: 3.5897 - val_acc: 0.5300\n",
            "Epoch 1138/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3280 - acc: 1.0000 - val_loss: 3.6675 - val_acc: 0.5200\n",
            "Epoch 1139/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3243 - acc: 1.0000 - val_loss: 3.9568 - val_acc: 0.5100\n",
            "Epoch 1140/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3208 - acc: 1.0000 - val_loss: 3.6709 - val_acc: 0.5300\n",
            "Epoch 1141/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3203 - acc: 1.0000 - val_loss: 3.7473 - val_acc: 0.5200\n",
            "Epoch 1142/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3152 - acc: 1.0000 - val_loss: 3.7528 - val_acc: 0.5200\n",
            "Epoch 1143/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3127 - acc: 1.0000 - val_loss: 3.9998 - val_acc: 0.5100\n",
            "Epoch 1144/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3089 - acc: 1.0000 - val_loss: 3.9738 - val_acc: 0.5100\n",
            "Epoch 1145/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3054 - acc: 1.0000 - val_loss: 3.9772 - val_acc: 0.5200\n",
            "Epoch 1146/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3023 - acc: 1.0000 - val_loss: 4.0382 - val_acc: 0.5100\n",
            "Epoch 1147/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3007 - acc: 1.0000 - val_loss: 4.0127 - val_acc: 0.5200\n",
            "Epoch 1148/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2963 - acc: 1.0000 - val_loss: 4.1098 - val_acc: 0.5100\n",
            "Epoch 1149/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2937 - acc: 1.0000 - val_loss: 4.0792 - val_acc: 0.5100\n",
            "Epoch 1150/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2901 - acc: 1.0000 - val_loss: 4.0214 - val_acc: 0.5200\n",
            "Epoch 1151/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2882 - acc: 1.0000 - val_loss: 4.2446 - val_acc: 0.5200\n",
            "Epoch 1152/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2854 - acc: 1.0000 - val_loss: 3.7764 - val_acc: 0.5200\n",
            "Epoch 1153/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2810 - acc: 1.0000 - val_loss: 3.7799 - val_acc: 0.5300\n",
            "Epoch 1154/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2779 - acc: 1.0000 - val_loss: 3.9023 - val_acc: 0.5200\n",
            "Epoch 1155/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2773 - acc: 1.0000 - val_loss: 3.8960 - val_acc: 0.5200\n",
            "Epoch 1156/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2726 - acc: 1.0000 - val_loss: 3.7932 - val_acc: 0.5200\n",
            "Epoch 1157/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2691 - acc: 1.0000 - val_loss: 3.6707 - val_acc: 0.5200\n",
            "Epoch 1158/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2665 - acc: 1.0000 - val_loss: 3.4220 - val_acc: 0.5400\n",
            "Epoch 1159/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2636 - acc: 1.0000 - val_loss: 3.7668 - val_acc: 0.5200\n",
            "Epoch 1160/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.3294 - acc: 0.9800 - val_loss: 3.3718 - val_acc: 0.5500\n",
            "Epoch 1161/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2603 - acc: 1.0000 - val_loss: 2.9223 - val_acc: 0.5900\n",
            "Epoch 1162/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2585 - acc: 1.0000 - val_loss: 3.2739 - val_acc: 0.6000\n",
            "Epoch 1163/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2572 - acc: 1.0000 - val_loss: 3.2652 - val_acc: 0.5800\n",
            "Epoch 1164/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2752 - acc: 0.9850 - val_loss: 3.4881 - val_acc: 0.5400\n",
            "Epoch 1165/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2606 - acc: 0.9950 - val_loss: 3.5907 - val_acc: 0.5500\n",
            "Epoch 1166/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2474 - acc: 1.0000 - val_loss: 3.1246 - val_acc: 0.5600\n",
            "Epoch 1167/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2509 - acc: 1.0000 - val_loss: 3.3042 - val_acc: 0.5400\n",
            "Epoch 1168/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2400 - acc: 1.0000 - val_loss: 4.0593 - val_acc: 0.5100\n",
            "Epoch 1169/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2379 - acc: 1.0000 - val_loss: 4.0611 - val_acc: 0.5100\n",
            "Epoch 1170/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2360 - acc: 1.0000 - val_loss: 3.5480 - val_acc: 0.5400\n",
            "Epoch 1171/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2331 - acc: 1.0000 - val_loss: 3.6158 - val_acc: 0.5300\n",
            "Epoch 1172/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2385 - acc: 0.9950 - val_loss: 3.6801 - val_acc: 0.5300\n",
            "Epoch 1173/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2286 - acc: 1.0000 - val_loss: 4.0522 - val_acc: 0.5400\n",
            "Epoch 1174/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2409 - acc: 0.9900 - val_loss: 3.9599 - val_acc: 0.5300\n",
            "Epoch 1175/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2227 - acc: 1.0000 - val_loss: 3.2376 - val_acc: 0.5900\n",
            "Epoch 1176/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2231 - acc: 1.0000 - val_loss: 3.8528 - val_acc: 0.5500\n",
            "Epoch 1177/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2242 - acc: 0.9900 - val_loss: 3.3037 - val_acc: 0.5800\n",
            "Epoch 1178/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2187 - acc: 1.0000 - val_loss: 3.4178 - val_acc: 0.5800\n",
            "Epoch 1179/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2593 - acc: 0.9900 - val_loss: 3.3530 - val_acc: 0.6000\n",
            "Epoch 1180/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2169 - acc: 0.9950 - val_loss: 2.5514 - val_acc: 0.7100\n",
            "Epoch 1181/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.4143 - acc: 0.9750 - val_loss: 2.9853 - val_acc: 0.6400\n",
            "Epoch 1182/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2963 - acc: 0.9800 - val_loss: 3.0450 - val_acc: 0.6100\n",
            "Epoch 1183/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2318 - acc: 0.9900 - val_loss: 3.3773 - val_acc: 0.5900\n",
            "Epoch 1184/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2386 - acc: 0.9850 - val_loss: 2.9970 - val_acc: 0.6200\n",
            "Epoch 1185/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2076 - acc: 0.9950 - val_loss: 3.1949 - val_acc: 0.5900\n",
            "Epoch 1186/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2430 - acc: 0.9900 - val_loss: 3.0414 - val_acc: 0.6100\n",
            "Epoch 1187/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2036 - acc: 1.0000 - val_loss: 3.3467 - val_acc: 0.5900\n",
            "Epoch 1188/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2331 - acc: 0.9850 - val_loss: 3.6441 - val_acc: 0.6000\n",
            "Epoch 1189/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2046 - acc: 0.9900 - val_loss: 3.2247 - val_acc: 0.5800\n",
            "Epoch 1190/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2035 - acc: 0.9950 - val_loss: 4.0772 - val_acc: 0.5200\n",
            "Epoch 1191/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1913 - acc: 1.0000 - val_loss: 4.3168 - val_acc: 0.5100\n",
            "Epoch 1192/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2229 - acc: 0.9950 - val_loss: 3.7338 - val_acc: 0.5200\n",
            "Epoch 1193/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1880 - acc: 1.0000 - val_loss: 3.9436 - val_acc: 0.5300\n",
            "Epoch 1194/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1932 - acc: 0.9950 - val_loss: 3.8262 - val_acc: 0.5300\n",
            "Epoch 1195/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1858 - acc: 0.9950 - val_loss: 3.6420 - val_acc: 0.5300\n",
            "Epoch 1196/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1783 - acc: 1.0000 - val_loss: 3.6166 - val_acc: 0.5300\n",
            "Epoch 1197/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1737 - acc: 1.0000 - val_loss: 3.5428 - val_acc: 0.5300\n",
            "Epoch 1198/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1720 - acc: 1.0000 - val_loss: 3.2539 - val_acc: 0.5500\n",
            "Epoch 1199/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1682 - acc: 1.0000 - val_loss: 3.4306 - val_acc: 0.5200\n",
            "Epoch 1200/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1658 - acc: 1.0000 - val_loss: 3.3681 - val_acc: 0.5300\n",
            "Epoch 1201/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1616 - acc: 1.0000 - val_loss: 3.4231 - val_acc: 0.5200\n",
            "Epoch 1202/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2016 - acc: 0.9800 - val_loss: 3.4158 - val_acc: 0.5400\n",
            "Epoch 1203/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1567 - acc: 1.0000 - val_loss: 2.8547 - val_acc: 0.6000\n",
            "Epoch 1204/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1681 - acc: 0.9900 - val_loss: 2.7807 - val_acc: 0.5900\n",
            "Epoch 1205/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1541 - acc: 1.0000 - val_loss: 2.8994 - val_acc: 0.5700\n",
            "Epoch 1206/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1538 - acc: 1.0000 - val_loss: 3.0834 - val_acc: 0.5400\n",
            "Epoch 1207/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1658 - acc: 0.9950 - val_loss: 3.1998 - val_acc: 0.5400\n",
            "Epoch 1208/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1497 - acc: 1.0000 - val_loss: 3.2220 - val_acc: 0.5300\n",
            "Epoch 1209/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1907 - acc: 0.9850 - val_loss: 3.3226 - val_acc: 0.5300\n",
            "Epoch 1210/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1420 - acc: 1.0000 - val_loss: 3.1069 - val_acc: 0.5700\n",
            "Epoch 1211/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1423 - acc: 1.0000 - val_loss: 2.6490 - val_acc: 0.5900\n",
            "Epoch 1212/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.2281 - acc: 0.9900 - val_loss: 2.9684 - val_acc: 0.5600\n",
            "Epoch 1213/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1331 - acc: 1.0000 - val_loss: 2.5122 - val_acc: 0.5900\n",
            "Epoch 1214/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1366 - acc: 1.0000 - val_loss: 2.5227 - val_acc: 0.6000\n",
            "Epoch 1215/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1321 - acc: 1.0000 - val_loss: 2.8897 - val_acc: 0.5500\n",
            "Epoch 1216/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1357 - acc: 0.9950 - val_loss: 2.6707 - val_acc: 0.5600\n",
            "Epoch 1217/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1279 - acc: 1.0000 - val_loss: 3.1938 - val_acc: 0.5300\n",
            "Epoch 1218/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1257 - acc: 1.0000 - val_loss: 2.9458 - val_acc: 0.5600\n",
            "Epoch 1219/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1229 - acc: 1.0000 - val_loss: 3.2988 - val_acc: 0.5200\n",
            "Epoch 1220/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1188 - acc: 1.0000 - val_loss: 2.8834 - val_acc: 0.5500\n",
            "Epoch 1221/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1165 - acc: 1.0000 - val_loss: 3.5093 - val_acc: 0.5100\n",
            "Epoch 1222/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1129 - acc: 1.0000 - val_loss: 3.4213 - val_acc: 0.5100\n",
            "Epoch 1223/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1098 - acc: 1.0000 - val_loss: 3.4716 - val_acc: 0.5100\n",
            "Epoch 1224/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1076 - acc: 1.0000 - val_loss: 3.4524 - val_acc: 0.5100\n",
            "Epoch 1225/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1055 - acc: 1.0000 - val_loss: 3.1862 - val_acc: 0.5300\n",
            "Epoch 1226/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1113 - acc: 0.9950 - val_loss: 3.6507 - val_acc: 0.5100\n",
            "Epoch 1227/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0997 - acc: 1.0000 - val_loss: 3.7446 - val_acc: 0.5100\n",
            "Epoch 1228/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0978 - acc: 1.0000 - val_loss: 3.5083 - val_acc: 0.5100\n",
            "Epoch 1229/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0949 - acc: 1.0000 - val_loss: 3.4564 - val_acc: 0.5100\n",
            "Epoch 1230/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0919 - acc: 1.0000 - val_loss: 3.6237 - val_acc: 0.5100\n",
            "Epoch 1231/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0893 - acc: 1.0000 - val_loss: 3.6239 - val_acc: 0.5100\n",
            "Epoch 1232/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1044 - acc: 0.9950 - val_loss: 3.4516 - val_acc: 0.5100\n",
            "Epoch 1233/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0879 - acc: 1.0000 - val_loss: 3.5478 - val_acc: 0.5000\n",
            "Epoch 1234/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1762 - acc: 0.9650 - val_loss: 3.3882 - val_acc: 0.5000\n",
            "Epoch 1235/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0843 - acc: 1.0000 - val_loss: 4.0885 - val_acc: 0.4900\n",
            "Epoch 1236/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.1387 - acc: 0.9850 - val_loss: 3.9513 - val_acc: 0.5100\n",
            "Epoch 1237/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0806 - acc: 1.0000 - val_loss: 3.7385 - val_acc: 0.5100\n",
            "Epoch 1238/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0769 - acc: 1.0000 - val_loss: 3.6309 - val_acc: 0.5100\n",
            "Epoch 1239/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0913 - acc: 0.9950 - val_loss: 3.4536 - val_acc: 0.5100\n",
            "Epoch 1240/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0885 - acc: 0.9950 - val_loss: 3.2843 - val_acc: 0.5100\n",
            "Epoch 1241/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0715 - acc: 1.0000 - val_loss: 2.9767 - val_acc: 0.5500\n",
            "Epoch 1242/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0692 - acc: 1.0000 - val_loss: 2.8954 - val_acc: 0.5200\n",
            "Epoch 1243/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0697 - acc: 1.0000 - val_loss: 2.8623 - val_acc: 0.5200\n",
            "Epoch 1244/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0647 - acc: 1.0000 - val_loss: 2.7209 - val_acc: 0.5100\n",
            "Epoch 1245/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0608 - acc: 1.0000 - val_loss: 2.7275 - val_acc: 0.5200\n",
            "Epoch 1246/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0781 - acc: 0.9950 - val_loss: 2.3377 - val_acc: 0.5400\n",
            "Epoch 1247/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0729 - acc: 0.9900 - val_loss: 1.9828 - val_acc: 0.5900\n",
            "Epoch 1248/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0663 - acc: 1.0000 - val_loss: 1.9861 - val_acc: 0.6100\n",
            "Epoch 1249/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0569 - acc: 1.0000 - val_loss: 2.1216 - val_acc: 0.5400\n",
            "Epoch 1250/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0589 - acc: 1.0000 - val_loss: 2.1882 - val_acc: 0.5400\n",
            "Epoch 1251/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0652 - acc: 0.9950 - val_loss: 2.4526 - val_acc: 0.5300\n",
            "Epoch 1252/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0507 - acc: 1.0000 - val_loss: 2.6617 - val_acc: 0.5100\n",
            "Epoch 1253/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0482 - acc: 1.0000 - val_loss: 2.5780 - val_acc: 0.5300\n",
            "Epoch 1254/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0461 - acc: 1.0000 - val_loss: 2.7198 - val_acc: 0.5300\n",
            "Epoch 1255/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0443 - acc: 1.0000 - val_loss: 2.9492 - val_acc: 0.5000\n",
            "Epoch 1256/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0518 - acc: 0.9900 - val_loss: 2.8063 - val_acc: 0.5200\n",
            "Epoch 1257/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0387 - acc: 1.0000 - val_loss: 2.9193 - val_acc: 0.5000\n",
            "Epoch 1258/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0495 - acc: 0.9950 - val_loss: 2.7496 - val_acc: 0.5100\n",
            "Epoch 1259/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0354 - acc: 1.0000 - val_loss: 2.7641 - val_acc: 0.5000\n",
            "Epoch 1260/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0342 - acc: 1.0000 - val_loss: 2.8237 - val_acc: 0.5000\n",
            "Epoch 1261/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0302 - acc: 1.0000 - val_loss: 2.8700 - val_acc: 0.5000\n",
            "Epoch 1262/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0304 - acc: 1.0000 - val_loss: 2.7393 - val_acc: 0.5000\n",
            "Epoch 1263/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0239 - acc: 1.0000 - val_loss: 2.8134 - val_acc: 0.5000\n",
            "Epoch 1264/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0211 - acc: 1.0000 - val_loss: 2.7162 - val_acc: 0.5000\n",
            "Epoch 1265/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0195 - acc: 1.0000 - val_loss: 2.7422 - val_acc: 0.5000\n",
            "Epoch 1266/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0163 - acc: 1.0000 - val_loss: 2.7235 - val_acc: 0.5000\n",
            "Epoch 1267/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0144 - acc: 1.0000 - val_loss: 2.7481 - val_acc: 0.5000\n",
            "Epoch 1268/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0111 - acc: 1.0000 - val_loss: 2.6119 - val_acc: 0.5000\n",
            "Epoch 1269/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0091 - acc: 1.0000 - val_loss: 2.6096 - val_acc: 0.5000\n",
            "Epoch 1270/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0062 - acc: 1.0000 - val_loss: 2.6299 - val_acc: 0.5000\n",
            "Epoch 1271/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0046 - acc: 1.0000 - val_loss: 2.6106 - val_acc: 0.5000\n",
            "Epoch 1272/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0013 - acc: 1.0000 - val_loss: 2.5799 - val_acc: 0.5000\n",
            "Epoch 1273/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9988 - acc: 1.0000 - val_loss: 2.6248 - val_acc: 0.5000\n",
            "Epoch 1274/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9963 - acc: 1.0000 - val_loss: 2.5489 - val_acc: 0.5000\n",
            "Epoch 1275/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0357 - acc: 0.9950 - val_loss: 2.5063 - val_acc: 0.5000\n",
            "Epoch 1276/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9919 - acc: 1.0000 - val_loss: 2.2635 - val_acc: 0.5000\n",
            "Epoch 1277/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 1.0490 - acc: 0.9850 - val_loss: 2.5308 - val_acc: 0.5000\n",
            "Epoch 1278/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9952 - acc: 1.0000 - val_loss: 2.3876 - val_acc: 0.5000\n",
            "Epoch 1279/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9887 - acc: 1.0000 - val_loss: 2.3996 - val_acc: 0.5000\n",
            "Epoch 1280/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9866 - acc: 1.0000 - val_loss: 2.3666 - val_acc: 0.5000\n",
            "Epoch 1281/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9847 - acc: 1.0000 - val_loss: 2.4329 - val_acc: 0.5000\n",
            "Epoch 1282/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9857 - acc: 1.0000 - val_loss: 2.1546 - val_acc: 0.5200\n",
            "Epoch 1283/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9798 - acc: 1.0000 - val_loss: 2.2530 - val_acc: 0.5300\n",
            "Epoch 1284/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9782 - acc: 1.0000 - val_loss: 2.3309 - val_acc: 0.5400\n",
            "Epoch 1285/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9760 - acc: 1.0000 - val_loss: 2.3733 - val_acc: 0.5400\n",
            "Epoch 1286/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9744 - acc: 1.0000 - val_loss: 2.8198 - val_acc: 0.5200\n",
            "Epoch 1287/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9714 - acc: 1.0000 - val_loss: 2.4228 - val_acc: 0.5300\n",
            "Epoch 1288/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9684 - acc: 1.0000 - val_loss: 2.6174 - val_acc: 0.5200\n",
            "Epoch 1289/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9660 - acc: 1.0000 - val_loss: 2.6713 - val_acc: 0.5200\n",
            "Epoch 1290/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9633 - acc: 1.0000 - val_loss: 3.1583 - val_acc: 0.5100\n",
            "Epoch 1291/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9610 - acc: 1.0000 - val_loss: 2.6337 - val_acc: 0.5200\n",
            "Epoch 1292/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9599 - acc: 1.0000 - val_loss: 2.9562 - val_acc: 0.5100\n",
            "Epoch 1293/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9563 - acc: 1.0000 - val_loss: 2.9998 - val_acc: 0.5000\n",
            "Epoch 1294/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9539 - acc: 1.0000 - val_loss: 3.1067 - val_acc: 0.5000\n",
            "Epoch 1295/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9514 - acc: 1.0000 - val_loss: 2.8811 - val_acc: 0.5100\n",
            "Epoch 1296/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9500 - acc: 1.0000 - val_loss: 2.9927 - val_acc: 0.5000\n",
            "Epoch 1297/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9472 - acc: 1.0000 - val_loss: 2.8731 - val_acc: 0.5100\n",
            "Epoch 1298/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9444 - acc: 1.0000 - val_loss: 3.0473 - val_acc: 0.5000\n",
            "Epoch 1299/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9749 - acc: 0.9900 - val_loss: 3.2043 - val_acc: 0.5000\n",
            "Epoch 1300/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9476 - acc: 0.9950 - val_loss: 2.7310 - val_acc: 0.5400\n",
            "Epoch 1301/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9738 - acc: 0.9900 - val_loss: 3.6726 - val_acc: 0.5000\n",
            "Epoch 1302/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9456 - acc: 1.0000 - val_loss: 3.0057 - val_acc: 0.5300\n",
            "Epoch 1303/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9372 - acc: 1.0000 - val_loss: 3.0099 - val_acc: 0.5300\n",
            "Epoch 1304/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9357 - acc: 1.0000 - val_loss: 2.9347 - val_acc: 0.5400\n",
            "Epoch 1305/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9735 - acc: 0.9950 - val_loss: 3.6125 - val_acc: 0.5100\n",
            "Epoch 1306/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9569 - acc: 0.9950 - val_loss: 2.7474 - val_acc: 0.5500\n",
            "Epoch 1307/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9363 - acc: 0.9950 - val_loss: 3.1320 - val_acc: 0.5400\n",
            "Epoch 1308/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9304 - acc: 1.0000 - val_loss: 2.4657 - val_acc: 0.6100\n",
            "Epoch 1309/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9580 - acc: 0.9900 - val_loss: 3.1295 - val_acc: 0.5100\n",
            "Epoch 1310/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9294 - acc: 1.0000 - val_loss: 3.2679 - val_acc: 0.5100\n",
            "Epoch 1311/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9290 - acc: 0.9950 - val_loss: 3.0923 - val_acc: 0.5100\n",
            "Epoch 1312/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9218 - acc: 1.0000 - val_loss: 2.9049 - val_acc: 0.5000\n",
            "Epoch 1313/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9196 - acc: 1.0000 - val_loss: 2.7799 - val_acc: 0.5000\n",
            "Epoch 1314/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9186 - acc: 1.0000 - val_loss: 2.5054 - val_acc: 0.5100\n",
            "Epoch 1315/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9160 - acc: 1.0000 - val_loss: 2.6389 - val_acc: 0.5000\n",
            "Epoch 1316/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9136 - acc: 1.0000 - val_loss: 2.5587 - val_acc: 0.5000\n",
            "Epoch 1317/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9105 - acc: 1.0000 - val_loss: 2.6734 - val_acc: 0.5000\n",
            "Epoch 1318/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9085 - acc: 1.0000 - val_loss: 2.8335 - val_acc: 0.5000\n",
            "Epoch 1319/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9059 - acc: 1.0000 - val_loss: 2.6098 - val_acc: 0.5000\n",
            "Epoch 1320/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9036 - acc: 1.0000 - val_loss: 2.7246 - val_acc: 0.5000\n",
            "Epoch 1321/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9019 - acc: 1.0000 - val_loss: 2.5401 - val_acc: 0.5000\n",
            "Epoch 1322/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9183 - acc: 0.9950 - val_loss: 2.7250 - val_acc: 0.5000\n",
            "Epoch 1323/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8968 - acc: 1.0000 - val_loss: 2.6582 - val_acc: 0.5000\n",
            "Epoch 1324/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8950 - acc: 1.0000 - val_loss: 2.7527 - val_acc: 0.5000\n",
            "Epoch 1325/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8936 - acc: 1.0000 - val_loss: 2.5640 - val_acc: 0.5000\n",
            "Epoch 1326/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8902 - acc: 1.0000 - val_loss: 2.7453 - val_acc: 0.5000\n",
            "Epoch 1327/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8880 - acc: 1.0000 - val_loss: 2.6036 - val_acc: 0.5000\n",
            "Epoch 1328/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8870 - acc: 1.0000 - val_loss: 2.6338 - val_acc: 0.5000\n",
            "Epoch 1329/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.9048 - acc: 0.9850 - val_loss: 2.6271 - val_acc: 0.5000\n",
            "Epoch 1330/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8824 - acc: 1.0000 - val_loss: 2.8194 - val_acc: 0.5000\n",
            "Epoch 1331/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8809 - acc: 1.0000 - val_loss: 2.9020 - val_acc: 0.5000\n",
            "Epoch 1332/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8803 - acc: 1.0000 - val_loss: 2.7042 - val_acc: 0.5000\n",
            "Epoch 1333/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8893 - acc: 0.9950 - val_loss: 2.8198 - val_acc: 0.5000\n",
            "Epoch 1334/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8762 - acc: 1.0000 - val_loss: 3.2223 - val_acc: 0.5000\n",
            "Epoch 1335/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8749 - acc: 1.0000 - val_loss: 3.0487 - val_acc: 0.5000\n",
            "Epoch 1336/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8720 - acc: 1.0000 - val_loss: 3.2600 - val_acc: 0.5000\n",
            "Epoch 1337/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8698 - acc: 1.0000 - val_loss: 3.2758 - val_acc: 0.5000\n",
            "Epoch 1338/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8675 - acc: 1.0000 - val_loss: 3.2095 - val_acc: 0.5000\n",
            "Epoch 1339/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8662 - acc: 1.0000 - val_loss: 3.1772 - val_acc: 0.5000\n",
            "Epoch 1340/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8634 - acc: 1.0000 - val_loss: 3.0650 - val_acc: 0.5000\n",
            "Epoch 1341/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8609 - acc: 1.0000 - val_loss: 3.1815 - val_acc: 0.5000\n",
            "Epoch 1342/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8586 - acc: 1.0000 - val_loss: 3.1230 - val_acc: 0.5000\n",
            "Epoch 1343/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8565 - acc: 1.0000 - val_loss: 3.3378 - val_acc: 0.5000\n",
            "Epoch 1344/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8547 - acc: 1.0000 - val_loss: 3.0328 - val_acc: 0.5000\n",
            "Epoch 1345/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8522 - acc: 1.0000 - val_loss: 3.1404 - val_acc: 0.5000\n",
            "Epoch 1346/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8521 - acc: 1.0000 - val_loss: 3.1802 - val_acc: 0.5000\n",
            "Epoch 1347/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8478 - acc: 1.0000 - val_loss: 2.7178 - val_acc: 0.5000\n",
            "Epoch 1348/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8456 - acc: 1.0000 - val_loss: 2.7181 - val_acc: 0.5000\n",
            "Epoch 1349/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8436 - acc: 1.0000 - val_loss: 2.7742 - val_acc: 0.5000\n",
            "Epoch 1350/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8412 - acc: 1.0000 - val_loss: 2.8696 - val_acc: 0.5000\n",
            "Epoch 1351/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8392 - acc: 1.0000 - val_loss: 2.8046 - val_acc: 0.5000\n",
            "Epoch 1352/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8369 - acc: 1.0000 - val_loss: 2.9009 - val_acc: 0.5000\n",
            "Epoch 1353/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8347 - acc: 1.0000 - val_loss: 2.7011 - val_acc: 0.5000\n",
            "Epoch 1354/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8325 - acc: 1.0000 - val_loss: 2.8825 - val_acc: 0.5000\n",
            "Epoch 1355/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8304 - acc: 1.0000 - val_loss: 2.8850 - val_acc: 0.5000\n",
            "Epoch 1356/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8284 - acc: 1.0000 - val_loss: 2.7737 - val_acc: 0.5000\n",
            "Epoch 1357/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8327 - acc: 0.9950 - val_loss: 2.9915 - val_acc: 0.5000\n",
            "Epoch 1358/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8245 - acc: 1.0000 - val_loss: 2.3762 - val_acc: 0.5000\n",
            "Epoch 1359/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8498 - acc: 0.9900 - val_loss: 2.5918 - val_acc: 0.5000\n",
            "Epoch 1360/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8251 - acc: 1.0000 - val_loss: 2.6327 - val_acc: 0.5000\n",
            "Epoch 1361/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8227 - acc: 1.0000 - val_loss: 2.7762 - val_acc: 0.5000\n",
            "Epoch 1362/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8215 - acc: 1.0000 - val_loss: 2.7194 - val_acc: 0.5100\n",
            "Epoch 1363/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8194 - acc: 1.0000 - val_loss: 2.8209 - val_acc: 0.5000\n",
            "Epoch 1364/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8174 - acc: 1.0000 - val_loss: 2.4682 - val_acc: 0.5200\n",
            "Epoch 1365/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8156 - acc: 1.0000 - val_loss: 2.7673 - val_acc: 0.5000\n",
            "Epoch 1366/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8135 - acc: 1.0000 - val_loss: 2.5486 - val_acc: 0.5200\n",
            "Epoch 1367/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8112 - acc: 1.0000 - val_loss: 2.9720 - val_acc: 0.5000\n",
            "Epoch 1368/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8100 - acc: 1.0000 - val_loss: 3.0269 - val_acc: 0.5000\n",
            "Epoch 1369/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8069 - acc: 1.0000 - val_loss: 2.7376 - val_acc: 0.5100\n",
            "Epoch 1370/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8046 - acc: 1.0000 - val_loss: 2.7811 - val_acc: 0.5000\n",
            "Epoch 1371/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8220 - acc: 0.9950 - val_loss: 3.1312 - val_acc: 0.5100\n",
            "Epoch 1372/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8087 - acc: 0.9950 - val_loss: 2.7092 - val_acc: 0.5700\n",
            "Epoch 1373/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8112 - acc: 0.9950 - val_loss: 3.5715 - val_acc: 0.5300\n",
            "Epoch 1374/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8019 - acc: 1.0000 - val_loss: 4.2598 - val_acc: 0.5000\n",
            "Epoch 1375/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8013 - acc: 1.0000 - val_loss: 3.5390 - val_acc: 0.5100\n",
            "Epoch 1376/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8441 - acc: 0.9900 - val_loss: 3.7566 - val_acc: 0.5100\n",
            "Epoch 1377/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7981 - acc: 1.0000 - val_loss: 4.2339 - val_acc: 0.5000\n",
            "Epoch 1378/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7966 - acc: 1.0000 - val_loss: 3.7464 - val_acc: 0.5000\n",
            "Epoch 1379/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7955 - acc: 1.0000 - val_loss: 4.3101 - val_acc: 0.5000\n",
            "Epoch 1380/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8111 - acc: 0.9900 - val_loss: 4.1764 - val_acc: 0.5000\n",
            "Epoch 1381/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7910 - acc: 1.0000 - val_loss: 7.8024 - val_acc: 0.1700\n",
            "Epoch 1382/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8076 - acc: 0.9950 - val_loss: 6.5127 - val_acc: 0.2400\n",
            "Epoch 1383/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8335 - acc: 0.9900 - val_loss: 7.1939 - val_acc: 0.3600\n",
            "Epoch 1384/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8318 - acc: 0.9950 - val_loss: 13.4483 - val_acc: 0.0500\n",
            "Epoch 1385/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7931 - acc: 1.0000 - val_loss: 14.2665 - val_acc: 0.0400\n",
            "Epoch 1386/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7932 - acc: 1.0000 - val_loss: 13.6308 - val_acc: 0.0500\n",
            "Epoch 1387/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7890 - acc: 1.0000 - val_loss: 8.3821 - val_acc: 0.3700\n",
            "Epoch 1388/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7874 - acc: 1.0000 - val_loss: 6.4175 - val_acc: 0.4900\n",
            "Epoch 1389/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7873 - acc: 1.0000 - val_loss: 6.0292 - val_acc: 0.4900\n",
            "Epoch 1390/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7837 - acc: 1.0000 - val_loss: 5.7949 - val_acc: 0.5000\n",
            "Epoch 1391/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7817 - acc: 1.0000 - val_loss: 5.4229 - val_acc: 0.5000\n",
            "Epoch 1392/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7801 - acc: 1.0000 - val_loss: 5.0430 - val_acc: 0.5000\n",
            "Epoch 1393/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7779 - acc: 1.0000 - val_loss: 4.4381 - val_acc: 0.5000\n",
            "Epoch 1394/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7758 - acc: 1.0000 - val_loss: 4.3958 - val_acc: 0.5000\n",
            "Epoch 1395/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7739 - acc: 1.0000 - val_loss: 4.2533 - val_acc: 0.5000\n",
            "Epoch 1396/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7791 - acc: 0.9950 - val_loss: 4.0274 - val_acc: 0.5000\n",
            "Epoch 1397/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7701 - acc: 1.0000 - val_loss: 3.6178 - val_acc: 0.5000\n",
            "Epoch 1398/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7744 - acc: 0.9950 - val_loss: 3.6260 - val_acc: 0.5000\n",
            "Epoch 1399/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7692 - acc: 1.0000 - val_loss: 3.4708 - val_acc: 0.5000\n",
            "Epoch 1400/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7722 - acc: 1.0000 - val_loss: 3.7601 - val_acc: 0.5000\n",
            "Epoch 1401/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8025 - acc: 0.9950 - val_loss: 3.6180 - val_acc: 0.5000\n",
            "Epoch 1402/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8498 - acc: 0.9900 - val_loss: 3.4298 - val_acc: 0.4600\n",
            "Epoch 1403/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8838 - acc: 0.9800 - val_loss: 2.8555 - val_acc: 0.4600\n",
            "Epoch 1404/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7830 - acc: 0.9950 - val_loss: 2.7014 - val_acc: 0.4400\n",
            "Epoch 1405/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7673 - acc: 1.0000 - val_loss: 3.7573 - val_acc: 0.4500\n",
            "Epoch 1406/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7699 - acc: 1.0000 - val_loss: 3.6296 - val_acc: 0.3500\n",
            "Epoch 1407/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8479 - acc: 0.9900 - val_loss: 3.5014 - val_acc: 0.5000\n",
            "Epoch 1408/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7648 - acc: 1.0000 - val_loss: 3.7422 - val_acc: 0.5000\n",
            "Epoch 1409/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7676 - acc: 1.0000 - val_loss: 3.7579 - val_acc: 0.5000\n",
            "Epoch 1410/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7876 - acc: 0.9950 - val_loss: 3.6582 - val_acc: 0.5000\n",
            "Epoch 1411/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7808 - acc: 0.9950 - val_loss: 3.4466 - val_acc: 0.5000\n",
            "Epoch 1412/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7707 - acc: 0.9950 - val_loss: 2.7096 - val_acc: 0.5100\n",
            "Epoch 1413/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8041 - acc: 0.9800 - val_loss: 4.1414 - val_acc: 0.5000\n",
            "Epoch 1414/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7593 - acc: 1.0000 - val_loss: 6.4842 - val_acc: 0.3600\n",
            "Epoch 1415/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7579 - acc: 1.0000 - val_loss: 7.7513 - val_acc: 0.2300\n",
            "Epoch 1416/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7565 - acc: 1.0000 - val_loss: 12.7780 - val_acc: 0.0200\n",
            "Epoch 1417/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7553 - acc: 1.0000 - val_loss: 7.0578 - val_acc: 0.2700\n",
            "Epoch 1418/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7597 - acc: 0.9950 - val_loss: 10.5883 - val_acc: 0.0100\n",
            "Epoch 1419/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7840 - acc: 0.9950 - val_loss: 4.2530 - val_acc: 0.4800\n",
            "Epoch 1420/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7499 - acc: 1.0000 - val_loss: 3.4664 - val_acc: 0.5000\n",
            "Epoch 1421/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8420 - acc: 0.9900 - val_loss: 3.7815 - val_acc: 0.5000\n",
            "Epoch 1422/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7535 - acc: 1.0000 - val_loss: 4.6605 - val_acc: 0.5000\n",
            "Epoch 1423/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7553 - acc: 1.0000 - val_loss: 4.7568 - val_acc: 0.5000\n",
            "Epoch 1424/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7548 - acc: 1.0000 - val_loss: 4.0743 - val_acc: 0.5000\n",
            "Epoch 1425/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7542 - acc: 1.0000 - val_loss: 4.1556 - val_acc: 0.5000\n",
            "Epoch 1426/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7528 - acc: 1.0000 - val_loss: 3.8540 - val_acc: 0.5000\n",
            "Epoch 1427/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7511 - acc: 1.0000 - val_loss: 3.2528 - val_acc: 0.5000\n",
            "Epoch 1428/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7492 - acc: 1.0000 - val_loss: 2.9265 - val_acc: 0.5000\n",
            "Epoch 1429/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8114 - acc: 0.9900 - val_loss: 3.2282 - val_acc: 0.5000\n",
            "Epoch 1430/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7683 - acc: 0.9950 - val_loss: 4.0292 - val_acc: 0.5000\n",
            "Epoch 1431/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7478 - acc: 1.0000 - val_loss: 4.3388 - val_acc: 0.5000\n",
            "Epoch 1432/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7503 - acc: 0.9950 - val_loss: 4.1437 - val_acc: 0.5000\n",
            "Epoch 1433/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7490 - acc: 0.9950 - val_loss: 3.8085 - val_acc: 0.5000\n",
            "Epoch 1434/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7438 - acc: 1.0000 - val_loss: 3.5806 - val_acc: 0.5000\n",
            "Epoch 1435/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7451 - acc: 0.9950 - val_loss: 3.1152 - val_acc: 0.5000\n",
            "Epoch 1436/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7390 - acc: 1.0000 - val_loss: 2.3626 - val_acc: 0.5000\n",
            "Epoch 1437/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7375 - acc: 1.0000 - val_loss: 2.1616 - val_acc: 0.5000\n",
            "Epoch 1438/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7376 - acc: 1.0000 - val_loss: 2.3281 - val_acc: 0.5000\n",
            "Epoch 1439/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7347 - acc: 1.0000 - val_loss: 1.9291 - val_acc: 0.5000\n",
            "Epoch 1440/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7330 - acc: 1.0000 - val_loss: 2.1037 - val_acc: 0.5000\n",
            "Epoch 1441/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7310 - acc: 1.0000 - val_loss: 2.1436 - val_acc: 0.5000\n",
            "Epoch 1442/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7298 - acc: 1.0000 - val_loss: 2.1170 - val_acc: 0.5000\n",
            "Epoch 1443/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7276 - acc: 1.0000 - val_loss: 2.1135 - val_acc: 0.5000\n",
            "Epoch 1444/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7254 - acc: 1.0000 - val_loss: 2.2594 - val_acc: 0.5000\n",
            "Epoch 1445/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7235 - acc: 1.0000 - val_loss: 2.2190 - val_acc: 0.5000\n",
            "Epoch 1446/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7855 - acc: 0.9850 - val_loss: 2.3151 - val_acc: 0.5000\n",
            "Epoch 1447/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7209 - acc: 1.0000 - val_loss: 2.9957 - val_acc: 0.5000\n",
            "Epoch 1448/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7202 - acc: 1.0000 - val_loss: 3.5150 - val_acc: 0.5000\n",
            "Epoch 1449/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7191 - acc: 1.0000 - val_loss: 3.5989 - val_acc: 0.5000\n",
            "Epoch 1450/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7196 - acc: 1.0000 - val_loss: 3.4767 - val_acc: 0.5200\n",
            "Epoch 1451/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7156 - acc: 1.0000 - val_loss: 3.6554 - val_acc: 0.5200\n",
            "Epoch 1452/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7144 - acc: 1.0000 - val_loss: 3.6498 - val_acc: 0.5000\n",
            "Epoch 1453/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7138 - acc: 1.0000 - val_loss: 3.4847 - val_acc: 0.5000\n",
            "Epoch 1454/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7152 - acc: 0.9950 - val_loss: 3.7939 - val_acc: 0.5000\n",
            "Epoch 1455/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7083 - acc: 1.0000 - val_loss: 3.6182 - val_acc: 0.5000\n",
            "Epoch 1456/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7094 - acc: 1.0000 - val_loss: 3.6027 - val_acc: 0.5000\n",
            "Epoch 1457/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7065 - acc: 1.0000 - val_loss: 3.1087 - val_acc: 0.5000\n",
            "Epoch 1458/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7035 - acc: 1.0000 - val_loss: 3.0475 - val_acc: 0.5000\n",
            "Epoch 1459/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7016 - acc: 1.0000 - val_loss: 3.0335 - val_acc: 0.5000\n",
            "Epoch 1460/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6998 - acc: 1.0000 - val_loss: 2.9422 - val_acc: 0.5000\n",
            "Epoch 1461/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6980 - acc: 1.0000 - val_loss: 3.0784 - val_acc: 0.5000\n",
            "Epoch 1462/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7232 - acc: 0.9950 - val_loss: 3.2604 - val_acc: 0.5000\n",
            "Epoch 1463/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6958 - acc: 1.0000 - val_loss: 3.9833 - val_acc: 0.5000\n",
            "Epoch 1464/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7553 - acc: 0.9850 - val_loss: 3.9276 - val_acc: 0.5000\n",
            "Epoch 1465/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6945 - acc: 1.0000 - val_loss: 3.8423 - val_acc: 0.5000\n",
            "Epoch 1466/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6953 - acc: 1.0000 - val_loss: 3.3148 - val_acc: 0.5000\n",
            "Epoch 1467/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6917 - acc: 1.0000 - val_loss: 3.4871 - val_acc: 0.5000\n",
            "Epoch 1468/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6902 - acc: 1.0000 - val_loss: 3.7488 - val_acc: 0.5000\n",
            "Epoch 1469/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6885 - acc: 1.0000 - val_loss: 3.7959 - val_acc: 0.5000\n",
            "Epoch 1470/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6869 - acc: 1.0000 - val_loss: 3.3558 - val_acc: 0.5000\n",
            "Epoch 1471/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6853 - acc: 1.0000 - val_loss: 3.5265 - val_acc: 0.5000\n",
            "Epoch 1472/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6835 - acc: 1.0000 - val_loss: 3.6662 - val_acc: 0.5000\n",
            "Epoch 1473/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6820 - acc: 1.0000 - val_loss: 3.4918 - val_acc: 0.5000\n",
            "Epoch 1474/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6807 - acc: 1.0000 - val_loss: 3.3567 - val_acc: 0.5000\n",
            "Epoch 1475/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6784 - acc: 1.0000 - val_loss: 3.3850 - val_acc: 0.5000\n",
            "Epoch 1476/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6767 - acc: 1.0000 - val_loss: 3.3073 - val_acc: 0.5000\n",
            "Epoch 1477/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6752 - acc: 1.0000 - val_loss: 3.2202 - val_acc: 0.5000\n",
            "Epoch 1478/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6733 - acc: 1.0000 - val_loss: 3.4311 - val_acc: 0.5000\n",
            "Epoch 1479/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6717 - acc: 1.0000 - val_loss: 3.5099 - val_acc: 0.5000\n",
            "Epoch 1480/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6701 - acc: 1.0000 - val_loss: 3.3414 - val_acc: 0.5000\n",
            "Epoch 1481/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6684 - acc: 1.0000 - val_loss: 3.2444 - val_acc: 0.5000\n",
            "Epoch 1482/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6667 - acc: 1.0000 - val_loss: 3.1997 - val_acc: 0.5000\n",
            "Epoch 1483/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6651 - acc: 1.0000 - val_loss: 3.3347 - val_acc: 0.5000\n",
            "Epoch 1484/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6633 - acc: 1.0000 - val_loss: 3.2560 - val_acc: 0.5000\n",
            "Epoch 1485/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6623 - acc: 1.0000 - val_loss: 3.2850 - val_acc: 0.5000\n",
            "Epoch 1486/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6601 - acc: 1.0000 - val_loss: 3.0540 - val_acc: 0.5000\n",
            "Epoch 1487/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6584 - acc: 1.0000 - val_loss: 3.2715 - val_acc: 0.5000\n",
            "Epoch 1488/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6568 - acc: 1.0000 - val_loss: 3.0644 - val_acc: 0.5000\n",
            "Epoch 1489/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6633 - acc: 0.9950 - val_loss: 3.1663 - val_acc: 0.5000\n",
            "Epoch 1490/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7166 - acc: 0.9900 - val_loss: 2.8394 - val_acc: 0.5000\n",
            "Epoch 1491/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6533 - acc: 1.0000 - val_loss: 3.3585 - val_acc: 0.5100\n",
            "Epoch 1492/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7209 - acc: 0.9750 - val_loss: 3.6045 - val_acc: 0.5000\n",
            "Epoch 1493/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7022 - acc: 0.9800 - val_loss: 2.4851 - val_acc: 0.5000\n",
            "Epoch 1494/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.8111 - acc: 0.9650 - val_loss: 1.7294 - val_acc: 0.6600\n",
            "Epoch 1495/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6566 - acc: 1.0000 - val_loss: 2.8316 - val_acc: 0.5000\n",
            "Epoch 1496/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7021 - acc: 0.9700 - val_loss: 2.4456 - val_acc: 0.5200\n",
            "Epoch 1497/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6606 - acc: 1.0000 - val_loss: 1.7013 - val_acc: 0.5400\n",
            "Epoch 1498/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6655 - acc: 1.0000 - val_loss: 1.4703 - val_acc: 0.5700\n",
            "Epoch 1499/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6615 - acc: 1.0000 - val_loss: 1.3500 - val_acc: 0.6000\n",
            "Epoch 1500/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7585 - acc: 0.9800 - val_loss: 2.1093 - val_acc: 0.5000\n",
            "Epoch 1501/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6606 - acc: 1.0000 - val_loss: 2.7886 - val_acc: 0.5000\n",
            "Epoch 1502/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6695 - acc: 1.0000 - val_loss: 2.9703 - val_acc: 0.5000\n",
            "Epoch 1503/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6594 - acc: 1.0000 - val_loss: 3.2206 - val_acc: 0.5000\n",
            "Epoch 1504/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6580 - acc: 1.0000 - val_loss: 3.1542 - val_acc: 0.5000\n",
            "Epoch 1505/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6570 - acc: 1.0000 - val_loss: 3.0380 - val_acc: 0.5000\n",
            "Epoch 1506/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6642 - acc: 0.9950 - val_loss: 3.1396 - val_acc: 0.5000\n",
            "Epoch 1507/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6546 - acc: 1.0000 - val_loss: 2.9908 - val_acc: 0.5000\n",
            "Epoch 1508/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6504 - acc: 1.0000 - val_loss: 3.0075 - val_acc: 0.5000\n",
            "Epoch 1509/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6492 - acc: 1.0000 - val_loss: 3.1730 - val_acc: 0.5000\n",
            "Epoch 1510/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6470 - acc: 1.0000 - val_loss: 3.6125 - val_acc: 0.5000\n",
            "Epoch 1511/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6455 - acc: 1.0000 - val_loss: 3.7645 - val_acc: 0.5000\n",
            "Epoch 1512/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6442 - acc: 1.0000 - val_loss: 3.3608 - val_acc: 0.5000\n",
            "Epoch 1513/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6423 - acc: 1.0000 - val_loss: 3.8347 - val_acc: 0.5000\n",
            "Epoch 1514/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6407 - acc: 1.0000 - val_loss: 3.2127 - val_acc: 0.5000\n",
            "Epoch 1515/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6390 - acc: 1.0000 - val_loss: 3.5220 - val_acc: 0.5000\n",
            "Epoch 1516/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6377 - acc: 1.0000 - val_loss: 3.4526 - val_acc: 0.5000\n",
            "Epoch 1517/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6363 - acc: 1.0000 - val_loss: 3.5131 - val_acc: 0.5000\n",
            "Epoch 1518/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6558 - acc: 0.9900 - val_loss: 3.9276 - val_acc: 0.5000\n",
            "Epoch 1519/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6350 - acc: 1.0000 - val_loss: 4.1254 - val_acc: 0.5000\n",
            "Epoch 1520/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6358 - acc: 1.0000 - val_loss: 4.3873 - val_acc: 0.5000\n",
            "Epoch 1521/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6430 - acc: 1.0000 - val_loss: 3.9536 - val_acc: 0.5000\n",
            "Epoch 1522/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6399 - acc: 1.0000 - val_loss: 4.1359 - val_acc: 0.5000\n",
            "Epoch 1523/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6365 - acc: 1.0000 - val_loss: 4.1168 - val_acc: 0.5000\n",
            "Epoch 1524/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6459 - acc: 0.9900 - val_loss: 4.0630 - val_acc: 0.5000\n",
            "Epoch 1525/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6306 - acc: 1.0000 - val_loss: 3.9945 - val_acc: 0.5000\n",
            "Epoch 1526/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6298 - acc: 1.0000 - val_loss: 3.5331 - val_acc: 0.5000\n",
            "Epoch 1527/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6296 - acc: 1.0000 - val_loss: 3.8460 - val_acc: 0.5000\n",
            "Epoch 1528/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6287 - acc: 1.0000 - val_loss: 3.7850 - val_acc: 0.5000\n",
            "Epoch 1529/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6271 - acc: 1.0000 - val_loss: 3.7562 - val_acc: 0.5000\n",
            "Epoch 1530/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6258 - acc: 1.0000 - val_loss: 3.8851 - val_acc: 0.5000\n",
            "Epoch 1531/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6248 - acc: 1.0000 - val_loss: 3.9825 - val_acc: 0.5000\n",
            "Epoch 1532/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6223 - acc: 1.0000 - val_loss: 4.3891 - val_acc: 0.5000\n",
            "Epoch 1533/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6210 - acc: 1.0000 - val_loss: 4.1839 - val_acc: 0.5000\n",
            "Epoch 1534/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6192 - acc: 1.0000 - val_loss: 4.4001 - val_acc: 0.5000\n",
            "Epoch 1535/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6179 - acc: 1.0000 - val_loss: 4.2896 - val_acc: 0.5000\n",
            "Epoch 1536/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6166 - acc: 1.0000 - val_loss: 4.2009 - val_acc: 0.5000\n",
            "Epoch 1537/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6147 - acc: 1.0000 - val_loss: 4.3965 - val_acc: 0.5000\n",
            "Epoch 1538/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6135 - acc: 1.0000 - val_loss: 4.3738 - val_acc: 0.5000\n",
            "Epoch 1539/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6115 - acc: 1.0000 - val_loss: 4.4391 - val_acc: 0.5000\n",
            "Epoch 1540/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6100 - acc: 1.0000 - val_loss: 4.4042 - val_acc: 0.5000\n",
            "Epoch 1541/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6090 - acc: 1.0000 - val_loss: 4.2724 - val_acc: 0.5000\n",
            "Epoch 1542/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6070 - acc: 1.0000 - val_loss: 4.2963 - val_acc: 0.5000\n",
            "Epoch 1543/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6055 - acc: 1.0000 - val_loss: 4.2304 - val_acc: 0.5000\n",
            "Epoch 1544/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6040 - acc: 1.0000 - val_loss: 3.9687 - val_acc: 0.5000\n",
            "Epoch 1545/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6026 - acc: 1.0000 - val_loss: 4.1952 - val_acc: 0.5000\n",
            "Epoch 1546/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6011 - acc: 1.0000 - val_loss: 3.9322 - val_acc: 0.5000\n",
            "Epoch 1547/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5994 - acc: 1.0000 - val_loss: 4.0479 - val_acc: 0.5000\n",
            "Epoch 1548/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5981 - acc: 1.0000 - val_loss: 3.9738 - val_acc: 0.5000\n",
            "Epoch 1549/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5966 - acc: 1.0000 - val_loss: 4.1167 - val_acc: 0.5000\n",
            "Epoch 1550/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5950 - acc: 1.0000 - val_loss: 4.1201 - val_acc: 0.5000\n",
            "Epoch 1551/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5936 - acc: 1.0000 - val_loss: 3.8196 - val_acc: 0.5000\n",
            "Epoch 1552/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5921 - acc: 1.0000 - val_loss: 3.7772 - val_acc: 0.5000\n",
            "Epoch 1553/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5908 - acc: 1.0000 - val_loss: 3.8499 - val_acc: 0.5000\n",
            "Epoch 1554/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5892 - acc: 1.0000 - val_loss: 3.9935 - val_acc: 0.5000\n",
            "Epoch 1555/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5877 - acc: 1.0000 - val_loss: 3.8462 - val_acc: 0.5000\n",
            "Epoch 1556/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5863 - acc: 1.0000 - val_loss: 3.9765 - val_acc: 0.5000\n",
            "Epoch 1557/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5849 - acc: 1.0000 - val_loss: 4.0151 - val_acc: 0.5000\n",
            "Epoch 1558/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5834 - acc: 1.0000 - val_loss: 3.9339 - val_acc: 0.5000\n",
            "Epoch 1559/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5819 - acc: 1.0000 - val_loss: 3.7801 - val_acc: 0.5000\n",
            "Epoch 1560/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5879 - acc: 0.9950 - val_loss: 3.8199 - val_acc: 0.5000\n",
            "Epoch 1561/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5804 - acc: 1.0000 - val_loss: 2.8259 - val_acc: 0.5000\n",
            "Epoch 1562/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5808 - acc: 1.0000 - val_loss: 2.5560 - val_acc: 0.5000\n",
            "Epoch 1563/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6092 - acc: 0.9900 - val_loss: 2.3671 - val_acc: 0.5000\n",
            "Epoch 1564/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5795 - acc: 1.0000 - val_loss: 3.9768 - val_acc: 0.3500\n",
            "Epoch 1565/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5929 - acc: 0.9950 - val_loss: 2.2564 - val_acc: 0.5000\n",
            "Epoch 1566/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5834 - acc: 1.0000 - val_loss: 2.7418 - val_acc: 0.5000\n",
            "Epoch 1567/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5862 - acc: 1.0000 - val_loss: 2.7908 - val_acc: 0.5000\n",
            "Epoch 1568/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5853 - acc: 1.0000 - val_loss: 2.7241 - val_acc: 0.5000\n",
            "Epoch 1569/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5845 - acc: 1.0000 - val_loss: 2.6231 - val_acc: 0.5000\n",
            "Epoch 1570/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5844 - acc: 1.0000 - val_loss: 2.6326 - val_acc: 0.5000\n",
            "Epoch 1571/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5821 - acc: 1.0000 - val_loss: 2.6292 - val_acc: 0.5000\n",
            "Epoch 1572/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5808 - acc: 1.0000 - val_loss: 2.5742 - val_acc: 0.5000\n",
            "Epoch 1573/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6873 - acc: 0.9800 - val_loss: 2.4063 - val_acc: 0.5000\n",
            "Epoch 1574/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6117 - acc: 0.9900 - val_loss: 2.3093 - val_acc: 0.5000\n",
            "Epoch 1575/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5844 - acc: 1.0000 - val_loss: 2.8228 - val_acc: 0.5100\n",
            "Epoch 1576/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5877 - acc: 1.0000 - val_loss: 2.4121 - val_acc: 0.5300\n",
            "Epoch 1577/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5887 - acc: 1.0000 - val_loss: 2.4780 - val_acc: 0.5100\n",
            "Epoch 1578/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5883 - acc: 1.0000 - val_loss: 2.5387 - val_acc: 0.5100\n",
            "Epoch 1579/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5876 - acc: 1.0000 - val_loss: 2.9047 - val_acc: 0.5000\n",
            "Epoch 1580/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5861 - acc: 1.0000 - val_loss: 2.9291 - val_acc: 0.5000\n",
            "Epoch 1581/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5886 - acc: 0.9950 - val_loss: 3.1034 - val_acc: 0.5000\n",
            "Epoch 1582/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5832 - acc: 1.0000 - val_loss: 2.9151 - val_acc: 0.5100\n",
            "Epoch 1583/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5817 - acc: 1.0000 - val_loss: 3.0282 - val_acc: 0.5000\n",
            "Epoch 1584/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5801 - acc: 1.0000 - val_loss: 3.3606 - val_acc: 0.5000\n",
            "Epoch 1585/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5789 - acc: 1.0000 - val_loss: 3.3596 - val_acc: 0.5000\n",
            "Epoch 1586/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5771 - acc: 1.0000 - val_loss: 3.4089 - val_acc: 0.5000\n",
            "Epoch 1587/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5756 - acc: 1.0000 - val_loss: 3.2386 - val_acc: 0.5000\n",
            "Epoch 1588/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5740 - acc: 1.0000 - val_loss: 3.5593 - val_acc: 0.5000\n",
            "Epoch 1589/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5724 - acc: 1.0000 - val_loss: 3.5117 - val_acc: 0.5000\n",
            "Epoch 1590/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5709 - acc: 1.0000 - val_loss: 3.3446 - val_acc: 0.5000\n",
            "Epoch 1591/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5693 - acc: 1.0000 - val_loss: 3.6223 - val_acc: 0.5000\n",
            "Epoch 1592/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5678 - acc: 1.0000 - val_loss: 3.3537 - val_acc: 0.5000\n",
            "Epoch 1593/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5664 - acc: 1.0000 - val_loss: 3.6961 - val_acc: 0.5000\n",
            "Epoch 1594/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5648 - acc: 1.0000 - val_loss: 3.4080 - val_acc: 0.5000\n",
            "Epoch 1595/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5636 - acc: 1.0000 - val_loss: 3.4270 - val_acc: 0.5000\n",
            "Epoch 1596/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5619 - acc: 1.0000 - val_loss: 3.6225 - val_acc: 0.5000\n",
            "Epoch 1597/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5605 - acc: 1.0000 - val_loss: 3.3881 - val_acc: 0.5000\n",
            "Epoch 1598/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7061 - acc: 0.9800 - val_loss: 3.1607 - val_acc: 0.5000\n",
            "Epoch 1599/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5604 - acc: 1.0000 - val_loss: 2.1088 - val_acc: 0.5000\n",
            "Epoch 1600/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5647 - acc: 1.0000 - val_loss: 2.2456 - val_acc: 0.5000\n",
            "Epoch 1601/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5682 - acc: 1.0000 - val_loss: 2.3519 - val_acc: 0.5200\n",
            "Epoch 1602/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5700 - acc: 1.0000 - val_loss: 2.0807 - val_acc: 0.5300\n",
            "Epoch 1603/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5736 - acc: 0.9950 - val_loss: 2.0456 - val_acc: 0.5300\n",
            "Epoch 1604/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5679 - acc: 1.0000 - val_loss: 1.7530 - val_acc: 0.5300\n",
            "Epoch 1605/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5669 - acc: 1.0000 - val_loss: 1.8907 - val_acc: 0.5300\n",
            "Epoch 1606/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5658 - acc: 1.0000 - val_loss: 1.9440 - val_acc: 0.5300\n",
            "Epoch 1607/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5644 - acc: 1.0000 - val_loss: 1.7752 - val_acc: 0.5300\n",
            "Epoch 1608/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5629 - acc: 1.0000 - val_loss: 1.8721 - val_acc: 0.5300\n",
            "Epoch 1609/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5614 - acc: 1.0000 - val_loss: 1.7686 - val_acc: 0.5300\n",
            "Epoch 1610/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5599 - acc: 1.0000 - val_loss: 1.5146 - val_acc: 0.5400\n",
            "Epoch 1611/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5583 - acc: 1.0000 - val_loss: 1.6346 - val_acc: 0.5300\n",
            "Epoch 1612/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6186 - acc: 0.9850 - val_loss: 1.5878 - val_acc: 0.5600\n",
            "Epoch 1613/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5987 - acc: 0.9850 - val_loss: 2.3594 - val_acc: 0.5300\n",
            "Epoch 1614/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5759 - acc: 0.9900 - val_loss: 2.5652 - val_acc: 0.5100\n",
            "Epoch 1615/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5707 - acc: 0.9950 - val_loss: 1.7151 - val_acc: 0.5800\n",
            "Epoch 1616/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5982 - acc: 0.9900 - val_loss: 1.8228 - val_acc: 0.5400\n",
            "Epoch 1617/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5669 - acc: 1.0000 - val_loss: 1.6932 - val_acc: 0.5300\n",
            "Epoch 1618/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5672 - acc: 1.0000 - val_loss: 1.7101 - val_acc: 0.5000\n",
            "Epoch 1619/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5809 - acc: 0.9900 - val_loss: 1.7509 - val_acc: 0.5000\n",
            "Epoch 1620/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5658 - acc: 1.0000 - val_loss: 2.4790 - val_acc: 0.5000\n",
            "Epoch 1621/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5656 - acc: 1.0000 - val_loss: 2.7255 - val_acc: 0.4500\n",
            "Epoch 1622/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5651 - acc: 1.0000 - val_loss: 2.7041 - val_acc: 0.5000\n",
            "Epoch 1623/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5651 - acc: 1.0000 - val_loss: 2.4970 - val_acc: 0.5000\n",
            "Epoch 1624/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5643 - acc: 1.0000 - val_loss: 2.6730 - val_acc: 0.5000\n",
            "Epoch 1625/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5618 - acc: 1.0000 - val_loss: 2.4221 - val_acc: 0.4900\n",
            "Epoch 1626/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5605 - acc: 1.0000 - val_loss: 2.3905 - val_acc: 0.5000\n",
            "Epoch 1627/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5586 - acc: 1.0000 - val_loss: 2.4521 - val_acc: 0.5000\n",
            "Epoch 1628/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5652 - acc: 1.0000 - val_loss: 2.4534 - val_acc: 0.5000\n",
            "Epoch 1629/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5556 - acc: 1.0000 - val_loss: 2.2864 - val_acc: 0.5000\n",
            "Epoch 1630/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5543 - acc: 1.0000 - val_loss: 2.1527 - val_acc: 0.5000\n",
            "Epoch 1631/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5531 - acc: 1.0000 - val_loss: 2.0993 - val_acc: 0.5000\n",
            "Epoch 1632/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5515 - acc: 1.0000 - val_loss: 2.2054 - val_acc: 0.5000\n",
            "Epoch 1633/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5497 - acc: 1.0000 - val_loss: 2.0829 - val_acc: 0.5000\n",
            "Epoch 1634/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5482 - acc: 1.0000 - val_loss: 2.1359 - val_acc: 0.5000\n",
            "Epoch 1635/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5468 - acc: 1.0000 - val_loss: 1.9951 - val_acc: 0.5100\n",
            "Epoch 1636/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5452 - acc: 1.0000 - val_loss: 2.1000 - val_acc: 0.5100\n",
            "Epoch 1637/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5438 - acc: 1.0000 - val_loss: 2.0671 - val_acc: 0.5000\n",
            "Epoch 1638/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5422 - acc: 1.0000 - val_loss: 2.1834 - val_acc: 0.5000\n",
            "Epoch 1639/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5407 - acc: 1.0000 - val_loss: 1.9682 - val_acc: 0.5200\n",
            "Epoch 1640/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5393 - acc: 1.0000 - val_loss: 2.1191 - val_acc: 0.5000\n",
            "Epoch 1641/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5377 - acc: 1.0000 - val_loss: 2.1046 - val_acc: 0.5000\n",
            "Epoch 1642/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5367 - acc: 1.0000 - val_loss: 2.0378 - val_acc: 0.5000\n",
            "Epoch 1643/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5349 - acc: 1.0000 - val_loss: 1.9711 - val_acc: 0.5200\n",
            "Epoch 1644/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5334 - acc: 1.0000 - val_loss: 1.9690 - val_acc: 0.5100\n",
            "Epoch 1645/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5319 - acc: 1.0000 - val_loss: 1.9356 - val_acc: 0.5200\n",
            "Epoch 1646/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5306 - acc: 1.0000 - val_loss: 1.9430 - val_acc: 0.5200\n",
            "Epoch 1647/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5291 - acc: 1.0000 - val_loss: 1.9994 - val_acc: 0.5000\n",
            "Epoch 1648/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5396 - acc: 0.9900 - val_loss: 1.9805 - val_acc: 0.5000\n",
            "Epoch 1649/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5276 - acc: 1.0000 - val_loss: 1.6051 - val_acc: 0.5600\n",
            "Epoch 1650/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5309 - acc: 1.0000 - val_loss: 1.5167 - val_acc: 0.5700\n",
            "Epoch 1651/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5294 - acc: 0.9950 - val_loss: 1.6136 - val_acc: 0.5700\n",
            "Epoch 1652/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5246 - acc: 1.0000 - val_loss: 1.5730 - val_acc: 0.5600\n",
            "Epoch 1653/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5246 - acc: 1.0000 - val_loss: 1.5995 - val_acc: 0.5700\n",
            "Epoch 1654/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5223 - acc: 1.0000 - val_loss: 1.5751 - val_acc: 0.5400\n",
            "Epoch 1655/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5284 - acc: 0.9950 - val_loss: 1.9220 - val_acc: 0.5300\n",
            "Epoch 1656/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5504 - acc: 0.9950 - val_loss: 1.9450 - val_acc: 0.4100\n",
            "Epoch 1657/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5177 - acc: 1.0000 - val_loss: 1.6381 - val_acc: 0.5200\n",
            "Epoch 1658/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6022 - acc: 0.9900 - val_loss: 1.6436 - val_acc: 0.5000\n",
            "Epoch 1659/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5362 - acc: 0.9900 - val_loss: 2.1349 - val_acc: 0.5000\n",
            "Epoch 1660/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6355 - acc: 0.9800 - val_loss: 2.9494 - val_acc: 0.5000\n",
            "Epoch 1661/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5295 - acc: 0.9950 - val_loss: 3.0269 - val_acc: 0.5000\n",
            "Epoch 1662/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5535 - acc: 0.9800 - val_loss: 2.9580 - val_acc: 0.5000\n",
            "Epoch 1663/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5386 - acc: 0.9900 - val_loss: 2.8142 - val_acc: 0.5000\n",
            "Epoch 1664/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5241 - acc: 1.0000 - val_loss: 2.5640 - val_acc: 0.5300\n",
            "Epoch 1665/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5236 - acc: 1.0000 - val_loss: 2.6467 - val_acc: 0.5000\n",
            "Epoch 1666/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5223 - acc: 1.0000 - val_loss: 2.6678 - val_acc: 0.5000\n",
            "Epoch 1667/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5209 - acc: 1.0000 - val_loss: 2.6874 - val_acc: 0.5000\n",
            "Epoch 1668/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5195 - acc: 1.0000 - val_loss: 2.6637 - val_acc: 0.5000\n",
            "Epoch 1669/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5184 - acc: 1.0000 - val_loss: 2.8290 - val_acc: 0.5000\n",
            "Epoch 1670/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5169 - acc: 1.0000 - val_loss: 2.4326 - val_acc: 0.5000\n",
            "Epoch 1671/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5159 - acc: 1.0000 - val_loss: 2.7948 - val_acc: 0.5000\n",
            "Epoch 1672/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5139 - acc: 1.0000 - val_loss: 2.7478 - val_acc: 0.5000\n",
            "Epoch 1673/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5128 - acc: 1.0000 - val_loss: 2.5590 - val_acc: 0.5000\n",
            "Epoch 1674/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5112 - acc: 1.0000 - val_loss: 2.6429 - val_acc: 0.5000\n",
            "Epoch 1675/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5239 - acc: 0.9950 - val_loss: 2.7042 - val_acc: 0.5000\n",
            "Epoch 1676/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5085 - acc: 1.0000 - val_loss: 2.6049 - val_acc: 0.5000\n",
            "Epoch 1677/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5071 - acc: 1.0000 - val_loss: 2.8517 - val_acc: 0.5000\n",
            "Epoch 1678/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5062 - acc: 1.0000 - val_loss: 2.7805 - val_acc: 0.5000\n",
            "Epoch 1679/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5045 - acc: 1.0000 - val_loss: 2.7351 - val_acc: 0.5000\n",
            "Epoch 1680/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5038 - acc: 1.0000 - val_loss: 2.4962 - val_acc: 0.5000\n",
            "Epoch 1681/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5027 - acc: 1.0000 - val_loss: 2.6718 - val_acc: 0.5000\n",
            "Epoch 1682/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5004 - acc: 1.0000 - val_loss: 2.4298 - val_acc: 0.5000\n",
            "Epoch 1683/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4992 - acc: 1.0000 - val_loss: 2.5419 - val_acc: 0.5000\n",
            "Epoch 1684/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4978 - acc: 1.0000 - val_loss: 2.4499 - val_acc: 0.5000\n",
            "Epoch 1685/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4993 - acc: 1.0000 - val_loss: 2.2733 - val_acc: 0.5000\n",
            "Epoch 1686/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4950 - acc: 1.0000 - val_loss: 2.2216 - val_acc: 0.5000\n",
            "Epoch 1687/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4937 - acc: 1.0000 - val_loss: 1.9637 - val_acc: 0.5000\n",
            "Epoch 1688/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4925 - acc: 1.0000 - val_loss: 2.0107 - val_acc: 0.5000\n",
            "Epoch 1689/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4912 - acc: 1.0000 - val_loss: 1.8263 - val_acc: 0.5000\n",
            "Epoch 1690/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4910 - acc: 1.0000 - val_loss: 1.7258 - val_acc: 0.5000\n",
            "Epoch 1691/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4887 - acc: 1.0000 - val_loss: 1.6226 - val_acc: 0.5000\n",
            "Epoch 1692/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4874 - acc: 1.0000 - val_loss: 1.6289 - val_acc: 0.5000\n",
            "Epoch 1693/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5331 - acc: 0.9850 - val_loss: 1.5558 - val_acc: 0.5000\n",
            "Epoch 1694/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4859 - acc: 1.0000 - val_loss: 2.7826 - val_acc: 0.5000\n",
            "Epoch 1695/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5016 - acc: 0.9950 - val_loss: 2.9739 - val_acc: 0.5000\n",
            "Epoch 1696/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5392 - acc: 0.9800 - val_loss: 3.3104 - val_acc: 0.3700\n",
            "Epoch 1697/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5631 - acc: 0.9750 - val_loss: 3.5148 - val_acc: 0.5000\n",
            "Epoch 1698/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5061 - acc: 0.9950 - val_loss: 5.0350 - val_acc: 0.5000\n",
            "Epoch 1699/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4995 - acc: 0.9950 - val_loss: 5.9916 - val_acc: 0.5000\n",
            "Epoch 1700/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4995 - acc: 1.0000 - val_loss: 5.9901 - val_acc: 0.5000\n",
            "Epoch 1701/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4980 - acc: 1.0000 - val_loss: 5.7158 - val_acc: 0.5000\n",
            "Epoch 1702/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4972 - acc: 1.0000 - val_loss: 5.2444 - val_acc: 0.5000\n",
            "Epoch 1703/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4963 - acc: 1.0000 - val_loss: 5.0278 - val_acc: 0.5000\n",
            "Epoch 1704/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5060 - acc: 0.9950 - val_loss: 4.5009 - val_acc: 0.5000\n",
            "Epoch 1705/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4943 - acc: 1.0000 - val_loss: 4.1954 - val_acc: 0.5000\n",
            "Epoch 1706/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4980 - acc: 1.0000 - val_loss: 3.7625 - val_acc: 0.5000\n",
            "Epoch 1707/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4938 - acc: 1.0000 - val_loss: 3.9083 - val_acc: 0.5000\n",
            "Epoch 1708/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4930 - acc: 1.0000 - val_loss: 3.8680 - val_acc: 0.5000\n",
            "Epoch 1709/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4909 - acc: 1.0000 - val_loss: 3.6110 - val_acc: 0.5000\n",
            "Epoch 1710/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4894 - acc: 1.0000 - val_loss: 3.5846 - val_acc: 0.5000\n",
            "Epoch 1711/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4898 - acc: 1.0000 - val_loss: 3.5817 - val_acc: 0.5000\n",
            "Epoch 1712/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4859 - acc: 1.0000 - val_loss: 3.3246 - val_acc: 0.5000\n",
            "Epoch 1713/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4846 - acc: 1.0000 - val_loss: 3.3566 - val_acc: 0.5000\n",
            "Epoch 1714/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4838 - acc: 1.0000 - val_loss: 3.1525 - val_acc: 0.5000\n",
            "Epoch 1715/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4818 - acc: 1.0000 - val_loss: 3.0976 - val_acc: 0.5000\n",
            "Epoch 1716/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4805 - acc: 1.0000 - val_loss: 3.0778 - val_acc: 0.5000\n",
            "Epoch 1717/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4792 - acc: 1.0000 - val_loss: 2.9809 - val_acc: 0.5000\n",
            "Epoch 1718/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4779 - acc: 1.0000 - val_loss: 2.8876 - val_acc: 0.5000\n",
            "Epoch 1719/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4764 - acc: 1.0000 - val_loss: 3.1106 - val_acc: 0.5000\n",
            "Epoch 1720/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4823 - acc: 0.9950 - val_loss: 2.9775 - val_acc: 0.5000\n",
            "Epoch 1721/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4738 - acc: 1.0000 - val_loss: 2.8328 - val_acc: 0.5000\n",
            "Epoch 1722/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4724 - acc: 1.0000 - val_loss: 2.9590 - val_acc: 0.5000\n",
            "Epoch 1723/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4713 - acc: 1.0000 - val_loss: 2.8785 - val_acc: 0.5000\n",
            "Epoch 1724/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4698 - acc: 1.0000 - val_loss: 2.8937 - val_acc: 0.5000\n",
            "Epoch 1725/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4685 - acc: 1.0000 - val_loss: 3.0277 - val_acc: 0.5000\n",
            "Epoch 1726/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4671 - acc: 1.0000 - val_loss: 2.8454 - val_acc: 0.5000\n",
            "Epoch 1727/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5642 - acc: 0.9850 - val_loss: 3.5415 - val_acc: 0.5000\n",
            "Epoch 1728/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4671 - acc: 1.0000 - val_loss: 9.5779 - val_acc: 0.3700\n",
            "Epoch 1729/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4937 - acc: 0.9950 - val_loss: 9.3459 - val_acc: 0.4200\n",
            "Epoch 1730/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5897 - acc: 0.9750 - val_loss: 8.9458 - val_acc: 0.4400\n",
            "Epoch 1731/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.7329 - acc: 0.9700 - val_loss: 9.5930 - val_acc: 0.3900\n",
            "Epoch 1732/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5403 - acc: 0.9950 - val_loss: 1.4397 - val_acc: 0.6000\n",
            "Epoch 1733/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5772 - acc: 0.9800 - val_loss: 8.6331 - val_acc: 0.4000\n",
            "Epoch 1734/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5097 - acc: 0.9950 - val_loss: 2.4345 - val_acc: 0.6300\n",
            "Epoch 1735/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4987 - acc: 1.0000 - val_loss: 1.4858 - val_acc: 0.8100\n",
            "Epoch 1736/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5003 - acc: 1.0000 - val_loss: 1.7484 - val_acc: 0.7600\n",
            "Epoch 1737/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5101 - acc: 0.9950 - val_loss: 4.3693 - val_acc: 0.5100\n",
            "Epoch 1738/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4998 - acc: 1.0000 - val_loss: 4.0267 - val_acc: 0.5100\n",
            "Epoch 1739/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4987 - acc: 1.0000 - val_loss: 3.7864 - val_acc: 0.5000\n",
            "Epoch 1740/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4977 - acc: 1.0000 - val_loss: 3.3877 - val_acc: 0.5000\n",
            "Epoch 1741/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4966 - acc: 1.0000 - val_loss: 3.2552 - val_acc: 0.5000\n",
            "Epoch 1742/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4955 - acc: 1.0000 - val_loss: 2.7714 - val_acc: 0.5000\n",
            "Epoch 1743/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4941 - acc: 1.0000 - val_loss: 2.6598 - val_acc: 0.5000\n",
            "Epoch 1744/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4945 - acc: 1.0000 - val_loss: 2.2902 - val_acc: 0.5000\n",
            "Epoch 1745/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5263 - acc: 0.9900 - val_loss: 2.3069 - val_acc: 0.5000\n",
            "Epoch 1746/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4906 - acc: 1.0000 - val_loss: 2.5279 - val_acc: 0.5000\n",
            "Epoch 1747/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5045 - acc: 0.9900 - val_loss: 3.4134 - val_acc: 0.5000\n",
            "Epoch 1748/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4927 - acc: 1.0000 - val_loss: 2.4832 - val_acc: 0.5100\n",
            "Epoch 1749/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4944 - acc: 1.0000 - val_loss: 1.8096 - val_acc: 0.5100\n",
            "Epoch 1750/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4936 - acc: 1.0000 - val_loss: 1.8686 - val_acc: 0.5100\n",
            "Epoch 1751/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4928 - acc: 1.0000 - val_loss: 2.0146 - val_acc: 0.5000\n",
            "Epoch 1752/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4912 - acc: 1.0000 - val_loss: 2.2125 - val_acc: 0.5000\n",
            "Epoch 1753/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4906 - acc: 1.0000 - val_loss: 2.2344 - val_acc: 0.5000\n",
            "Epoch 1754/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4911 - acc: 1.0000 - val_loss: 1.9842 - val_acc: 0.5100\n",
            "Epoch 1755/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4877 - acc: 1.0000 - val_loss: 1.9531 - val_acc: 0.5000\n",
            "Epoch 1756/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4864 - acc: 1.0000 - val_loss: 1.9570 - val_acc: 0.5000\n",
            "Epoch 1757/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4854 - acc: 1.0000 - val_loss: 1.9374 - val_acc: 0.5000\n",
            "Epoch 1758/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4837 - acc: 1.0000 - val_loss: 1.9382 - val_acc: 0.5000\n",
            "Epoch 1759/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5059 - acc: 0.9950 - val_loss: 2.0946 - val_acc: 0.5000\n",
            "Epoch 1760/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4824 - acc: 1.0000 - val_loss: 2.5533 - val_acc: 0.5000\n",
            "Epoch 1761/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5868 - acc: 0.9800 - val_loss: 2.4357 - val_acc: 0.5000\n",
            "Epoch 1762/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5139 - acc: 0.9950 - val_loss: 1.9567 - val_acc: 0.5000\n",
            "Epoch 1763/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4809 - acc: 1.0000 - val_loss: 1.8721 - val_acc: 0.5000\n",
            "Epoch 1764/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4804 - acc: 1.0000 - val_loss: 1.8659 - val_acc: 0.5000\n",
            "Epoch 1765/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4797 - acc: 1.0000 - val_loss: 1.7692 - val_acc: 0.5000\n",
            "Epoch 1766/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4788 - acc: 1.0000 - val_loss: 1.7810 - val_acc: 0.5000\n",
            "Epoch 1767/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4775 - acc: 1.0000 - val_loss: 1.7035 - val_acc: 0.5000\n",
            "Epoch 1768/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4766 - acc: 1.0000 - val_loss: 1.7555 - val_acc: 0.5000\n",
            "Epoch 1769/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4752 - acc: 1.0000 - val_loss: 1.6914 - val_acc: 0.5000\n",
            "Epoch 1770/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4742 - acc: 1.0000 - val_loss: 1.7007 - val_acc: 0.5000\n",
            "Epoch 1771/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4728 - acc: 1.0000 - val_loss: 1.7039 - val_acc: 0.5000\n",
            "Epoch 1772/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4716 - acc: 1.0000 - val_loss: 1.6806 - val_acc: 0.5000\n",
            "Epoch 1773/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5326 - acc: 0.9850 - val_loss: 1.5983 - val_acc: 0.5000\n",
            "Epoch 1774/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4695 - acc: 1.0000 - val_loss: 1.4481 - val_acc: 0.5000\n",
            "Epoch 1775/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5069 - acc: 0.9850 - val_loss: 1.7773 - val_acc: 0.5000\n",
            "Epoch 1776/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4768 - acc: 0.9950 - val_loss: 2.1794 - val_acc: 0.5000\n",
            "Epoch 1777/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4727 - acc: 1.0000 - val_loss: 3.3212 - val_acc: 0.1500\n",
            "Epoch 1778/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4951 - acc: 0.9950 - val_loss: 2.2088 - val_acc: 0.5000\n",
            "Epoch 1779/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4746 - acc: 1.0000 - val_loss: 2.2779 - val_acc: 0.5000\n",
            "Epoch 1780/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4743 - acc: 1.0000 - val_loss: 2.2280 - val_acc: 0.5000\n",
            "Epoch 1781/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4736 - acc: 1.0000 - val_loss: 2.2779 - val_acc: 0.5000\n",
            "Epoch 1782/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4726 - acc: 1.0000 - val_loss: 2.2677 - val_acc: 0.5000\n",
            "Epoch 1783/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4717 - acc: 1.0000 - val_loss: 2.3403 - val_acc: 0.5000\n",
            "Epoch 1784/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4704 - acc: 1.0000 - val_loss: 2.3655 - val_acc: 0.5000\n",
            "Epoch 1785/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4697 - acc: 1.0000 - val_loss: 2.3356 - val_acc: 0.5000\n",
            "Epoch 1786/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4682 - acc: 1.0000 - val_loss: 2.4033 - val_acc: 0.5000\n",
            "Epoch 1787/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4669 - acc: 1.0000 - val_loss: 2.3707 - val_acc: 0.5000\n",
            "Epoch 1788/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4660 - acc: 1.0000 - val_loss: 2.3437 - val_acc: 0.5000\n",
            "Epoch 1789/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4648 - acc: 1.0000 - val_loss: 2.2125 - val_acc: 0.5000\n",
            "Epoch 1790/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4635 - acc: 1.0000 - val_loss: 2.3217 - val_acc: 0.5000\n",
            "Epoch 1791/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4625 - acc: 1.0000 - val_loss: 2.2561 - val_acc: 0.5000\n",
            "Epoch 1792/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4613 - acc: 1.0000 - val_loss: 2.2622 - val_acc: 0.5000\n",
            "Epoch 1793/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4601 - acc: 1.0000 - val_loss: 2.2970 - val_acc: 0.5000\n",
            "Epoch 1794/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4590 - acc: 1.0000 - val_loss: 2.1867 - val_acc: 0.5000\n",
            "Epoch 1795/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4580 - acc: 1.0000 - val_loss: 2.3055 - val_acc: 0.5000\n",
            "Epoch 1796/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4577 - acc: 1.0000 - val_loss: 2.2179 - val_acc: 0.5000\n",
            "Epoch 1797/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4646 - acc: 0.9950 - val_loss: 1.9817 - val_acc: 0.5000\n",
            "Epoch 1798/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4547 - acc: 1.0000 - val_loss: 2.0413 - val_acc: 0.5000\n",
            "Epoch 1799/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4540 - acc: 1.0000 - val_loss: 1.7901 - val_acc: 0.5000\n",
            "Epoch 1800/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4531 - acc: 1.0000 - val_loss: 2.0378 - val_acc: 0.5000\n",
            "Epoch 1801/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4931 - acc: 0.9900 - val_loss: 2.0883 - val_acc: 0.5000\n",
            "Epoch 1802/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4514 - acc: 1.0000 - val_loss: 2.5273 - val_acc: 0.5000\n",
            "Epoch 1803/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4530 - acc: 1.0000 - val_loss: 2.6213 - val_acc: 0.5000\n",
            "Epoch 1804/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4541 - acc: 1.0000 - val_loss: 2.7268 - val_acc: 0.4600\n",
            "Epoch 1805/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4498 - acc: 1.0000 - val_loss: 2.4863 - val_acc: 0.4900\n",
            "Epoch 1806/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4518 - acc: 1.0000 - val_loss: 2.5905 - val_acc: 0.5000\n",
            "Epoch 1807/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4495 - acc: 1.0000 - val_loss: 2.5077 - val_acc: 0.4900\n",
            "Epoch 1808/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4466 - acc: 1.0000 - val_loss: 2.6567 - val_acc: 0.5000\n",
            "Epoch 1809/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4458 - acc: 1.0000 - val_loss: 2.5351 - val_acc: 0.5000\n",
            "Epoch 1810/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4463 - acc: 1.0000 - val_loss: 2.5997 - val_acc: 0.5000\n",
            "Epoch 1811/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4435 - acc: 1.0000 - val_loss: 2.6252 - val_acc: 0.5000\n",
            "Epoch 1812/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4424 - acc: 1.0000 - val_loss: 2.5114 - val_acc: 0.5000\n",
            "Epoch 1813/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5129 - acc: 0.9850 - val_loss: 2.3644 - val_acc: 0.5000\n",
            "Epoch 1814/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6207 - acc: 0.9750 - val_loss: 2.3144 - val_acc: 0.5000\n",
            "Epoch 1815/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4848 - acc: 0.9850 - val_loss: 2.2647 - val_acc: 0.5000\n",
            "Epoch 1816/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4830 - acc: 0.9950 - val_loss: 2.7117 - val_acc: 0.5000\n",
            "Epoch 1817/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4516 - acc: 0.9950 - val_loss: 3.2524 - val_acc: 0.5000\n",
            "Epoch 1818/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4460 - acc: 1.0000 - val_loss: 3.3378 - val_acc: 0.5000\n",
            "Epoch 1819/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4469 - acc: 1.0000 - val_loss: 3.4024 - val_acc: 0.5000\n",
            "Epoch 1820/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4457 - acc: 1.0000 - val_loss: 3.2921 - val_acc: 0.5000\n",
            "Epoch 1821/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4447 - acc: 1.0000 - val_loss: 3.3265 - val_acc: 0.5000\n",
            "Epoch 1822/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4434 - acc: 1.0000 - val_loss: 3.2712 - val_acc: 0.5000\n",
            "Epoch 1823/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4423 - acc: 1.0000 - val_loss: 3.3386 - val_acc: 0.5000\n",
            "Epoch 1824/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4512 - acc: 0.9950 - val_loss: 3.3642 - val_acc: 0.5000\n",
            "Epoch 1825/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4422 - acc: 1.0000 - val_loss: 3.4684 - val_acc: 0.5000\n",
            "Epoch 1826/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4418 - acc: 1.0000 - val_loss: 3.5010 - val_acc: 0.5000\n",
            "Epoch 1827/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4391 - acc: 1.0000 - val_loss: 3.3622 - val_acc: 0.5000\n",
            "Epoch 1828/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4557 - acc: 0.9950 - val_loss: 3.4675 - val_acc: 0.5000\n",
            "Epoch 1829/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4406 - acc: 1.0000 - val_loss: 2.5896 - val_acc: 0.5000\n",
            "Epoch 1830/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4500 - acc: 0.9950 - val_loss: 1.8576 - val_acc: 0.5000\n",
            "Epoch 1831/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4365 - acc: 1.0000 - val_loss: 1.7465 - val_acc: 0.5000\n",
            "Epoch 1832/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4360 - acc: 1.0000 - val_loss: 1.7139 - val_acc: 0.5000\n",
            "Epoch 1833/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4394 - acc: 1.0000 - val_loss: 1.6655 - val_acc: 0.5000\n",
            "Epoch 1834/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4346 - acc: 1.0000 - val_loss: 1.6572 - val_acc: 0.5000\n",
            "Epoch 1835/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5827 - acc: 0.9900 - val_loss: 1.7107 - val_acc: 0.5000\n",
            "Epoch 1836/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4345 - acc: 1.0000 - val_loss: 2.4205 - val_acc: 0.5000\n",
            "Epoch 1837/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4399 - acc: 1.0000 - val_loss: 2.6506 - val_acc: 0.5000\n",
            "Epoch 1838/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4433 - acc: 1.0000 - val_loss: 2.5597 - val_acc: 0.5000\n",
            "Epoch 1839/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4450 - acc: 1.0000 - val_loss: 2.5448 - val_acc: 0.5000\n",
            "Epoch 1840/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4447 - acc: 1.0000 - val_loss: 2.3873 - val_acc: 0.5000\n",
            "Epoch 1841/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4444 - acc: 1.0000 - val_loss: 2.3196 - val_acc: 0.5000\n",
            "Epoch 1842/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4468 - acc: 1.0000 - val_loss: 2.2753 - val_acc: 0.5000\n",
            "Epoch 1843/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4430 - acc: 1.0000 - val_loss: 2.2780 - val_acc: 0.5000\n",
            "Epoch 1844/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4416 - acc: 1.0000 - val_loss: 2.3181 - val_acc: 0.5000\n",
            "Epoch 1845/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4406 - acc: 1.0000 - val_loss: 2.2663 - val_acc: 0.5000\n",
            "Epoch 1846/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4396 - acc: 1.0000 - val_loss: 2.3244 - val_acc: 0.5000\n",
            "Epoch 1847/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6058 - acc: 0.9750 - val_loss: 2.3106 - val_acc: 0.5000\n",
            "Epoch 1848/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4380 - acc: 1.0000 - val_loss: 2.3103 - val_acc: 0.5000\n",
            "Epoch 1849/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4402 - acc: 1.0000 - val_loss: 2.3395 - val_acc: 0.5000\n",
            "Epoch 1850/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4386 - acc: 1.0000 - val_loss: 2.2133 - val_acc: 0.5000\n",
            "Epoch 1851/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4383 - acc: 1.0000 - val_loss: 2.3664 - val_acc: 0.5000\n",
            "Epoch 1852/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4380 - acc: 1.0000 - val_loss: 2.2413 - val_acc: 0.5000\n",
            "Epoch 1853/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4824 - acc: 0.9850 - val_loss: 2.3214 - val_acc: 0.5000\n",
            "Epoch 1854/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4359 - acc: 1.0000 - val_loss: 2.2986 - val_acc: 0.5000\n",
            "Epoch 1855/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4377 - acc: 1.0000 - val_loss: 2.3143 - val_acc: 0.5000\n",
            "Epoch 1856/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4410 - acc: 1.0000 - val_loss: 2.3758 - val_acc: 0.5000\n",
            "Epoch 1857/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4381 - acc: 1.0000 - val_loss: 2.1438 - val_acc: 0.5000\n",
            "Epoch 1858/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4467 - acc: 0.9950 - val_loss: 2.0380 - val_acc: 0.5000\n",
            "Epoch 1859/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4323 - acc: 1.0000 - val_loss: 1.8970 - val_acc: 0.5000\n",
            "Epoch 1860/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4314 - acc: 1.0000 - val_loss: 1.9383 - val_acc: 0.5000\n",
            "Epoch 1861/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4306 - acc: 1.0000 - val_loss: 1.7925 - val_acc: 0.5000\n",
            "Epoch 1862/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4296 - acc: 1.0000 - val_loss: 1.9110 - val_acc: 0.5000\n",
            "Epoch 1863/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4286 - acc: 1.0000 - val_loss: 1.9324 - val_acc: 0.5000\n",
            "Epoch 1864/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4277 - acc: 1.0000 - val_loss: 1.7358 - val_acc: 0.5000\n",
            "Epoch 1865/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4266 - acc: 1.0000 - val_loss: 1.8204 - val_acc: 0.5000\n",
            "Epoch 1866/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4260 - acc: 1.0000 - val_loss: 1.7001 - val_acc: 0.5000\n",
            "Epoch 1867/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4247 - acc: 1.0000 - val_loss: 1.7825 - val_acc: 0.5000\n",
            "Epoch 1868/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4236 - acc: 1.0000 - val_loss: 1.6944 - val_acc: 0.5000\n",
            "Epoch 1869/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4960 - acc: 0.9850 - val_loss: 1.7592 - val_acc: 0.5000\n",
            "Epoch 1870/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4225 - acc: 1.0000 - val_loss: 1.8679 - val_acc: 0.5000\n",
            "Epoch 1871/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4575 - acc: 0.9900 - val_loss: 1.9386 - val_acc: 0.5000\n",
            "Epoch 1872/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5496 - acc: 0.9850 - val_loss: 2.4355 - val_acc: 0.5000\n",
            "Epoch 1873/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5970 - acc: 0.9750 - val_loss: 3.1822 - val_acc: 0.5000\n",
            "Epoch 1874/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4256 - acc: 1.0000 - val_loss: 3.3408 - val_acc: 0.5000\n",
            "Epoch 1875/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4412 - acc: 0.9950 - val_loss: 3.2739 - val_acc: 0.5000\n",
            "Epoch 1876/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4455 - acc: 0.9950 - val_loss: 3.8257 - val_acc: 0.5100\n",
            "Epoch 1877/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4298 - acc: 1.0000 - val_loss: 3.1032 - val_acc: 0.5000\n",
            "Epoch 1878/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4291 - acc: 1.0000 - val_loss: 3.6695 - val_acc: 0.5000\n",
            "Epoch 1879/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4285 - acc: 1.0000 - val_loss: 3.4085 - val_acc: 0.5100\n",
            "Epoch 1880/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4278 - acc: 1.0000 - val_loss: 3.4809 - val_acc: 0.5000\n",
            "Epoch 1881/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4270 - acc: 1.0000 - val_loss: 3.3914 - val_acc: 0.5000\n",
            "Epoch 1882/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4259 - acc: 1.0000 - val_loss: 3.1303 - val_acc: 0.5000\n",
            "Epoch 1883/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4247 - acc: 1.0000 - val_loss: 3.2399 - val_acc: 0.5000\n",
            "Epoch 1884/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4290 - acc: 0.9950 - val_loss: 3.2952 - val_acc: 0.5000\n",
            "Epoch 1885/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4224 - acc: 1.0000 - val_loss: 3.9065 - val_acc: 0.5000\n",
            "Epoch 1886/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4372 - acc: 0.9950 - val_loss: 3.8587 - val_acc: 0.5000\n",
            "Epoch 1887/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4226 - acc: 1.0000 - val_loss: 2.7043 - val_acc: 0.5000\n",
            "Epoch 1888/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4469 - acc: 0.9950 - val_loss: 2.7542 - val_acc: 0.5100\n",
            "Epoch 1889/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4234 - acc: 1.0000 - val_loss: 2.6079 - val_acc: 0.5100\n",
            "Epoch 1890/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4332 - acc: 0.9950 - val_loss: 2.4421 - val_acc: 0.5100\n",
            "Epoch 1891/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4234 - acc: 1.0000 - val_loss: 2.8002 - val_acc: 0.5000\n",
            "Epoch 1892/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4199 - acc: 1.0000 - val_loss: 2.8821 - val_acc: 0.5000\n",
            "Epoch 1893/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4186 - acc: 1.0000 - val_loss: 2.8189 - val_acc: 0.5000\n",
            "Epoch 1894/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4182 - acc: 1.0000 - val_loss: 2.9657 - val_acc: 0.5000\n",
            "Epoch 1895/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4164 - acc: 1.0000 - val_loss: 3.0063 - val_acc: 0.5000\n",
            "Epoch 1896/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4154 - acc: 1.0000 - val_loss: 3.0522 - val_acc: 0.5000\n",
            "Epoch 1897/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.6479 - acc: 0.9700 - val_loss: 2.6741 - val_acc: 0.5000\n",
            "Epoch 1898/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4155 - acc: 1.0000 - val_loss: 1.5215 - val_acc: 0.5000\n",
            "Epoch 1899/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4153 - acc: 1.0000 - val_loss: 1.3029 - val_acc: 0.5000\n",
            "Epoch 1900/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4158 - acc: 1.0000 - val_loss: 1.2694 - val_acc: 0.5000\n",
            "Epoch 1901/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4481 - acc: 0.9900 - val_loss: 1.2654 - val_acc: 0.5000\n",
            "Epoch 1902/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4158 - acc: 1.0000 - val_loss: 1.3745 - val_acc: 0.5000\n",
            "Epoch 1903/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4292 - acc: 0.9950 - val_loss: 1.5225 - val_acc: 0.5000\n",
            "Epoch 1904/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4263 - acc: 0.9950 - val_loss: 1.6540 - val_acc: 0.5000\n",
            "Epoch 1905/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4396 - acc: 0.9950 - val_loss: 1.6671 - val_acc: 0.5000\n",
            "Epoch 1906/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4175 - acc: 1.0000 - val_loss: 1.6476 - val_acc: 0.5000\n",
            "Epoch 1907/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4165 - acc: 1.0000 - val_loss: 1.5837 - val_acc: 0.5000\n",
            "Epoch 1908/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4177 - acc: 1.0000 - val_loss: 1.6715 - val_acc: 0.5000\n",
            "Epoch 1909/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4151 - acc: 1.0000 - val_loss: 1.6411 - val_acc: 0.5000\n",
            "Epoch 1910/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4204 - acc: 0.9950 - val_loss: 1.6481 - val_acc: 0.5000\n",
            "Epoch 1911/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4133 - acc: 1.0000 - val_loss: 1.6762 - val_acc: 0.5000\n",
            "Epoch 1912/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4123 - acc: 1.0000 - val_loss: 1.6788 - val_acc: 0.5000\n",
            "Epoch 1913/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4128 - acc: 1.0000 - val_loss: 1.6880 - val_acc: 0.5000\n",
            "Epoch 1914/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4114 - acc: 1.0000 - val_loss: 1.7220 - val_acc: 0.5000\n",
            "Epoch 1915/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4095 - acc: 1.0000 - val_loss: 1.7426 - val_acc: 0.5000\n",
            "Epoch 1916/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4084 - acc: 1.0000 - val_loss: 1.7871 - val_acc: 0.5000\n",
            "Epoch 1917/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4075 - acc: 1.0000 - val_loss: 1.7849 - val_acc: 0.5000\n",
            "Epoch 1918/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4063 - acc: 1.0000 - val_loss: 1.7629 - val_acc: 0.5000\n",
            "Epoch 1919/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4055 - acc: 1.0000 - val_loss: 1.8136 - val_acc: 0.5000\n",
            "Epoch 1920/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4044 - acc: 1.0000 - val_loss: 1.7218 - val_acc: 0.5000\n",
            "Epoch 1921/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4049 - acc: 1.0000 - val_loss: 1.7940 - val_acc: 0.5000\n",
            "Epoch 1922/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4025 - acc: 1.0000 - val_loss: 1.7468 - val_acc: 0.5000\n",
            "Epoch 1923/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4015 - acc: 1.0000 - val_loss: 1.7739 - val_acc: 0.5000\n",
            "Epoch 1924/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4006 - acc: 1.0000 - val_loss: 1.6738 - val_acc: 0.5000\n",
            "Epoch 1925/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4263 - acc: 0.9950 - val_loss: 1.6425 - val_acc: 0.5000\n",
            "Epoch 1926/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3992 - acc: 1.0000 - val_loss: 1.3481 - val_acc: 0.5000\n",
            "Epoch 1927/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5637 - acc: 0.9800 - val_loss: 1.3539 - val_acc: 0.5000\n",
            "Epoch 1928/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4088 - acc: 0.9950 - val_loss: 1.3463 - val_acc: 0.5000\n",
            "Epoch 1929/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4723 - acc: 0.9850 - val_loss: 1.3927 - val_acc: 0.5000\n",
            "Epoch 1930/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3976 - acc: 1.0000 - val_loss: 1.2933 - val_acc: 0.5000\n",
            "Epoch 1931/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3978 - acc: 1.0000 - val_loss: 1.3536 - val_acc: 0.5000\n",
            "Epoch 1932/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3977 - acc: 1.0000 - val_loss: 1.2999 - val_acc: 0.5000\n",
            "Epoch 1933/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3976 - acc: 1.0000 - val_loss: 1.3775 - val_acc: 0.5000\n",
            "Epoch 1934/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3974 - acc: 1.0000 - val_loss: 1.4019 - val_acc: 0.5000\n",
            "Epoch 1935/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3957 - acc: 1.0000 - val_loss: 1.4953 - val_acc: 0.5000\n",
            "Epoch 1936/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3951 - acc: 1.0000 - val_loss: 1.4570 - val_acc: 0.5000\n",
            "Epoch 1937/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3943 - acc: 1.0000 - val_loss: 1.3927 - val_acc: 0.5000\n",
            "Epoch 1938/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4534 - acc: 0.9800 - val_loss: 1.4238 - val_acc: 0.5000\n",
            "Epoch 1939/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3977 - acc: 0.9950 - val_loss: 1.5315 - val_acc: 0.5000\n",
            "Epoch 1940/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4307 - acc: 0.9950 - val_loss: 1.6604 - val_acc: 0.5000\n",
            "Epoch 1941/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3969 - acc: 1.0000 - val_loss: 1.7249 - val_acc: 0.5000\n",
            "Epoch 1942/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3976 - acc: 1.0000 - val_loss: 1.7756 - val_acc: 0.5000\n",
            "Epoch 1943/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3954 - acc: 1.0000 - val_loss: 1.6736 - val_acc: 0.5000\n",
            "Epoch 1944/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3948 - acc: 1.0000 - val_loss: 1.7896 - val_acc: 0.5000\n",
            "Epoch 1945/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3947 - acc: 1.0000 - val_loss: 1.7119 - val_acc: 0.5000\n",
            "Epoch 1946/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3931 - acc: 1.0000 - val_loss: 1.8089 - val_acc: 0.5000\n",
            "Epoch 1947/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3922 - acc: 1.0000 - val_loss: 1.7324 - val_acc: 0.5000\n",
            "Epoch 1948/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3912 - acc: 1.0000 - val_loss: 1.7894 - val_acc: 0.5000\n",
            "Epoch 1949/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3901 - acc: 1.0000 - val_loss: 1.8477 - val_acc: 0.5000\n",
            "Epoch 1950/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3892 - acc: 1.0000 - val_loss: 1.8281 - val_acc: 0.5000\n",
            "Epoch 1951/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3884 - acc: 1.0000 - val_loss: 1.8267 - val_acc: 0.5000\n",
            "Epoch 1952/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3884 - acc: 1.0000 - val_loss: 1.8308 - val_acc: 0.5000\n",
            "Epoch 1953/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3863 - acc: 1.0000 - val_loss: 1.7993 - val_acc: 0.5000\n",
            "Epoch 1954/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3855 - acc: 1.0000 - val_loss: 1.8830 - val_acc: 0.5000\n",
            "Epoch 1955/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3858 - acc: 1.0000 - val_loss: 1.8856 - val_acc: 0.5000\n",
            "Epoch 1956/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3836 - acc: 1.0000 - val_loss: 1.8759 - val_acc: 0.5000\n",
            "Epoch 1957/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3947 - acc: 0.9950 - val_loss: 1.8381 - val_acc: 0.5000\n",
            "Epoch 1958/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3828 - acc: 1.0000 - val_loss: 1.6809 - val_acc: 0.5000\n",
            "Epoch 1959/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4777 - acc: 0.9800 - val_loss: 1.7283 - val_acc: 0.5000\n",
            "Epoch 1960/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3828 - acc: 1.0000 - val_loss: 2.1833 - val_acc: 0.5000\n",
            "Epoch 1961/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3887 - acc: 1.0000 - val_loss: 2.1573 - val_acc: 0.5000\n",
            "Epoch 1962/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3936 - acc: 1.0000 - val_loss: 2.2460 - val_acc: 0.5000\n",
            "Epoch 1963/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4320 - acc: 0.9850 - val_loss: 2.0947 - val_acc: 0.5000\n",
            "Epoch 1964/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3994 - acc: 0.9900 - val_loss: 2.2722 - val_acc: 0.5000\n",
            "Epoch 1965/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3856 - acc: 1.0000 - val_loss: 2.1082 - val_acc: 0.5000\n",
            "Epoch 1966/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3847 - acc: 1.0000 - val_loss: 2.2319 - val_acc: 0.5000\n",
            "Epoch 1967/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3836 - acc: 1.0000 - val_loss: 2.3522 - val_acc: 0.5000\n",
            "Epoch 1968/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3828 - acc: 1.0000 - val_loss: 2.3694 - val_acc: 0.5000\n",
            "Epoch 1969/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3819 - acc: 1.0000 - val_loss: 2.4151 - val_acc: 0.5000\n",
            "Epoch 1970/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3815 - acc: 1.0000 - val_loss: 2.3681 - val_acc: 0.5000\n",
            "Epoch 1971/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3797 - acc: 1.0000 - val_loss: 2.4617 - val_acc: 0.5000\n",
            "Epoch 1972/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3788 - acc: 1.0000 - val_loss: 2.4973 - val_acc: 0.5000\n",
            "Epoch 1973/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3779 - acc: 1.0000 - val_loss: 2.4610 - val_acc: 0.5000\n",
            "Epoch 1974/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3771 - acc: 1.0000 - val_loss: 2.6108 - val_acc: 0.5000\n",
            "Epoch 1975/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3759 - acc: 1.0000 - val_loss: 2.5951 - val_acc: 0.5000\n",
            "Epoch 1976/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4640 - acc: 0.9900 - val_loss: 2.7708 - val_acc: 0.5000\n",
            "Epoch 1977/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3751 - acc: 1.0000 - val_loss: 2.7920 - val_acc: 0.4500\n",
            "Epoch 1978/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4470 - acc: 0.9750 - val_loss: 2.8552 - val_acc: 0.5300\n",
            "Epoch 1979/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3790 - acc: 1.0000 - val_loss: 2.5249 - val_acc: 0.5000\n",
            "Epoch 1980/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3777 - acc: 1.0000 - val_loss: 2.3675 - val_acc: 0.5000\n",
            "Epoch 1981/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3795 - acc: 1.0000 - val_loss: 2.2030 - val_acc: 0.5000\n",
            "Epoch 1982/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3885 - acc: 0.9950 - val_loss: 2.2151 - val_acc: 0.5000\n",
            "Epoch 1983/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3766 - acc: 1.0000 - val_loss: 2.0400 - val_acc: 0.5000\n",
            "Epoch 1984/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3759 - acc: 1.0000 - val_loss: 2.1261 - val_acc: 0.5000\n",
            "Epoch 1985/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3751 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.5000\n",
            "Epoch 1986/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3763 - acc: 1.0000 - val_loss: 2.0509 - val_acc: 0.5000\n",
            "Epoch 1987/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3733 - acc: 1.0000 - val_loss: 2.1441 - val_acc: 0.5000\n",
            "Epoch 1988/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3725 - acc: 1.0000 - val_loss: 2.2107 - val_acc: 0.5000\n",
            "Epoch 1989/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3744 - acc: 1.0000 - val_loss: 2.0999 - val_acc: 0.5000\n",
            "Epoch 1990/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3712 - acc: 1.0000 - val_loss: 2.0828 - val_acc: 0.5000\n",
            "Epoch 1991/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3695 - acc: 1.0000 - val_loss: 2.1500 - val_acc: 0.5000\n",
            "Epoch 1992/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3687 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.5000\n",
            "Epoch 1993/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3698 - acc: 1.0000 - val_loss: 2.1240 - val_acc: 0.5000\n",
            "Epoch 1994/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3668 - acc: 1.0000 - val_loss: 2.1548 - val_acc: 0.5000\n",
            "Epoch 1995/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3657 - acc: 1.0000 - val_loss: 2.0752 - val_acc: 0.5000\n",
            "Epoch 1996/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3649 - acc: 1.0000 - val_loss: 2.1236 - val_acc: 0.5000\n",
            "Epoch 1997/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3704 - acc: 0.9950 - val_loss: 2.1240 - val_acc: 0.5000\n",
            "Epoch 1998/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3632 - acc: 1.0000 - val_loss: 2.1213 - val_acc: 0.5000\n",
            "Epoch 1999/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3629 - acc: 1.0000 - val_loss: 2.3403 - val_acc: 0.5000\n",
            "Epoch 2000/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3623 - acc: 1.0000 - val_loss: 2.3539 - val_acc: 0.5000\n",
            "Epoch 2001/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3616 - acc: 1.0000 - val_loss: 2.4099 - val_acc: 0.5000\n",
            "Epoch 2002/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3610 - acc: 1.0000 - val_loss: 2.1878 - val_acc: 0.5000\n",
            "Epoch 2003/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3599 - acc: 1.0000 - val_loss: 2.2129 - val_acc: 0.5000\n",
            "Epoch 2004/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3591 - acc: 1.0000 - val_loss: 2.2789 - val_acc: 0.5000\n",
            "Epoch 2005/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3582 - acc: 1.0000 - val_loss: 2.3623 - val_acc: 0.5000\n",
            "Epoch 2006/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3573 - acc: 1.0000 - val_loss: 2.2744 - val_acc: 0.5000\n",
            "Epoch 2007/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3573 - acc: 1.0000 - val_loss: 2.4899 - val_acc: 0.5000\n",
            "Epoch 2008/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3556 - acc: 1.0000 - val_loss: 2.3495 - val_acc: 0.5000\n",
            "Epoch 2009/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3546 - acc: 1.0000 - val_loss: 2.4273 - val_acc: 0.5000\n",
            "Epoch 2010/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3606 - acc: 0.9950 - val_loss: 2.5056 - val_acc: 0.5000\n",
            "Epoch 2011/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3533 - acc: 1.0000 - val_loss: 2.3414 - val_acc: 0.5000\n",
            "Epoch 2012/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3538 - acc: 1.0000 - val_loss: 2.1890 - val_acc: 0.5000\n",
            "Epoch 2013/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3523 - acc: 1.0000 - val_loss: 2.0532 - val_acc: 0.5000\n",
            "Epoch 2014/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3523 - acc: 1.0000 - val_loss: 2.0500 - val_acc: 0.5000\n",
            "Epoch 2015/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3512 - acc: 1.0000 - val_loss: 2.1837 - val_acc: 0.5000\n",
            "Epoch 2016/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3516 - acc: 1.0000 - val_loss: 2.0511 - val_acc: 0.5000\n",
            "Epoch 2017/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3529 - acc: 1.0000 - val_loss: 2.0790 - val_acc: 0.5000\n",
            "Epoch 2018/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3490 - acc: 1.0000 - val_loss: 1.8731 - val_acc: 0.5000\n",
            "Epoch 2019/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3479 - acc: 1.0000 - val_loss: 1.7088 - val_acc: 0.5000\n",
            "Epoch 2020/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3473 - acc: 1.0000 - val_loss: 1.7292 - val_acc: 0.5000\n",
            "Epoch 2021/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3465 - acc: 1.0000 - val_loss: 1.7413 - val_acc: 0.5000\n",
            "Epoch 2022/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3457 - acc: 1.0000 - val_loss: 1.6585 - val_acc: 0.5000\n",
            "Epoch 2023/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3449 - acc: 1.0000 - val_loss: 1.6218 - val_acc: 0.5000\n",
            "Epoch 2024/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3440 - acc: 1.0000 - val_loss: 1.6037 - val_acc: 0.5000\n",
            "Epoch 2025/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3434 - acc: 1.0000 - val_loss: 1.5968 - val_acc: 0.5000\n",
            "Epoch 2026/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3424 - acc: 1.0000 - val_loss: 1.7147 - val_acc: 0.5000\n",
            "Epoch 2027/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3416 - acc: 1.0000 - val_loss: 1.7218 - val_acc: 0.5000\n",
            "Epoch 2028/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3408 - acc: 1.0000 - val_loss: 1.6988 - val_acc: 0.5000\n",
            "Epoch 2029/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3399 - acc: 1.0000 - val_loss: 1.5739 - val_acc: 0.5000\n",
            "Epoch 2030/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3392 - acc: 1.0000 - val_loss: 1.6741 - val_acc: 0.5000\n",
            "Epoch 2031/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3387 - acc: 1.0000 - val_loss: 1.5902 - val_acc: 0.5000\n",
            "Epoch 2032/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3375 - acc: 1.0000 - val_loss: 1.5232 - val_acc: 0.5000\n",
            "Epoch 2033/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3366 - acc: 1.0000 - val_loss: 1.5278 - val_acc: 0.5000\n",
            "Epoch 2034/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3360 - acc: 1.0000 - val_loss: 1.4494 - val_acc: 0.5000\n",
            "Epoch 2035/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4344 - acc: 0.9800 - val_loss: 1.6158 - val_acc: 0.5000\n",
            "Epoch 2036/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3362 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.5000\n",
            "Epoch 2037/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3408 - acc: 1.0000 - val_loss: 1.5127 - val_acc: 0.6400\n",
            "Epoch 2038/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3441 - acc: 1.0000 - val_loss: 0.9139 - val_acc: 0.8400\n",
            "Epoch 2039/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4239 - acc: 0.9900 - val_loss: 7.8268 - val_acc: 0.0200\n",
            "Epoch 2040/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5860 - acc: 0.9700 - val_loss: 2.5161 - val_acc: 0.5000\n",
            "Epoch 2041/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3519 - acc: 1.0000 - val_loss: 2.1280 - val_acc: 0.5000\n",
            "Epoch 2042/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3554 - acc: 1.0000 - val_loss: 2.2628 - val_acc: 0.5000\n",
            "Epoch 2043/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3607 - acc: 0.9950 - val_loss: 2.0289 - val_acc: 0.5000\n",
            "Epoch 2044/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3583 - acc: 1.0000 - val_loss: 2.0744 - val_acc: 0.5000\n",
            "Epoch 2045/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3602 - acc: 1.0000 - val_loss: 2.0145 - val_acc: 0.5000\n",
            "Epoch 2046/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3571 - acc: 1.0000 - val_loss: 2.1381 - val_acc: 0.5000\n",
            "Epoch 2047/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3563 - acc: 1.0000 - val_loss: 1.9940 - val_acc: 0.5000\n",
            "Epoch 2048/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3554 - acc: 1.0000 - val_loss: 2.0026 - val_acc: 0.5000\n",
            "Epoch 2049/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3544 - acc: 1.0000 - val_loss: 1.8978 - val_acc: 0.5000\n",
            "Epoch 2050/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3535 - acc: 1.0000 - val_loss: 1.9787 - val_acc: 0.5000\n",
            "Epoch 2051/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3525 - acc: 1.0000 - val_loss: 2.0023 - val_acc: 0.5000\n",
            "Epoch 2052/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3516 - acc: 1.0000 - val_loss: 2.0311 - val_acc: 0.5000\n",
            "Epoch 2053/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3506 - acc: 1.0000 - val_loss: 1.9314 - val_acc: 0.5000\n",
            "Epoch 2054/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3499 - acc: 1.0000 - val_loss: 2.1024 - val_acc: 0.5000\n",
            "Epoch 2055/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3488 - acc: 1.0000 - val_loss: 1.9930 - val_acc: 0.5000\n",
            "Epoch 2056/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3482 - acc: 1.0000 - val_loss: 1.9122 - val_acc: 0.5000\n",
            "Epoch 2057/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3470 - acc: 1.0000 - val_loss: 2.0257 - val_acc: 0.5000\n",
            "Epoch 2058/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3459 - acc: 1.0000 - val_loss: 1.9891 - val_acc: 0.5000\n",
            "Epoch 2059/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3555 - acc: 0.9950 - val_loss: 2.0668 - val_acc: 0.5000\n",
            "Epoch 2060/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3772 - acc: 0.9900 - val_loss: 1.8794 - val_acc: 0.5000\n",
            "Epoch 2061/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4002 - acc: 0.9750 - val_loss: 1.5990 - val_acc: 0.5000\n",
            "Epoch 2062/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3452 - acc: 1.0000 - val_loss: 1.4749 - val_acc: 0.5000\n",
            "Epoch 2063/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3458 - acc: 1.0000 - val_loss: 1.4817 - val_acc: 0.5000\n",
            "Epoch 2064/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3583 - acc: 0.9950 - val_loss: 1.6257 - val_acc: 0.5000\n",
            "Epoch 2065/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3560 - acc: 0.9900 - val_loss: 1.6520 - val_acc: 0.5000\n",
            "Epoch 2066/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3435 - acc: 1.0000 - val_loss: 1.5695 - val_acc: 0.5000\n",
            "Epoch 2067/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3427 - acc: 1.0000 - val_loss: 1.6431 - val_acc: 0.5000\n",
            "Epoch 2068/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3418 - acc: 1.0000 - val_loss: 1.7351 - val_acc: 0.5000\n",
            "Epoch 2069/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3409 - acc: 1.0000 - val_loss: 1.7447 - val_acc: 0.5000\n",
            "Epoch 2070/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3399 - acc: 1.0000 - val_loss: 1.6804 - val_acc: 0.5000\n",
            "Epoch 2071/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3390 - acc: 1.0000 - val_loss: 1.8405 - val_acc: 0.5000\n",
            "Epoch 2072/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3381 - acc: 1.0000 - val_loss: 1.7588 - val_acc: 0.5000\n",
            "Epoch 2073/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3372 - acc: 1.0000 - val_loss: 1.8452 - val_acc: 0.5000\n",
            "Epoch 2074/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3363 - acc: 1.0000 - val_loss: 1.8974 - val_acc: 0.5000\n",
            "Epoch 2075/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3355 - acc: 1.0000 - val_loss: 1.6892 - val_acc: 0.5000\n",
            "Epoch 2076/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3347 - acc: 1.0000 - val_loss: 1.8923 - val_acc: 0.5000\n",
            "Epoch 2077/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3336 - acc: 1.0000 - val_loss: 1.9297 - val_acc: 0.5000\n",
            "Epoch 2078/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3328 - acc: 1.0000 - val_loss: 1.8546 - val_acc: 0.5000\n",
            "Epoch 2079/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3319 - acc: 1.0000 - val_loss: 1.9294 - val_acc: 0.5000\n",
            "Epoch 2080/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3318 - acc: 1.0000 - val_loss: 1.9196 - val_acc: 0.5000\n",
            "Epoch 2081/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3301 - acc: 1.0000 - val_loss: 1.7666 - val_acc: 0.5000\n",
            "Epoch 2082/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3296 - acc: 1.0000 - val_loss: 1.7771 - val_acc: 0.5000\n",
            "Epoch 2083/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3285 - acc: 1.0000 - val_loss: 1.7898 - val_acc: 0.5000\n",
            "Epoch 2084/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3277 - acc: 1.0000 - val_loss: 1.9147 - val_acc: 0.5000\n",
            "Epoch 2085/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3267 - acc: 1.0000 - val_loss: 1.8212 - val_acc: 0.5000\n",
            "Epoch 2086/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3259 - acc: 1.0000 - val_loss: 1.7292 - val_acc: 0.5000\n",
            "Epoch 2087/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3254 - acc: 1.0000 - val_loss: 1.9092 - val_acc: 0.5000\n",
            "Epoch 2088/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3242 - acc: 1.0000 - val_loss: 1.8225 - val_acc: 0.5000\n",
            "Epoch 2089/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3233 - acc: 1.0000 - val_loss: 1.8679 - val_acc: 0.5000\n",
            "Epoch 2090/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3227 - acc: 1.0000 - val_loss: 1.9289 - val_acc: 0.5000\n",
            "Epoch 2091/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3218 - acc: 1.0000 - val_loss: 1.9916 - val_acc: 0.5000\n",
            "Epoch 2092/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3211 - acc: 1.0000 - val_loss: 2.0224 - val_acc: 0.5000\n",
            "Epoch 2093/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3201 - acc: 1.0000 - val_loss: 1.9138 - val_acc: 0.5000\n",
            "Epoch 2094/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3193 - acc: 1.0000 - val_loss: 1.9746 - val_acc: 0.5000\n",
            "Epoch 2095/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3185 - acc: 1.0000 - val_loss: 1.7551 - val_acc: 0.5000\n",
            "Epoch 2096/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3176 - acc: 1.0000 - val_loss: 2.1060 - val_acc: 0.5000\n",
            "Epoch 2097/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3168 - acc: 1.0000 - val_loss: 1.8038 - val_acc: 0.5000\n",
            "Epoch 2098/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3161 - acc: 1.0000 - val_loss: 1.9033 - val_acc: 0.5000\n",
            "Epoch 2099/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3152 - acc: 1.0000 - val_loss: 1.8542 - val_acc: 0.5000\n",
            "Epoch 2100/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3144 - acc: 1.0000 - val_loss: 2.0984 - val_acc: 0.5000\n",
            "Epoch 2101/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3137 - acc: 1.0000 - val_loss: 1.9412 - val_acc: 0.5000\n",
            "Epoch 2102/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3129 - acc: 1.0000 - val_loss: 1.8883 - val_acc: 0.5000\n",
            "Epoch 2103/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3121 - acc: 1.0000 - val_loss: 1.9709 - val_acc: 0.5000\n",
            "Epoch 2104/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3113 - acc: 1.0000 - val_loss: 1.8882 - val_acc: 0.5000\n",
            "Epoch 2105/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3105 - acc: 1.0000 - val_loss: 2.0966 - val_acc: 0.5000\n",
            "Epoch 2106/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3097 - acc: 1.0000 - val_loss: 1.9564 - val_acc: 0.5000\n",
            "Epoch 2107/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3090 - acc: 1.0000 - val_loss: 1.9789 - val_acc: 0.5000\n",
            "Epoch 2108/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4363 - acc: 0.9850 - val_loss: 1.6481 - val_acc: 0.5000\n",
            "Epoch 2109/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3087 - acc: 1.0000 - val_loss: 1.6514 - val_acc: 0.5000\n",
            "Epoch 2110/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3243 - acc: 0.9950 - val_loss: 1.5637 - val_acc: 0.5000\n",
            "Epoch 2111/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3142 - acc: 1.0000 - val_loss: 1.4266 - val_acc: 0.5000\n",
            "Epoch 2112/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3164 - acc: 1.0000 - val_loss: 1.4127 - val_acc: 0.5000\n",
            "Epoch 2113/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3640 - acc: 0.9850 - val_loss: 1.3585 - val_acc: 0.5000\n",
            "Epoch 2114/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3182 - acc: 1.0000 - val_loss: 1.2988 - val_acc: 0.5000\n",
            "Epoch 2115/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3282 - acc: 0.9950 - val_loss: 1.2859 - val_acc: 0.5000\n",
            "Epoch 2116/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3715 - acc: 0.9850 - val_loss: 1.2617 - val_acc: 0.5000\n",
            "Epoch 2117/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3168 - acc: 1.0000 - val_loss: 1.3154 - val_acc: 0.5000\n",
            "Epoch 2118/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3150 - acc: 1.0000 - val_loss: 1.2528 - val_acc: 0.5000\n",
            "Epoch 2119/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3143 - acc: 1.0000 - val_loss: 1.2806 - val_acc: 0.5000\n",
            "Epoch 2120/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3137 - acc: 1.0000 - val_loss: 1.2537 - val_acc: 0.5000\n",
            "Epoch 2121/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3126 - acc: 1.0000 - val_loss: 1.2248 - val_acc: 0.5000\n",
            "Epoch 2122/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3119 - acc: 1.0000 - val_loss: 1.2601 - val_acc: 0.5000\n",
            "Epoch 2123/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3111 - acc: 1.0000 - val_loss: 1.1717 - val_acc: 0.5000\n",
            "Epoch 2124/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3101 - acc: 1.0000 - val_loss: 1.2277 - val_acc: 0.5000\n",
            "Epoch 2125/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3093 - acc: 1.0000 - val_loss: 1.2677 - val_acc: 0.5000\n",
            "Epoch 2126/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3086 - acc: 1.0000 - val_loss: 1.2410 - val_acc: 0.5000\n",
            "Epoch 2127/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3106 - acc: 1.0000 - val_loss: 1.1672 - val_acc: 0.5000\n",
            "Epoch 2128/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3068 - acc: 1.0000 - val_loss: 1.2186 - val_acc: 0.5000\n",
            "Epoch 2129/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3061 - acc: 1.0000 - val_loss: 1.1360 - val_acc: 0.5000\n",
            "Epoch 2130/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3053 - acc: 1.0000 - val_loss: 1.1543 - val_acc: 0.5000\n",
            "Epoch 2131/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3045 - acc: 1.0000 - val_loss: 1.1090 - val_acc: 0.5000\n",
            "Epoch 2132/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3042 - acc: 1.0000 - val_loss: 1.1018 - val_acc: 0.5000\n",
            "Epoch 2133/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3029 - acc: 1.0000 - val_loss: 1.1272 - val_acc: 0.5000\n",
            "Epoch 2134/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3022 - acc: 1.0000 - val_loss: 1.1359 - val_acc: 0.5000\n",
            "Epoch 2135/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3015 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.5000\n",
            "Epoch 2136/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3007 - acc: 1.0000 - val_loss: 1.1578 - val_acc: 0.5000\n",
            "Epoch 2137/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3000 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.5000\n",
            "Epoch 2138/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2991 - acc: 1.0000 - val_loss: 1.1229 - val_acc: 0.5000\n",
            "Epoch 2139/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2984 - acc: 1.0000 - val_loss: 1.1165 - val_acc: 0.5000\n",
            "Epoch 2140/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2976 - acc: 1.0000 - val_loss: 1.1348 - val_acc: 0.5000\n",
            "Epoch 2141/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2969 - acc: 1.0000 - val_loss: 1.1080 - val_acc: 0.5000\n",
            "Epoch 2142/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2961 - acc: 1.0000 - val_loss: 0.9811 - val_acc: 0.5000\n",
            "Epoch 2143/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2953 - acc: 1.0000 - val_loss: 1.0866 - val_acc: 0.5000\n",
            "Epoch 2144/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2947 - acc: 1.0000 - val_loss: 1.0906 - val_acc: 0.5000\n",
            "Epoch 2145/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3850 - acc: 0.9800 - val_loss: 1.0502 - val_acc: 0.5000\n",
            "Epoch 2146/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2966 - acc: 1.0000 - val_loss: 0.6721 - val_acc: 0.6700\n",
            "Epoch 2147/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5070 - acc: 0.9700 - val_loss: 0.5424 - val_acc: 0.8500\n",
            "Epoch 2148/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3884 - acc: 0.9850 - val_loss: 2.1871 - val_acc: 0.5500\n",
            "Epoch 2149/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3364 - acc: 1.0000 - val_loss: 9.0177 - val_acc: 0.2400\n",
            "Epoch 2150/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3466 - acc: 1.0000 - val_loss: 9.2622 - val_acc: 0.2600\n",
            "Epoch 2151/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3517 - acc: 1.0000 - val_loss: 9.1995 - val_acc: 0.2000\n",
            "Epoch 2152/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3544 - acc: 1.0000 - val_loss: 9.4048 - val_acc: 0.2300\n",
            "Epoch 2153/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3599 - acc: 1.0000 - val_loss: 6.4968 - val_acc: 0.2400\n",
            "Epoch 2154/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3542 - acc: 1.0000 - val_loss: 2.1119 - val_acc: 0.5200\n",
            "Epoch 2155/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3538 - acc: 1.0000 - val_loss: 3.6598 - val_acc: 0.1800\n",
            "Epoch 2156/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4244 - acc: 0.9800 - val_loss: 1.9272 - val_acc: 0.5000\n",
            "Epoch 2157/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4028 - acc: 0.9850 - val_loss: 3.9799 - val_acc: 0.5000\n",
            "Epoch 2158/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3612 - acc: 1.0000 - val_loss: 4.5842 - val_acc: 0.5000\n",
            "Epoch 2159/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3669 - acc: 1.0000 - val_loss: 4.6171 - val_acc: 0.5000\n",
            "Epoch 2160/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3701 - acc: 1.0000 - val_loss: 4.6296 - val_acc: 0.5000\n",
            "Epoch 2161/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3713 - acc: 1.0000 - val_loss: 4.1625 - val_acc: 0.5000\n",
            "Epoch 2162/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3719 - acc: 1.0000 - val_loss: 3.3669 - val_acc: 0.5000\n",
            "Epoch 2163/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3714 - acc: 1.0000 - val_loss: 2.7108 - val_acc: 0.5000\n",
            "Epoch 2164/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3700 - acc: 1.0000 - val_loss: 2.6466 - val_acc: 0.5000\n",
            "Epoch 2165/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3693 - acc: 1.0000 - val_loss: 2.4013 - val_acc: 0.5000\n",
            "Epoch 2166/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3682 - acc: 1.0000 - val_loss: 1.9906 - val_acc: 0.5000\n",
            "Epoch 2167/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3680 - acc: 1.0000 - val_loss: 1.8059 - val_acc: 0.5000\n",
            "Epoch 2168/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3664 - acc: 1.0000 - val_loss: 1.8341 - val_acc: 0.5000\n",
            "Epoch 2169/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3654 - acc: 1.0000 - val_loss: 1.6260 - val_acc: 0.5000\n",
            "Epoch 2170/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3645 - acc: 1.0000 - val_loss: 1.6705 - val_acc: 0.5000\n",
            "Epoch 2171/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3635 - acc: 1.0000 - val_loss: 1.5879 - val_acc: 0.5000\n",
            "Epoch 2172/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3636 - acc: 1.0000 - val_loss: 1.4614 - val_acc: 0.5000\n",
            "Epoch 2173/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4108 - acc: 0.9900 - val_loss: 1.3529 - val_acc: 0.5000\n",
            "Epoch 2174/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3634 - acc: 1.0000 - val_loss: 1.2431 - val_acc: 0.5000\n",
            "Epoch 2175/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5170 - acc: 0.9800 - val_loss: 1.2768 - val_acc: 0.5000\n",
            "Epoch 2176/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3613 - acc: 1.0000 - val_loss: 1.9183 - val_acc: 0.5000\n",
            "Epoch 2177/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3645 - acc: 1.0000 - val_loss: 2.0013 - val_acc: 0.5000\n",
            "Epoch 2178/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4681 - acc: 0.9900 - val_loss: 1.9231 - val_acc: 0.5000\n",
            "Epoch 2179/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3663 - acc: 1.0000 - val_loss: 1.9027 - val_acc: 0.5000\n",
            "Epoch 2180/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3818 - acc: 0.9950 - val_loss: 1.9908 - val_acc: 0.5100\n",
            "Epoch 2181/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3648 - acc: 1.0000 - val_loss: 2.3939 - val_acc: 0.5200\n",
            "Epoch 2182/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4109 - acc: 0.9850 - val_loss: 2.4440 - val_acc: 0.5300\n",
            "Epoch 2183/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3648 - acc: 1.0000 - val_loss: 2.2045 - val_acc: 0.5400\n",
            "Epoch 2184/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3655 - acc: 1.0000 - val_loss: 2.4407 - val_acc: 0.5400\n",
            "Epoch 2185/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3640 - acc: 1.0000 - val_loss: 2.5945 - val_acc: 0.5100\n",
            "Epoch 2186/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3618 - acc: 1.0000 - val_loss: 2.4454 - val_acc: 0.5300\n",
            "Epoch 2187/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3607 - acc: 1.0000 - val_loss: 2.4622 - val_acc: 0.5300\n",
            "Epoch 2188/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3600 - acc: 1.0000 - val_loss: 2.5514 - val_acc: 0.5200\n",
            "Epoch 2189/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3593 - acc: 1.0000 - val_loss: 2.8763 - val_acc: 0.5100\n",
            "Epoch 2190/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3589 - acc: 1.0000 - val_loss: 2.9041 - val_acc: 0.5100\n",
            "Epoch 2191/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3571 - acc: 1.0000 - val_loss: 2.9694 - val_acc: 0.5100\n",
            "Epoch 2192/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3565 - acc: 1.0000 - val_loss: 2.9600 - val_acc: 0.5100\n",
            "Epoch 2193/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3583 - acc: 1.0000 - val_loss: 2.8472 - val_acc: 0.5100\n",
            "Epoch 2194/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3543 - acc: 1.0000 - val_loss: 2.8579 - val_acc: 0.5100\n",
            "Epoch 2195/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3545 - acc: 1.0000 - val_loss: 2.7655 - val_acc: 0.5100\n",
            "Epoch 2196/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3525 - acc: 1.0000 - val_loss: 3.0022 - val_acc: 0.5100\n",
            "Epoch 2197/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3524 - acc: 1.0000 - val_loss: 3.0688 - val_acc: 0.5100\n",
            "Epoch 2198/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3509 - acc: 1.0000 - val_loss: 2.8804 - val_acc: 0.5100\n",
            "Epoch 2199/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3500 - acc: 1.0000 - val_loss: 2.7892 - val_acc: 0.5100\n",
            "Epoch 2200/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3490 - acc: 1.0000 - val_loss: 2.7182 - val_acc: 0.5100\n",
            "Epoch 2201/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3489 - acc: 1.0000 - val_loss: 2.9737 - val_acc: 0.5100\n",
            "Epoch 2202/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3472 - acc: 1.0000 - val_loss: 2.7963 - val_acc: 0.5100\n",
            "Epoch 2203/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3464 - acc: 1.0000 - val_loss: 2.9562 - val_acc: 0.5100\n",
            "Epoch 2204/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3460 - acc: 1.0000 - val_loss: 2.7208 - val_acc: 0.5100\n",
            "Epoch 2205/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3812 - acc: 0.9900 - val_loss: 3.0189 - val_acc: 0.5100\n",
            "Epoch 2206/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3447 - acc: 1.0000 - val_loss: 3.0549 - val_acc: 0.5600\n",
            "Epoch 2207/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4735 - acc: 0.9850 - val_loss: 3.7929 - val_acc: 0.4900\n",
            "Epoch 2208/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3973 - acc: 0.9950 - val_loss: 3.3251 - val_acc: 0.5000\n",
            "Epoch 2209/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3468 - acc: 1.0000 - val_loss: 3.1915 - val_acc: 0.5000\n",
            "Epoch 2210/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3479 - acc: 1.0000 - val_loss: 3.0120 - val_acc: 0.5000\n",
            "Epoch 2211/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3463 - acc: 1.0000 - val_loss: 2.8433 - val_acc: 0.5000\n",
            "Epoch 2212/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3505 - acc: 0.9950 - val_loss: 2.6008 - val_acc: 0.5000\n",
            "Epoch 2213/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3449 - acc: 1.0000 - val_loss: 2.6065 - val_acc: 0.5000\n",
            "Epoch 2214/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3441 - acc: 1.0000 - val_loss: 2.4496 - val_acc: 0.5000\n",
            "Epoch 2215/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3434 - acc: 1.0000 - val_loss: 2.1975 - val_acc: 0.5000\n",
            "Epoch 2216/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3425 - acc: 1.0000 - val_loss: 2.2359 - val_acc: 0.5000\n",
            "Epoch 2217/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3416 - acc: 1.0000 - val_loss: 2.2258 - val_acc: 0.5000\n",
            "Epoch 2218/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3408 - acc: 1.0000 - val_loss: 2.0916 - val_acc: 0.5000\n",
            "Epoch 2219/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3400 - acc: 1.0000 - val_loss: 1.9918 - val_acc: 0.5000\n",
            "Epoch 2220/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3391 - acc: 1.0000 - val_loss: 1.9588 - val_acc: 0.5000\n",
            "Epoch 2221/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3445 - acc: 0.9950 - val_loss: 1.8471 - val_acc: 0.5000\n",
            "Epoch 2222/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3376 - acc: 1.0000 - val_loss: 1.7103 - val_acc: 0.5000\n",
            "Epoch 2223/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3367 - acc: 1.0000 - val_loss: 1.5795 - val_acc: 0.5000\n",
            "Epoch 2224/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3364 - acc: 1.0000 - val_loss: 1.5876 - val_acc: 0.5000\n",
            "Epoch 2225/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3352 - acc: 1.0000 - val_loss: 1.5733 - val_acc: 0.5000\n",
            "Epoch 2226/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3345 - acc: 1.0000 - val_loss: 1.5414 - val_acc: 0.5000\n",
            "Epoch 2227/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3339 - acc: 1.0000 - val_loss: 1.5369 - val_acc: 0.5000\n",
            "Epoch 2228/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3334 - acc: 1.0000 - val_loss: 1.5703 - val_acc: 0.5000\n",
            "Epoch 2229/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3318 - acc: 1.0000 - val_loss: 1.5696 - val_acc: 0.5000\n",
            "Epoch 2230/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3310 - acc: 1.0000 - val_loss: 1.6365 - val_acc: 0.5000\n",
            "Epoch 2231/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3302 - acc: 1.0000 - val_loss: 1.5688 - val_acc: 0.5000\n",
            "Epoch 2232/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3295 - acc: 1.0000 - val_loss: 1.5444 - val_acc: 0.5000\n",
            "Epoch 2233/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3313 - acc: 1.0000 - val_loss: 1.4665 - val_acc: 0.5000\n",
            "Epoch 2234/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3281 - acc: 1.0000 - val_loss: 1.6768 - val_acc: 0.5000\n",
            "Epoch 2235/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3271 - acc: 1.0000 - val_loss: 1.7448 - val_acc: 0.5000\n",
            "Epoch 2236/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3266 - acc: 1.0000 - val_loss: 1.6918 - val_acc: 0.5000\n",
            "Epoch 2237/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3256 - acc: 1.0000 - val_loss: 1.7569 - val_acc: 0.5000\n",
            "Epoch 2238/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3248 - acc: 1.0000 - val_loss: 1.7936 - val_acc: 0.5000\n",
            "Epoch 2239/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3239 - acc: 1.0000 - val_loss: 1.8341 - val_acc: 0.5000\n",
            "Epoch 2240/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3232 - acc: 1.0000 - val_loss: 1.7222 - val_acc: 0.5000\n",
            "Epoch 2241/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3225 - acc: 1.0000 - val_loss: 1.8504 - val_acc: 0.5000\n",
            "Epoch 2242/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3220 - acc: 1.0000 - val_loss: 1.7674 - val_acc: 0.5000\n",
            "Epoch 2243/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3208 - acc: 1.0000 - val_loss: 1.9395 - val_acc: 0.5000\n",
            "Epoch 2244/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3202 - acc: 1.0000 - val_loss: 1.8592 - val_acc: 0.5000\n",
            "Epoch 2245/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3193 - acc: 1.0000 - val_loss: 1.8802 - val_acc: 0.5000\n",
            "Epoch 2246/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3186 - acc: 1.0000 - val_loss: 1.8710 - val_acc: 0.5000\n",
            "Epoch 2247/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3178 - acc: 1.0000 - val_loss: 1.8457 - val_acc: 0.5000\n",
            "Epoch 2248/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3171 - acc: 1.0000 - val_loss: 1.8773 - val_acc: 0.5000\n",
            "Epoch 2249/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3166 - acc: 1.0000 - val_loss: 1.8975 - val_acc: 0.5000\n",
            "Epoch 2250/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3155 - acc: 1.0000 - val_loss: 1.7976 - val_acc: 0.5000\n",
            "Epoch 2251/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3149 - acc: 1.0000 - val_loss: 1.8773 - val_acc: 0.5000\n",
            "Epoch 2252/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4810 - acc: 0.9900 - val_loss: 1.8854 - val_acc: 0.5000\n",
            "Epoch 2253/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3134 - acc: 1.0000 - val_loss: 1.7690 - val_acc: 0.5000\n",
            "Epoch 2254/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3128 - acc: 1.0000 - val_loss: 1.7257 - val_acc: 0.5000\n",
            "Epoch 2255/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3974 - acc: 0.9900 - val_loss: 1.7633 - val_acc: 0.5000\n",
            "Epoch 2256/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3168 - acc: 0.9950 - val_loss: 2.0488 - val_acc: 0.5000\n",
            "Epoch 2257/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3109 - acc: 1.0000 - val_loss: 2.2384 - val_acc: 0.5000\n",
            "Epoch 2258/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3218 - acc: 0.9950 - val_loss: 2.1744 - val_acc: 0.5000\n",
            "Epoch 2259/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3106 - acc: 1.0000 - val_loss: 2.0263 - val_acc: 0.5000\n",
            "Epoch 2260/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3090 - acc: 1.0000 - val_loss: 1.9608 - val_acc: 0.5000\n",
            "Epoch 2261/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3086 - acc: 1.0000 - val_loss: 1.7214 - val_acc: 0.5000\n",
            "Epoch 2262/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3153 - acc: 0.9950 - val_loss: 1.7853 - val_acc: 0.5000\n",
            "Epoch 2263/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3068 - acc: 1.0000 - val_loss: 1.9273 - val_acc: 0.5000\n",
            "Epoch 2264/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3077 - acc: 1.0000 - val_loss: 1.9306 - val_acc: 0.5000\n",
            "Epoch 2265/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3115 - acc: 1.0000 - val_loss: 2.1625 - val_acc: 0.5000\n",
            "Epoch 2266/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3062 - acc: 1.0000 - val_loss: 2.2028 - val_acc: 0.5000\n",
            "Epoch 2267/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3042 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 0.5000\n",
            "Epoch 2268/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3034 - acc: 1.0000 - val_loss: 2.2166 - val_acc: 0.5000\n",
            "Epoch 2269/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3029 - acc: 1.0000 - val_loss: 2.3922 - val_acc: 0.5000\n",
            "Epoch 2270/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3024 - acc: 1.0000 - val_loss: 2.4243 - val_acc: 0.5000\n",
            "Epoch 2271/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3014 - acc: 1.0000 - val_loss: 2.3517 - val_acc: 0.5000\n",
            "Epoch 2272/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3006 - acc: 1.0000 - val_loss: 2.3236 - val_acc: 0.5000\n",
            "Epoch 2273/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2999 - acc: 1.0000 - val_loss: 2.2800 - val_acc: 0.5000\n",
            "Epoch 2274/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2994 - acc: 1.0000 - val_loss: 2.2370 - val_acc: 0.5000\n",
            "Epoch 2275/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2985 - acc: 1.0000 - val_loss: 2.2678 - val_acc: 0.5000\n",
            "Epoch 2276/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2979 - acc: 1.0000 - val_loss: 2.2191 - val_acc: 0.5000\n",
            "Epoch 2277/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2971 - acc: 1.0000 - val_loss: 2.3603 - val_acc: 0.5000\n",
            "Epoch 2278/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2966 - acc: 1.0000 - val_loss: 2.1127 - val_acc: 0.5000\n",
            "Epoch 2279/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2960 - acc: 1.0000 - val_loss: 2.2639 - val_acc: 0.5000\n",
            "Epoch 2280/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2951 - acc: 1.0000 - val_loss: 2.2398 - val_acc: 0.5000\n",
            "Epoch 2281/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2979 - acc: 1.0000 - val_loss: 2.2411 - val_acc: 0.5000\n",
            "Epoch 2282/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2938 - acc: 1.0000 - val_loss: 2.2592 - val_acc: 0.5000\n",
            "Epoch 2283/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2935 - acc: 1.0000 - val_loss: 2.4110 - val_acc: 0.5000\n",
            "Epoch 2284/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2925 - acc: 1.0000 - val_loss: 2.2371 - val_acc: 0.5000\n",
            "Epoch 2285/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2919 - acc: 1.0000 - val_loss: 2.2389 - val_acc: 0.5000\n",
            "Epoch 2286/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2914 - acc: 1.0000 - val_loss: 2.3553 - val_acc: 0.5000\n",
            "Epoch 2287/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2907 - acc: 1.0000 - val_loss: 2.2665 - val_acc: 0.5000\n",
            "Epoch 2288/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2962 - acc: 0.9950 - val_loss: 2.3029 - val_acc: 0.5000\n",
            "Epoch 2289/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2893 - acc: 1.0000 - val_loss: 2.1121 - val_acc: 0.5000\n",
            "Epoch 2290/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2887 - acc: 1.0000 - val_loss: 2.2253 - val_acc: 0.5000\n",
            "Epoch 2291/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2918 - acc: 0.9950 - val_loss: 2.2428 - val_acc: 0.5000\n",
            "Epoch 2292/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2875 - acc: 1.0000 - val_loss: 2.4125 - val_acc: 0.5000\n",
            "Epoch 2293/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2871 - acc: 1.0000 - val_loss: 2.7233 - val_acc: 0.5000\n",
            "Epoch 2294/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2866 - acc: 1.0000 - val_loss: 2.7078 - val_acc: 0.5000\n",
            "Epoch 2295/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2867 - acc: 1.0000 - val_loss: 2.6428 - val_acc: 0.5000\n",
            "Epoch 2296/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2855 - acc: 1.0000 - val_loss: 2.5426 - val_acc: 0.5000\n",
            "Epoch 2297/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2848 - acc: 1.0000 - val_loss: 2.5511 - val_acc: 0.5000\n",
            "Epoch 2298/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2842 - acc: 1.0000 - val_loss: 2.5333 - val_acc: 0.5000\n",
            "Epoch 2299/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2836 - acc: 1.0000 - val_loss: 2.4709 - val_acc: 0.5000\n",
            "Epoch 2300/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2830 - acc: 1.0000 - val_loss: 2.5880 - val_acc: 0.5000\n",
            "Epoch 2301/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2823 - acc: 1.0000 - val_loss: 2.5215 - val_acc: 0.5000\n",
            "Epoch 2302/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2833 - acc: 1.0000 - val_loss: 2.5044 - val_acc: 0.5000\n",
            "Epoch 2303/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2811 - acc: 1.0000 - val_loss: 2.3760 - val_acc: 0.5000\n",
            "Epoch 2304/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2805 - acc: 1.0000 - val_loss: 2.3945 - val_acc: 0.5000\n",
            "Epoch 2305/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2799 - acc: 1.0000 - val_loss: 2.3441 - val_acc: 0.5000\n",
            "Epoch 2306/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2793 - acc: 1.0000 - val_loss: 2.5541 - val_acc: 0.5000\n",
            "Epoch 2307/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2787 - acc: 1.0000 - val_loss: 2.3735 - val_acc: 0.5000\n",
            "Epoch 2308/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2781 - acc: 1.0000 - val_loss: 2.3777 - val_acc: 0.5000\n",
            "Epoch 2309/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2775 - acc: 1.0000 - val_loss: 2.4835 - val_acc: 0.5000\n",
            "Epoch 2310/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2769 - acc: 1.0000 - val_loss: 2.4070 - val_acc: 0.5000\n",
            "Epoch 2311/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2765 - acc: 1.0000 - val_loss: 2.4360 - val_acc: 0.5000\n",
            "Epoch 2312/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2757 - acc: 1.0000 - val_loss: 2.3712 - val_acc: 0.5000\n",
            "Epoch 2313/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2751 - acc: 1.0000 - val_loss: 2.3377 - val_acc: 0.5000\n",
            "Epoch 2314/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2745 - acc: 1.0000 - val_loss: 2.3729 - val_acc: 0.5000\n",
            "Epoch 2315/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2739 - acc: 1.0000 - val_loss: 2.5526 - val_acc: 0.5000\n",
            "Epoch 2316/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2746 - acc: 1.0000 - val_loss: 2.4396 - val_acc: 0.5000\n",
            "Epoch 2317/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2727 - acc: 1.0000 - val_loss: 2.2003 - val_acc: 0.5000\n",
            "Epoch 2318/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2722 - acc: 1.0000 - val_loss: 2.3672 - val_acc: 0.5000\n",
            "Epoch 2319/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2716 - acc: 1.0000 - val_loss: 2.3645 - val_acc: 0.5000\n",
            "Epoch 2320/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2710 - acc: 1.0000 - val_loss: 2.3789 - val_acc: 0.5000\n",
            "Epoch 2321/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2830 - acc: 0.9950 - val_loss: 2.1025 - val_acc: 0.5000\n",
            "Epoch 2322/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2699 - acc: 1.0000 - val_loss: 2.2298 - val_acc: 0.5000\n",
            "Epoch 2323/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2846 - acc: 0.9900 - val_loss: 2.3619 - val_acc: 0.5000\n",
            "Epoch 2324/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2690 - acc: 1.0000 - val_loss: 2.0239 - val_acc: 0.5000\n",
            "Epoch 2325/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2688 - acc: 1.0000 - val_loss: 1.9066 - val_acc: 0.5000\n",
            "Epoch 2326/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2685 - acc: 1.0000 - val_loss: 2.0359 - val_acc: 0.5000\n",
            "Epoch 2327/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2681 - acc: 1.0000 - val_loss: 2.1285 - val_acc: 0.5000\n",
            "Epoch 2328/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2792 - acc: 0.9950 - val_loss: 1.9920 - val_acc: 0.5000\n",
            "Epoch 2329/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2671 - acc: 1.0000 - val_loss: 1.7158 - val_acc: 0.5000\n",
            "Epoch 2330/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.5035 - acc: 0.9750 - val_loss: 0.9696 - val_acc: 0.5000\n",
            "Epoch 2331/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2672 - acc: 1.0000 - val_loss: 0.9461 - val_acc: 0.5000\n",
            "Epoch 2332/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3061 - acc: 0.9950 - val_loss: 0.8640 - val_acc: 0.5000\n",
            "Epoch 2333/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2743 - acc: 1.0000 - val_loss: 0.4779 - val_acc: 0.9300\n",
            "Epoch 2334/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2757 - acc: 1.0000 - val_loss: 0.6870 - val_acc: 0.5500\n",
            "Epoch 2335/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4165 - acc: 0.9900 - val_loss: 0.8748 - val_acc: 0.5000\n",
            "Epoch 2336/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2800 - acc: 1.0000 - val_loss: 1.3017 - val_acc: 0.8100\n",
            "Epoch 2337/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3243 - acc: 0.9950 - val_loss: 1.4636 - val_acc: 0.5000\n",
            "Epoch 2338/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2941 - acc: 1.0000 - val_loss: 1.0758 - val_acc: 0.5000\n",
            "Epoch 2339/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2978 - acc: 1.0000 - val_loss: 1.0615 - val_acc: 0.5000\n",
            "Epoch 2340/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2982 - acc: 1.0000 - val_loss: 1.0078 - val_acc: 0.5000\n",
            "Epoch 2341/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2985 - acc: 1.0000 - val_loss: 0.9605 - val_acc: 0.5000\n",
            "Epoch 2342/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4846 - acc: 0.9750 - val_loss: 0.9779 - val_acc: 0.5000\n",
            "Epoch 2343/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2993 - acc: 1.0000 - val_loss: 1.1529 - val_acc: 0.5000\n",
            "Epoch 2344/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3044 - acc: 1.0000 - val_loss: 0.8325 - val_acc: 0.6000\n",
            "Epoch 2345/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4692 - acc: 0.9850 - val_loss: 0.7879 - val_acc: 0.6300\n",
            "Epoch 2346/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4475 - acc: 0.9900 - val_loss: 1.1013 - val_acc: 0.5000\n",
            "Epoch 2347/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3113 - acc: 1.0000 - val_loss: 1.0797 - val_acc: 0.5000\n",
            "Epoch 2348/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4219 - acc: 0.9800 - val_loss: 1.0362 - val_acc: 0.5000\n",
            "Epoch 2349/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3154 - acc: 1.0000 - val_loss: 1.0898 - val_acc: 0.5200\n",
            "Epoch 2350/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3187 - acc: 1.0000 - val_loss: 1.4095 - val_acc: 0.5000\n",
            "Epoch 2351/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3373 - acc: 0.9900 - val_loss: 1.2339 - val_acc: 0.5100\n",
            "Epoch 2352/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4108 - acc: 0.9900 - val_loss: 1.1414 - val_acc: 0.5700\n",
            "Epoch 2353/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4126 - acc: 0.9900 - val_loss: 1.7351 - val_acc: 0.5000\n",
            "Epoch 2354/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3243 - acc: 1.0000 - val_loss: 1.7152 - val_acc: 0.5000\n",
            "Epoch 2355/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3265 - acc: 1.0000 - val_loss: 1.7654 - val_acc: 0.5100\n",
            "Epoch 2356/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3459 - acc: 0.9900 - val_loss: 1.9227 - val_acc: 0.5100\n",
            "Epoch 2357/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4588 - acc: 0.9700 - val_loss: 1.7424 - val_acc: 0.5100\n",
            "Epoch 2358/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3277 - acc: 1.0000 - val_loss: 1.8772 - val_acc: 0.5200\n",
            "Epoch 2359/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3294 - acc: 1.0000 - val_loss: 1.9194 - val_acc: 0.5000\n",
            "Epoch 2360/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3297 - acc: 1.0000 - val_loss: 1.7883 - val_acc: 0.5000\n",
            "Epoch 2361/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3298 - acc: 1.0000 - val_loss: 1.9007 - val_acc: 0.5000\n",
            "Epoch 2362/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3293 - acc: 1.0000 - val_loss: 1.9548 - val_acc: 0.5000\n",
            "Epoch 2363/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3354 - acc: 0.9950 - val_loss: 1.9915 - val_acc: 0.5000\n",
            "Epoch 2364/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3278 - acc: 1.0000 - val_loss: 1.8247 - val_acc: 0.5000\n",
            "Epoch 2365/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3270 - acc: 1.0000 - val_loss: 1.8194 - val_acc: 0.5000\n",
            "Epoch 2366/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3368 - acc: 0.9950 - val_loss: 1.7217 - val_acc: 0.5000\n",
            "Epoch 2367/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3319 - acc: 0.9950 - val_loss: 1.9184 - val_acc: 0.5000\n",
            "Epoch 2368/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3325 - acc: 0.9950 - val_loss: 1.8918 - val_acc: 0.5000\n",
            "Epoch 2369/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3236 - acc: 1.0000 - val_loss: 1.8746 - val_acc: 0.5000\n",
            "Epoch 2370/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3232 - acc: 1.0000 - val_loss: 2.0140 - val_acc: 0.5000\n",
            "Epoch 2371/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3224 - acc: 1.0000 - val_loss: 2.1386 - val_acc: 0.5000\n",
            "Epoch 2372/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3974 - acc: 0.9800 - val_loss: 2.0271 - val_acc: 0.5000\n",
            "Epoch 2373/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3204 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 0.5000\n",
            "Epoch 2374/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3197 - acc: 1.0000 - val_loss: 2.1387 - val_acc: 0.5000\n",
            "Epoch 2375/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3191 - acc: 1.0000 - val_loss: 2.0762 - val_acc: 0.5000\n",
            "Epoch 2376/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3201 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.5000\n",
            "Epoch 2377/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3179 - acc: 1.0000 - val_loss: 2.3179 - val_acc: 0.5000\n",
            "Epoch 2378/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3164 - acc: 1.0000 - val_loss: 2.4865 - val_acc: 0.5000\n",
            "Epoch 2379/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3159 - acc: 1.0000 - val_loss: 2.1643 - val_acc: 0.5000\n",
            "Epoch 2380/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3148 - acc: 1.0000 - val_loss: 2.1958 - val_acc: 0.5000\n",
            "Epoch 2381/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3146 - acc: 1.0000 - val_loss: 2.3493 - val_acc: 0.5000\n",
            "Epoch 2382/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3184 - acc: 0.9950 - val_loss: 2.5042 - val_acc: 0.5000\n",
            "Epoch 2383/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3346 - acc: 0.9900 - val_loss: 2.2927 - val_acc: 0.5000\n",
            "Epoch 2384/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3115 - acc: 1.0000 - val_loss: 1.7849 - val_acc: 0.5000\n",
            "Epoch 2385/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3115 - acc: 1.0000 - val_loss: 1.6337 - val_acc: 0.5000\n",
            "Epoch 2386/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3111 - acc: 1.0000 - val_loss: 1.8354 - val_acc: 0.5000\n",
            "Epoch 2387/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3184 - acc: 0.9950 - val_loss: 1.7295 - val_acc: 0.5000\n",
            "Epoch 2388/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3129 - acc: 1.0000 - val_loss: 1.5915 - val_acc: 0.5000\n",
            "Epoch 2389/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3089 - acc: 1.0000 - val_loss: 1.7727 - val_acc: 0.5000\n",
            "Epoch 2390/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3139 - acc: 0.9950 - val_loss: 1.6287 - val_acc: 0.5000\n",
            "Epoch 2391/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3082 - acc: 1.0000 - val_loss: 1.4505 - val_acc: 0.5000\n",
            "Epoch 2392/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3059 - acc: 1.0000 - val_loss: 1.6354 - val_acc: 0.5000\n",
            "Epoch 2393/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3067 - acc: 1.0000 - val_loss: 1.3628 - val_acc: 0.5000\n",
            "Epoch 2394/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3045 - acc: 1.0000 - val_loss: 1.3817 - val_acc: 0.5000\n",
            "Epoch 2395/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3031 - acc: 1.0000 - val_loss: 1.5080 - val_acc: 0.5000\n",
            "Epoch 2396/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3689 - acc: 0.9850 - val_loss: 1.1924 - val_acc: 0.5000\n",
            "Epoch 2397/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3026 - acc: 1.0000 - val_loss: 1.2998 - val_acc: 0.5000\n",
            "Epoch 2398/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3828 - acc: 0.9900 - val_loss: 0.9071 - val_acc: 0.5500\n",
            "Epoch 2399/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3057 - acc: 1.0000 - val_loss: 0.6975 - val_acc: 0.6200\n",
            "Epoch 2400/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3065 - acc: 1.0000 - val_loss: 0.6132 - val_acc: 0.7000\n",
            "Epoch 2401/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3077 - acc: 1.0000 - val_loss: 0.6813 - val_acc: 0.6600\n",
            "Epoch 2402/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3072 - acc: 1.0000 - val_loss: 0.7908 - val_acc: 0.5800\n",
            "Epoch 2403/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3101 - acc: 1.0000 - val_loss: 1.2426 - val_acc: 0.5000\n",
            "Epoch 2404/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3090 - acc: 1.0000 - val_loss: 1.0659 - val_acc: 0.5200\n",
            "Epoch 2405/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3047 - acc: 1.0000 - val_loss: 1.2595 - val_acc: 0.5200\n",
            "Epoch 2406/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3039 - acc: 1.0000 - val_loss: 1.4484 - val_acc: 0.5000\n",
            "Epoch 2407/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3032 - acc: 1.0000 - val_loss: 1.7002 - val_acc: 0.5000\n",
            "Epoch 2408/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3024 - acc: 1.0000 - val_loss: 1.8425 - val_acc: 0.5000\n",
            "Epoch 2409/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3018 - acc: 1.0000 - val_loss: 1.8908 - val_acc: 0.5000\n",
            "Epoch 2410/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3008 - acc: 1.0000 - val_loss: 2.4249 - val_acc: 0.5000\n",
            "Epoch 2411/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3001 - acc: 1.0000 - val_loss: 2.3348 - val_acc: 0.5000\n",
            "Epoch 2412/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2993 - acc: 1.0000 - val_loss: 2.9898 - val_acc: 0.5000\n",
            "Epoch 2413/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3004 - acc: 1.0000 - val_loss: 2.8866 - val_acc: 0.5000\n",
            "Epoch 2414/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2978 - acc: 1.0000 - val_loss: 2.8851 - val_acc: 0.5000\n",
            "Epoch 2415/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2972 - acc: 1.0000 - val_loss: 3.2699 - val_acc: 0.5000\n",
            "Epoch 2416/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2964 - acc: 1.0000 - val_loss: 3.2722 - val_acc: 0.5000\n",
            "Epoch 2417/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2956 - acc: 1.0000 - val_loss: 3.4995 - val_acc: 0.5000\n",
            "Epoch 2418/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2954 - acc: 1.0000 - val_loss: 3.6248 - val_acc: 0.5000\n",
            "Epoch 2419/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3196 - acc: 0.9900 - val_loss: 3.7229 - val_acc: 0.5000\n",
            "Epoch 2420/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2939 - acc: 1.0000 - val_loss: 3.2462 - val_acc: 0.5000\n",
            "Epoch 2421/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2946 - acc: 1.0000 - val_loss: 3.2968 - val_acc: 0.5000\n",
            "Epoch 2422/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2954 - acc: 1.0000 - val_loss: 2.9026 - val_acc: 0.5000\n",
            "Epoch 2423/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2946 - acc: 1.0000 - val_loss: 3.0613 - val_acc: 0.5000\n",
            "Epoch 2424/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2946 - acc: 1.0000 - val_loss: 2.9048 - val_acc: 0.5000\n",
            "Epoch 2425/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2985 - acc: 0.9950 - val_loss: 3.1107 - val_acc: 0.5000\n",
            "Epoch 2426/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3103 - acc: 0.9900 - val_loss: 3.2300 - val_acc: 0.5000\n",
            "Epoch 2427/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2997 - acc: 0.9950 - val_loss: 3.5576 - val_acc: 0.5000\n",
            "Epoch 2428/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3392 - acc: 0.9900 - val_loss: 3.3135 - val_acc: 0.5000\n",
            "Epoch 2429/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2931 - acc: 1.0000 - val_loss: 4.3157 - val_acc: 0.5000\n",
            "Epoch 2430/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3149 - acc: 0.9950 - val_loss: 4.6160 - val_acc: 0.5000\n",
            "Epoch 2431/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3066 - acc: 0.9950 - val_loss: 4.6858 - val_acc: 0.5000\n",
            "Epoch 2432/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2924 - acc: 1.0000 - val_loss: 4.4869 - val_acc: 0.5000\n",
            "Epoch 2433/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2910 - acc: 1.0000 - val_loss: 4.3991 - val_acc: 0.5000\n",
            "Epoch 2434/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2905 - acc: 1.0000 - val_loss: 4.5187 - val_acc: 0.5000\n",
            "Epoch 2435/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2896 - acc: 1.0000 - val_loss: 4.4839 - val_acc: 0.5000\n",
            "Epoch 2436/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2945 - acc: 0.9950 - val_loss: 4.5139 - val_acc: 0.5000\n",
            "Epoch 2437/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3538 - acc: 0.9800 - val_loss: 4.4679 - val_acc: 0.5000\n",
            "Epoch 2438/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3011 - acc: 0.9950 - val_loss: 3.9619 - val_acc: 0.5000\n",
            "Epoch 2439/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3137 - acc: 0.9950 - val_loss: 3.5358 - val_acc: 0.4800\n",
            "Epoch 2440/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2975 - acc: 0.9900 - val_loss: 3.4543 - val_acc: 0.5000\n",
            "Epoch 2441/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2885 - acc: 1.0000 - val_loss: 3.4890 - val_acc: 0.5000\n",
            "Epoch 2442/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2870 - acc: 1.0000 - val_loss: 3.4726 - val_acc: 0.5000\n",
            "Epoch 2443/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2864 - acc: 1.0000 - val_loss: 3.5330 - val_acc: 0.5000\n",
            "Epoch 2444/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2858 - acc: 1.0000 - val_loss: 3.3425 - val_acc: 0.5000\n",
            "Epoch 2445/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2851 - acc: 1.0000 - val_loss: 3.4445 - val_acc: 0.5000\n",
            "Epoch 2446/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2844 - acc: 1.0000 - val_loss: 3.9414 - val_acc: 0.5000\n",
            "Epoch 2447/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2837 - acc: 1.0000 - val_loss: 3.8174 - val_acc: 0.5000\n",
            "Epoch 2448/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3853 - acc: 0.9850 - val_loss: 3.8842 - val_acc: 0.5000\n",
            "Epoch 2449/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2831 - acc: 1.0000 - val_loss: 4.2361 - val_acc: 0.5000\n",
            "Epoch 2450/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2856 - acc: 1.0000 - val_loss: 4.0425 - val_acc: 0.5000\n",
            "Epoch 2451/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2907 - acc: 0.9950 - val_loss: 4.4021 - val_acc: 0.5000\n",
            "Epoch 2452/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2860 - acc: 1.0000 - val_loss: 4.9010 - val_acc: 0.5000\n",
            "Epoch 2453/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2859 - acc: 1.0000 - val_loss: 4.3704 - val_acc: 0.5000\n",
            "Epoch 2454/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2873 - acc: 1.0000 - val_loss: 4.7946 - val_acc: 0.5000\n",
            "Epoch 2455/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2849 - acc: 1.0000 - val_loss: 4.6656 - val_acc: 0.5000\n",
            "Epoch 2456/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2848 - acc: 1.0000 - val_loss: 5.1011 - val_acc: 0.5000\n",
            "Epoch 2457/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2833 - acc: 1.0000 - val_loss: 4.8575 - val_acc: 0.5000\n",
            "Epoch 2458/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2833 - acc: 1.0000 - val_loss: 4.9362 - val_acc: 0.5000\n",
            "Epoch 2459/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2820 - acc: 1.0000 - val_loss: 5.1890 - val_acc: 0.5000\n",
            "Epoch 2460/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2815 - acc: 1.0000 - val_loss: 5.4253 - val_acc: 0.5000\n",
            "Epoch 2461/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2805 - acc: 1.0000 - val_loss: 5.2019 - val_acc: 0.5000\n",
            "Epoch 2462/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2821 - acc: 1.0000 - val_loss: 5.1428 - val_acc: 0.5000\n",
            "Epoch 2463/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2791 - acc: 1.0000 - val_loss: 5.3438 - val_acc: 0.5000\n",
            "Epoch 2464/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2785 - acc: 1.0000 - val_loss: 5.6733 - val_acc: 0.5000\n",
            "Epoch 2465/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2816 - acc: 1.0000 - val_loss: 5.4181 - val_acc: 0.5000\n",
            "Epoch 2466/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2779 - acc: 1.0000 - val_loss: 5.4638 - val_acc: 0.5000\n",
            "Epoch 2467/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2768 - acc: 1.0000 - val_loss: 5.3547 - val_acc: 0.5000\n",
            "Epoch 2468/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2760 - acc: 1.0000 - val_loss: 5.2184 - val_acc: 0.5000\n",
            "Epoch 2469/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2755 - acc: 1.0000 - val_loss: 5.0918 - val_acc: 0.5000\n",
            "Epoch 2470/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2749 - acc: 1.0000 - val_loss: 5.0332 - val_acc: 0.5000\n",
            "Epoch 2471/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2742 - acc: 1.0000 - val_loss: 5.0570 - val_acc: 0.5000\n",
            "Epoch 2472/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2734 - acc: 1.0000 - val_loss: 5.1443 - val_acc: 0.5000\n",
            "Epoch 2473/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2729 - acc: 1.0000 - val_loss: 4.7518 - val_acc: 0.5000\n",
            "Epoch 2474/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2722 - acc: 1.0000 - val_loss: 5.0576 - val_acc: 0.5000\n",
            "Epoch 2475/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2718 - acc: 1.0000 - val_loss: 5.1714 - val_acc: 0.5000\n",
            "Epoch 2476/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2708 - acc: 1.0000 - val_loss: 4.7552 - val_acc: 0.5000\n",
            "Epoch 2477/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2701 - acc: 1.0000 - val_loss: 4.9834 - val_acc: 0.5000\n",
            "Epoch 2478/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2696 - acc: 1.0000 - val_loss: 4.8814 - val_acc: 0.5000\n",
            "Epoch 2479/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2698 - acc: 1.0000 - val_loss: 4.8851 - val_acc: 0.5000\n",
            "Epoch 2480/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2681 - acc: 1.0000 - val_loss: 4.8851 - val_acc: 0.5000\n",
            "Epoch 2481/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2675 - acc: 1.0000 - val_loss: 4.8862 - val_acc: 0.5000\n",
            "Epoch 2482/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2669 - acc: 1.0000 - val_loss: 4.9319 - val_acc: 0.5000\n",
            "Epoch 2483/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2677 - acc: 1.0000 - val_loss: 4.9802 - val_acc: 0.5000\n",
            "Epoch 2484/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2673 - acc: 1.0000 - val_loss: 5.0007 - val_acc: 0.5000\n",
            "Epoch 2485/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2650 - acc: 1.0000 - val_loss: 4.7950 - val_acc: 0.5000\n",
            "Epoch 2486/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2644 - acc: 1.0000 - val_loss: 4.9747 - val_acc: 0.5000\n",
            "Epoch 2487/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2720 - acc: 0.9950 - val_loss: 4.7644 - val_acc: 0.5000\n",
            "Epoch 2488/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2633 - acc: 1.0000 - val_loss: 4.3521 - val_acc: 0.5000\n",
            "Epoch 2489/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2632 - acc: 1.0000 - val_loss: 4.3674 - val_acc: 0.5000\n",
            "Epoch 2490/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2627 - acc: 1.0000 - val_loss: 3.8413 - val_acc: 0.5000\n",
            "Epoch 2491/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2627 - acc: 1.0000 - val_loss: 4.0817 - val_acc: 0.5000\n",
            "Epoch 2492/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2622 - acc: 1.0000 - val_loss: 3.6258 - val_acc: 0.5100\n",
            "Epoch 2493/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2612 - acc: 1.0000 - val_loss: 3.6565 - val_acc: 0.5100\n",
            "Epoch 2494/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2607 - acc: 1.0000 - val_loss: 3.8530 - val_acc: 0.5000\n",
            "Epoch 2495/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2600 - acc: 1.0000 - val_loss: 3.5815 - val_acc: 0.5000\n",
            "Epoch 2496/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2604 - acc: 1.0000 - val_loss: 4.3248 - val_acc: 0.5000\n",
            "Epoch 2497/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2596 - acc: 1.0000 - val_loss: 4.4592 - val_acc: 0.5000\n",
            "Epoch 2498/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2582 - acc: 1.0000 - val_loss: 3.6288 - val_acc: 0.5000\n",
            "Epoch 2499/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2590 - acc: 1.0000 - val_loss: 4.2032 - val_acc: 0.5000\n",
            "Epoch 2500/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2570 - acc: 1.0000 - val_loss: 4.4262 - val_acc: 0.5000\n",
            "Epoch 2501/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2565 - acc: 1.0000 - val_loss: 4.1006 - val_acc: 0.5000\n",
            "Epoch 2502/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2559 - acc: 1.0000 - val_loss: 4.0433 - val_acc: 0.5000\n",
            "Epoch 2503/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2553 - acc: 1.0000 - val_loss: 3.5440 - val_acc: 0.5000\n",
            "Epoch 2504/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2547 - acc: 1.0000 - val_loss: 4.0919 - val_acc: 0.5000\n",
            "Epoch 2505/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2542 - acc: 1.0000 - val_loss: 4.0488 - val_acc: 0.5000\n",
            "Epoch 2506/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2536 - acc: 1.0000 - val_loss: 3.7859 - val_acc: 0.5000\n",
            "Epoch 2507/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2535 - acc: 1.0000 - val_loss: 3.7040 - val_acc: 0.5000\n",
            "Epoch 2508/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2524 - acc: 1.0000 - val_loss: 4.1937 - val_acc: 0.5000\n",
            "Epoch 2509/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2519 - acc: 1.0000 - val_loss: 3.8953 - val_acc: 0.5000\n",
            "Epoch 2510/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2513 - acc: 1.0000 - val_loss: 4.1658 - val_acc: 0.5000\n",
            "Epoch 2511/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2512 - acc: 1.0000 - val_loss: 3.4256 - val_acc: 0.5000\n",
            "Epoch 2512/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2506 - acc: 1.0000 - val_loss: 3.8670 - val_acc: 0.5000\n",
            "Epoch 2513/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2497 - acc: 1.0000 - val_loss: 3.4941 - val_acc: 0.5000\n",
            "Epoch 2514/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2495 - acc: 1.0000 - val_loss: 4.0055 - val_acc: 0.5000\n",
            "Epoch 2515/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2486 - acc: 1.0000 - val_loss: 3.8818 - val_acc: 0.5000\n",
            "Epoch 2516/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2481 - acc: 1.0000 - val_loss: 3.6635 - val_acc: 0.5000\n",
            "Epoch 2517/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2475 - acc: 1.0000 - val_loss: 3.4408 - val_acc: 0.5000\n",
            "Epoch 2518/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2470 - acc: 1.0000 - val_loss: 3.0659 - val_acc: 0.5000\n",
            "Epoch 2519/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2648 - acc: 0.9950 - val_loss: 4.2727 - val_acc: 0.5000\n",
            "Epoch 2520/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2462 - acc: 1.0000 - val_loss: 4.9942 - val_acc: 0.5000\n",
            "Epoch 2521/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2728 - acc: 0.9950 - val_loss: 4.8958 - val_acc: 0.5000\n",
            "Epoch 2522/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2477 - acc: 1.0000 - val_loss: 5.9971 - val_acc: 0.5000\n",
            "Epoch 2523/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3003 - acc: 0.9900 - val_loss: 5.1394 - val_acc: 0.5000\n",
            "Epoch 2524/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2495 - acc: 1.0000 - val_loss: 4.9152 - val_acc: 0.5000\n",
            "Epoch 2525/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2503 - acc: 1.0000 - val_loss: 5.3342 - val_acc: 0.5000\n",
            "Epoch 2526/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2504 - acc: 1.0000 - val_loss: 5.3197 - val_acc: 0.5000\n",
            "Epoch 2527/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2502 - acc: 1.0000 - val_loss: 4.8727 - val_acc: 0.5000\n",
            "Epoch 2528/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2528 - acc: 1.0000 - val_loss: 4.8479 - val_acc: 0.5000\n",
            "Epoch 2529/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2495 - acc: 1.0000 - val_loss: 4.5665 - val_acc: 0.5000\n",
            "Epoch 2530/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2488 - acc: 1.0000 - val_loss: 4.6231 - val_acc: 0.5000\n",
            "Epoch 2531/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2483 - acc: 1.0000 - val_loss: 4.2584 - val_acc: 0.5000\n",
            "Epoch 2532/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2476 - acc: 1.0000 - val_loss: 4.2797 - val_acc: 0.5000\n",
            "Epoch 2533/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2470 - acc: 1.0000 - val_loss: 3.9678 - val_acc: 0.5000\n",
            "Epoch 2534/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2467 - acc: 1.0000 - val_loss: 4.1816 - val_acc: 0.5000\n",
            "Epoch 2535/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2460 - acc: 1.0000 - val_loss: 3.9481 - val_acc: 0.5000\n",
            "Epoch 2536/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2453 - acc: 1.0000 - val_loss: 3.6801 - val_acc: 0.5000\n",
            "Epoch 2537/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2447 - acc: 1.0000 - val_loss: 3.7735 - val_acc: 0.5000\n",
            "Epoch 2538/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2915 - acc: 0.9900 - val_loss: 3.7421 - val_acc: 0.5000\n",
            "Epoch 2539/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2444 - acc: 1.0000 - val_loss: 3.1268 - val_acc: 0.5000\n",
            "Epoch 2540/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2447 - acc: 1.0000 - val_loss: 1.6552 - val_acc: 0.7300\n",
            "Epoch 2541/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2451 - acc: 1.0000 - val_loss: 1.4088 - val_acc: 0.7300\n",
            "Epoch 2542/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2641 - acc: 0.9950 - val_loss: 3.0644 - val_acc: 0.5000\n",
            "Epoch 2543/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2451 - acc: 1.0000 - val_loss: 3.1996 - val_acc: 0.5000\n",
            "Epoch 2544/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2447 - acc: 1.0000 - val_loss: 3.0939 - val_acc: 0.5000\n",
            "Epoch 2545/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3268 - acc: 0.9850 - val_loss: 2.7634 - val_acc: 0.5000\n",
            "Epoch 2546/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.3548 - acc: 0.9900 - val_loss: 1.4863 - val_acc: 0.5000\n",
            "Epoch 2547/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.4046 - acc: 0.9850 - val_loss: 1.9530 - val_acc: 0.5000\n",
            "Epoch 2548/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2590 - acc: 1.0000 - val_loss: 3.1278 - val_acc: 0.5000\n",
            "Epoch 2549/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2683 - acc: 1.0000 - val_loss: 3.4181 - val_acc: 0.5000\n",
            "Epoch 2550/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2720 - acc: 1.0000 - val_loss: 3.3920 - val_acc: 0.5000\n",
            "Epoch 2551/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2765 - acc: 1.0000 - val_loss: 3.1402 - val_acc: 0.5000\n",
            "Epoch 2552/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2746 - acc: 1.0000 - val_loss: 3.0279 - val_acc: 0.5000\n",
            "Epoch 2553/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2745 - acc: 1.0000 - val_loss: 3.2258 - val_acc: 0.5000\n",
            "Epoch 2554/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2740 - acc: 1.0000 - val_loss: 2.9934 - val_acc: 0.5000\n",
            "Epoch 2555/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2734 - acc: 1.0000 - val_loss: 2.7877 - val_acc: 0.5000\n",
            "Epoch 2556/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2727 - acc: 1.0000 - val_loss: 3.1038 - val_acc: 0.5000\n",
            "Epoch 2557/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2719 - acc: 1.0000 - val_loss: 2.8565 - val_acc: 0.5000\n",
            "Epoch 2558/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2713 - acc: 1.0000 - val_loss: 2.8782 - val_acc: 0.5000\n",
            "Epoch 2559/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2705 - acc: 1.0000 - val_loss: 2.9987 - val_acc: 0.5000\n",
            "Epoch 2560/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2700 - acc: 1.0000 - val_loss: 2.3635 - val_acc: 0.5000\n",
            "Epoch 2561/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2689 - acc: 1.0000 - val_loss: 2.8096 - val_acc: 0.5000\n",
            "Epoch 2562/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2684 - acc: 1.0000 - val_loss: 2.6722 - val_acc: 0.5000\n",
            "Epoch 2563/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2674 - acc: 1.0000 - val_loss: 2.5260 - val_acc: 0.5000\n",
            "Epoch 2564/3000\n",
            "200/200 [==============================] - 1s 5ms/step - loss: 0.2666 - acc: 1.0000 - val_loss: 2.6785 - val_acc: 0.5000\n",
            "Epoch 2565/3000\n",
            " 64/200 [========>.....................] - ETA: 0s - loss: 0.2661 - acc: 1.0000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-211-d27419bf0708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_r\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nDX-7pYMDEGH"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZIeezj5mDEbw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "stzybbTL-fr0",
        "outputId": "4bd7dcde-2f9b-4382-e02c-8eb9e8a99070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "cell_type": "code",
      "source": [
        "print(d_loss.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(d_loss.history['acc'])\n",
        "plt.plot(d_loss.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(d_loss.history['loss'])\n",
        "plt.plot(d_loss.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA1lAV7a4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4VFX6wPHvm0knIaGGbuhdQBCx\nNyyLXUTsve3qz7q69rXrrn0VC4q9i12xIhYQUUB6C51QQhJICOnl/P64dzIlM8MkZEqS9/M8ee6d\ne8+9czKQ+87pYoxBKaVUyxUT6QwopZSKLA0ESinVwmkgUEqpFk4DgVJKtXAaCJRSqoXTQKCUUi2c\nBgKlAhCR10TkgSDTrheRsXt7H6XCTQOBUkq1cBoIlFKqhdNAoJo8u0rmZhFZJCLFIjJFRDJE5GsR\nKRKRH0SkjVv6k0VkqYgUiMhPIjLQ7dwIEZlvX/c+kOj1XieKyAL72t9EZN8G5vlyEVktIjtE5HMR\n6WIfFxF5UkS2i8guEVksIkPsc+NEZJmdt80i8s8GfWBKedFAoJqL8cAxQD/gJOBr4HagA9b/82sB\nRKQf8C5wvX1uGvCFiMSLSDzwKfAm0Bb40L4v9rUjgFeAK4F2wIvA5yKSUJ+MishRwMPAmUBnYAPw\nnn36WOAw+/dIs9Pk2+emAFcaY1KBIcCP9XlfpfzRQKCai2eMMTnGmM3Ar8AcY8xfxpgy4BNghJ1u\nIvCVMeZ7Y0wl8BiQBBwEjAHigKeMMZXGmKnAn27vcQXwojFmjjGm2hjzOlBuX1cf5wKvGGPmG2PK\ngduAA0UkE6gEUoEBgBhjlhtjttrXVQKDRKS1MWanMWZ+Pd9XKZ80EKjmIsdtv9TH6xR7vwvWN3AA\njDE1wCagq31us/GciXGD2/4+wE12tVCBiBQA3e3r6sM7D7uxvvV3Ncb8CDwLTAK2i8hkEWltJx0P\njAM2iMjPInJgPd9XKZ80EKiWZgvWAx2w6uSxHuabga1AV/uYUw+3/U3Ag8aYdLefZGPMu3uZh1ZY\nVU2bAYwx/zPGjAQGYVUR3Wwf/9MYcwrQEasK64N6vq9SPmkgUC3NB8AJInK0iMQBN2FV7/wGzAaq\ngGtFJE5ETgdGu137EnCViBxgN+q2EpETRCS1nnl4F7hYRIbb7QsPYVVlrReR/e37xwHFQBlQY7dh\nnCsiaXaV1i6gZi8+B6VqaSBQLYoxZiVwHvAMkIfVsHySMabCGFMBnA5cBOzAak/42O3aucDlWFU3\nO4HVdtr65uEH4C7gI6xSSG/gLPt0a6yAsxOr+igfeNQ+dz6wXkR2AVdhtTUotddEF6ZRSqmWTUsE\nSinVwmkgUEqpFk4DgVJKtXAaCJRSqoWLjXQGgtG+fXuTmZkZ6WwopVSTMm/evDxjTIc9pWsSgSAz\nM5O5c+dGOhtKKdWkiMiGPafSqiGllGrxNBAopVQLp4FAKaVauCbRRuBLZWUl2dnZlJWVRTorIZWY\nmEi3bt2Ii4uLdFaUUs1Ukw0E2dnZpKamkpmZiedkkc2HMYb8/Hyys7Pp2bNnpLOjlGqmmmzVUFlZ\nGe3atWu2QQBARGjXrl2zL/UopSKryQYCoFkHAaeW8DsqpSKrSQeCoBkDxXlQUx3pnCilVNRp/oHA\n1MDWBVC4CXZtabTbFhQU8Nxzz9X7unHjxlFQUNBo+VBKqb3VvANBVQVsXeh6XZIH2xZDddVe39pf\nIKiqCnzvadOmkZ6evtfvr5RSjaV5B4LSna79dn1BHFBTBZUle33rW2+9lTVr1jB8+HD2339/Dj30\nUE4++WQGDRoEwKmnnsrIkSMZPHgwkydPrr0uMzOTvLw81q9fz8CBA7n88ssZPHgwxx57LKWlpXud\nL6WUqq8m233U3b1fLGXZll11T1SXQ3UlxLcCFgMGKorBUQSOwP3yB3Vpzb9PGuz3/COPPMKSJUtY\nsGABP/30EyeccAJLliyp7eb5yiuv0LZtW0pLS9l///0ZP3487dq187hHVlYW7777Li+99BJnnnkm\nH330Eeedd159f32llNorzSIQ+CUOcAjg7Hlj79dU7jEQ1Nfo0aM9+vr/73//45NPPgFg06ZNZGVl\n1QkEPXv2ZPjw4QCMHDmS9evXN2qelFIqGM0iEAT65l7Hri2wO8cKEu36QHxyo+ShVatWtfs//fQT\nP/zwA7NnzyY5OZkjjjjC51iAhISE2n2Hw6FVQ0qpiGjWbQTF5VXsKK7wPJiQam1NNeSthIJNVvfS\nekpNTaWoqMjnucLCQtq0aUNycjIrVqzg999/r/f9lVIqXJpFicCfgtJKCkoqaNsq3nUwPgUS06Cs\n0HpdkmdVE6V2qte927Vrx8EHH8yQIUNISkoiIyOj9tzxxx/PCy+8wMCBA+nfvz9jxoxpjF9HKaVC\nQkwDvg2H26hRo4z3wjTLly9n4MCBAa/bVljG9qIyhnZNqztCt6oMCrOhfDdgIGNIo7cbNJZgflel\nlPImIvOMMaP2lK5ZVw05YqyHf3WNj2AXm2i1EbRqb73OWQLF+Q2qJlJKqaasWQeCeIcVCIorAgzy\nSnGrEircCDvWhDhXSikVXZp1IEhNjMMRI+QWlftP5IiF9H1cr8uLrGkplFKqhQhZIBCR7iIyQ0SW\nichSEbnOPn6PiGwWkQX2z7hQ5SEmRkhLjKOkopqCkgr/CZPbQudhroBQFSCtUko1M6HsNVQF3GSM\nmS8iqcA8EfnePvekMeaxEL53rY6tE9lRUsGmnaUkxjlIjHP4TigxEGv36y/Jt9oOYhN8p1VKqWYk\nZCUCY8xWY8x8e78IWA50DdX7+RMfG0O/jFQcIqzN3U15ZYCpqOOSrG3xdti+TKetVkq1CGFpIxCR\nTGAEMMc+dI2ILBKRV0SkjZ9rrhCRuSIyNzc3d6/ePzHOQY92ydQYWJdXTHG5n8ZjiYG07q7XxXv3\nvu5SUlIa7V5KKdWYQh4IRCQF+Ai43hizC3ge6A0MB7YCj/u6zhgz2RgzyhgzqkOHDnudj5SEWDLb\nW9NArM0tptBfm0Gr9tB5uDUFRbW2FSilmr+QjiwWkTisIPC2MeZjAGNMjtv5l4AvQ5kHdykJsfTp\nmMK6vGKyC6w2gwRfbQYi1uCyknyrhOBjuchbb72V7t27c/XVVwNwzz33EBsby4wZM9i5cyeVlZU8\n8MADnHLKKaH+tZRSaq+ELBCINZR3CrDcGPOE2/HOxpit9svTgCV7/WZf32otOBOEWKC3MZRWVlMF\nxMY5cPhaFzi9BxxwhbW6Wbs+rjmKbBMnTuT666+vDQQffPAB3377Lddeey2tW7cmLy+PMWPGcPLJ\nJ+u6w0qpqBbKEsHBwPnAYhFZYB+7HThbRIYDBlgPXBnCPPgUI0JSnIPSymrKK6tJincgeD2sE90e\n/Ltz6gSCESNGsH37drZs2UJubi5t2rShU6dO3HDDDfzyyy/ExMSwefNmcnJy6NSpfvMYKaVUOIUs\nEBhjZoL30xWAaY3+Zn97pN6XxACUV7Emr5ikOAe92rciJsYruxUlkL8aKkutQWbi2aQyYcIEpk6d\nyrZt25g4cSJvv/02ubm5zJs3j7i4ODIzM31OP62UUtGkWY8s3pNWCbH0aJtESUUVm3aWUGcCvvhk\nq4qopspa+3j7MmvfNnHiRN577z2mTp3KhAkTKCwspGPHjsTFxTFjxgw2bNgQ5t9IKaXqr0UHAoC0\npHg6pyVRWFrJtl0+vr0nuS00X1VutUXYI48HDx5MUVERXbt2pXPnzpx77rnMnTuXoUOH8sYbbzBg\nwIAw/RZKKdVwzXo9gmC1T4mnoqqG3KJyHCJ0bJ3omaBtLysI7NpsvS7cBO16A7B4sauRun379sye\nPdvne+zevTskeVdKqb2lgQAQEbqkJ1JVU0POrnKS4x2kJLqtTZCYZm2T21ormpXv8tlmoJRSTZE+\nyWwiQrc2SSTExbBhRwkVVT6ml4iJtaqKTI3VgFwVYFZTpZRqIpp0IGjs1dUcMTHs085azH59Xgk1\nvha0ibMXu89bZTUeO5e8DJGmsIKcUqppa7KBIDExkfz8/EZ/UCbEOujRNpmyqmq2FJTWTRCbYJUM\nnHZuCNmqZsYY8vPzSUxM3HNipZRqoCbbRtCtWzeys7PZ2wnp/CktrWR5WRXbkuNoleD1MZkYKCu1\nFrEB2FJktR+EQGJiIt26dQvJvZVSCppwIIiLi6Nnz54hu391jeGiV/9gzrocPvnHQQzuklY30daF\n8OJh1v5hN8NRd4YsP0opFSpNtmoo1BwxwlMTh9MmOY5r3vmL3b6mru48DA79p7X/y6OQr+sdK6Wa\nHg0EAbRLSeB/Z41gQ34xd3yy2Hd7xFF3QsfB1v78N8KbQaWUagQaCPbggF7tuPGYfny2YAvv/7mp\nbgIR+Mdv0HUkzHoK1s8MfyaVUmovaCAIwt+P6MMhfdpz9+dLWZRd4DtR/3HW9rUTdIlLpVSTooEg\nCI4Y4Ykzh5GeFMctUxdRUVVTN9EhN8BB11r7O9aFN4PBqKmG3FWRzoVSKgppIAhSx9aJPHTaUFZs\nK+Lp6T4eqDEO6Hm4tV+cCznLQja+oEFmPAST9oe8rEjnRCkVZTQQ1MPYQRlMGNmN539aw7wNO+om\nSG5jbb/5Fzx/INybDht8T0IXdht/t7ZFWwOnU0q1OBoI6unukwbRJT2JGz9YSLF3l9J2fazt1oWu\nY1nfhS9zgTiXy4ymUopSKipoIKin1MQ4HpswjI07Snho2nLPk4lp0GGg57EYR/gyF0htIPDRvqGU\natE0EDTAmF7tuOyQnrw9ZyMzVm73PHn5dLh8Bty5HZLbwe7tvm8SbtWV1lYDgVLKiwaCBrrp2P70\nz0jllqmL2Flc4ToR3wq67mdNTtdxEMx/PTqqYzbabRXRkBelVFTRQNBAiXEOnpg4jIKSCu78dInv\nUced9rW2634Ob+a8eeRNA4FSypMGgr0wuEsa14/tx1eLt/LFIh+9cQ6+ztq+cQpkz4Xy3ZFZzMa9\nOkirhpRSXjQQ7KWrDu/NsO7p3Pv5Una4VxEBtOrg2n/5aHi4K7w1PrwZBM+Rzlo1pJTyooFgLzli\nhP+MH0phaSX3f7nM82SMj493/a/hfxgb90CgJQKllCcNBI1gQKfW/OPIPnzy1+a6vYgmvA7HPQwn\nPA7xqdax3JXhzWCNBgKllH8aCBrJ1Uf2pm/HFG6Zuoi83W7tAINPhQP/AftfBqc+Zx177gDYPD98\nmdMSgVIqAA0EjSQh1sFTZw1nZ3FF3YFmTs4ZSgH+mByejAFUFLv2jc6MqpTypIGgEQ3uksY/jujN\nx/M3M315Tt0Ejli4Zq61v3hq+DL2hNto52ofK60ppVq0kAUCEekuIjNEZJmILBWR6+zjbUXkexHJ\nsrdtQpWHSLjmqL707ZjCXZ8uqTsXEUD7voBATSUs/zLs+aNGA4FSylMoSwRVwE3GmEHAGOBqERkE\n3ApMN8b0Babbr5uN+NgYHhk/lC2FZTz2nZ9G4fEvW9v3zw1/D6KayvC+n1Iq6oUsEBhjthpj5tv7\nRcByoCtwCvC6nex14NRQ5SFSRu7TlvPG9OC139bz18addRMMONG1/82t8Ndb4cuclgiUUl7C0kYg\nIpnACGAOkGGMcQ7D3QZk+LnmChGZKyJzc3Nzw5HNRnXL8QPISE3kto8X113RLC4RTn3B2p/zAnx2\ndfgypm0ESikvIQ8EIpICfARcb4zZ5X7OWBP0+KwbMcZMNsaMMsaM6tChg68kUa11Yhz3nzqEFduK\nmPzLmroJhniNMA7XOsdaIlBKeQlpIBCROKwg8LYx5mP7cI6IdLbPdwaiZJ7mxnfMoAxOGNqZ/01f\nzZrc3Z4nY+Ph5rXW+AKwlrcMB20jUEp5CWWvIQGmAMuNMU+4nfocuNDevxD4LFR5iAb/PnkQiXEx\n3PbxYmpqvAo/rdpBxhBr//H+8Nk1oc+QlgiUUl5CWSI4GDgfOEpEFtg/44BHgGNEJAsYa79utjqm\nJnLnCYP4Y90O3vtzU90EzkAA8NebUOqjcbkxaRuBUspLKHsNzTTGiDFmX2PMcPtnmjEm3xhztDGm\nrzFmrDHGxyrwzcuEUd04qHc7Hp62nJxdZZ4nu+8PR9/ter1zfWgzoyUCpZQXHVkcBiLCQ6cNpaK6\nhrt8LWLTZT/XfqiXttQ2AqWUFw0EYZLZvhU3HNOP75bl8PWSbZ4nex4OB1xl7Rdtq3txY4lPgaqK\nPadTSrUoGgjC6LJDejKka2vu/mwpBSVuD+SYGDjmPmv/i2th9qTQlAwqdsPvkxr/vkqpJk0DQRjF\nOmL473hrneMHvvKaoTQ2wbX/7e3w6rjGm36idTcYfl7j3Esp1exoIAizQV1ac9XhvZk6L5tfVnmN\nHTjuYdd+fhbcm2718vnmNpizF9NWmxqQhl+ulGreNBBEwDVH9aF3h1bc9vFizxlKh50FKV4zbuze\nBr8/B1/fvBfvaEDc/ql13WKllBsNBBGQGOfgP+P3ZUthKY9+6zZDaXJb+Ocqz8RPDt77NzQ1noGg\nWnsOKaVcNBBEyKjMtlx4YCavz17PvA0hHkphagCB/S6wXldrzyGllIsGggi6+bj+dElL4papiyir\ndJt0buy9MHRC3QuqKuDHB6BsV91zgRi7asg5qvi3ZxqeaaVUs6OBIIJaJcTy0OlDWZNbzKQZq10n\nDrneWrxmxPmeFzzQAX55FH59zPP4zKdg/Sz/b+SsGtphz4K68bfG+QWUUs2CBoIIO7xfB8bv143n\nf1rDsi1e3/RPfgZuWVf3ollPu/aryuGHf8Nr4/y/iakBEdf0EuLY+4wrpZoNDQRR4K4TB9KmVTzX\nv/8XldVui9iIWA3Ixz7o/+JdW4J4B7tqKKWT9XLtjL3Kr1KqedFAEAXSk+N5+LShrMrZzXMzfCxi\nM8LHYLB1v1rbylJr6+9bftb3UFYICJzwmO80SqkWTQNBlBg7KIOTh3XhmR+zWLHNq4ooKR0u/d7z\n2Ov2usdVzkDg55/y7TNc55PaNl6GlVLNhgaCKHLPyYNJS4rjlqmLqKr2Wue4+2i4+Bu4wG0dn5oa\nqLSntY7ZQ72/CDjiGzfDSqlmQQNBFGnbKp57TxnMouxCz4FmTvscCN3HuF7PeGDPJQInEWtyO6WU\n8qJPhihz4r5dOOeAHkz+dS1/rPMx0CwuEc5619r/9XGY4ZyfyJ5MaPtyqyvpjw94TiWxp0ChlGqx\n9OkQhe4YN5BubZK49SOvgWZOA8ZBn2Os/c1zrW18MpQXwXNjrK6kvzwKZQWua5yBYOTFkNw+tL+A\nUqpJ0UAQhVolxPLwafuyNq+Yx3xVEUHdnkSOeCjY6HmsprrufmyiNfZAKaVsGgii1CF923P+mH2Y\nMmsds9fk100w6BS45Du4daM1HcWuzbB4qmeait2u/Sq7UTk1AyqKoLQApZQCDQRR7bZxA8hs14qz\nX/qdVTlFnidFoMcBkJgG7ftbx2Y+4ZnGfdlL53iDtr2t7c71IcmzUqrp0UAQxZLjY3n8zGEAXPnm\nPCqqanwnHH257+O/ugUGZ4nAud5BcW7d9EqpFkkDQZTbr0cb7jlpEOvyinnsOz/tBUnpvuckyvrW\nte+cZyilg7Vd+1Oj5lMp1XRpIGgCLjq4J6fv15VXZq5jyeZC34mS28LdAdY1cNhrIrfqaG1nP9u4\nmVRKNVkaCJqIu08cRPuUBK597y9KK3x0KQVrdPE5H8KAE+uec8RZ24SU0GVSKdUkaSBoItKT43n8\nzGGszS3mga+W+U/Y71jPNoMR50P/cXDEba5jHQdDUpvQZVYp1aRoIGhCDu7TnisO68Xbczby/bIc\n/wkT09320+DsdyGtq+tYt5FQURK6jCqlmhQNBE3MTcf2Y1Dn1vzro0Vs31XmO1GbTNf+sLPqnl/0\nIVSXQ76PKa+VUi2OBoImJiHWwf/OHkFJRRU3fbiQmhpTN1FSOly3CO7MhU5D654ffo619TeWoLrK\nmtlUKdUihCwQiMgrIrJdRJa4HbtHRDaLyAL7J8D6isqfPh1TuOvEQfyalccrs3x0GwVosw/E+pl2\n+sCrre3u7b7P398eJh+29xlVSjUJoSwRvAYc7+P4k8aY4fbPtBC+f7N2zugeHDMog/9+s5KlW/x0\nKfXHOahs9zY/CQxsWwyb5+9VHgOqLIOqitDdXykVtJAFAmPML0CAju1qb4gI/xm/L+nJcVz7boAu\npb44u5D+cA/sDjDCOJRrGz+YAS8eGrr7K6WCFok2gmtEZJFddeS3D6OIXCEic0Vkbm6uTofgS9tW\n8Txx5nDW5Bbz0LTlDbvJ+z7WQ3ZyDj4LldwVob2/Uioo4Q4EzwO9geHAVuBxfwmNMZONMaOMMaM6\ndOgQrvw1OYf0bc/lh/bkzd83MH15gC6l/mz63f+01M5BaEqpZi2sgcAYk2OMqTbG1AAvAaPD+f7N\n1T+P68/Azq25ZeoicouCXGvgmrlw+L+s/e1upQn3Vc3KvWY8VUo1S0EFAhG5TkRai2WKiMwXkWPr\n+2Yi0tnt5WnAEn9pVfASYh08fdZwdpdXceMHCzDGR5dSb+37Qg97/WPnFNXgmqUUYNo/4eMrGzez\nSqmoE2yJ4BJjzC7gWKANcD7wSKALRORdYDbQX0SyReRS4L8islhEFgFHAjc0POvKXb+MVG4fN5Bf\ns/K449Mg46tzIrpqt1KEe1AAWPRe42RQKRW1YoNMZ6+MzjjgTWPMUhGRQBcYY872cXhKfTKn6ueC\nA/dhYXYB78zZyGF9O3D8kE6BL4hNtLbubQRVfkYrK6WarWBLBPNE5DusQPCtiKQCOvQ0yogI958y\nhEGdW3PjBwvqrmrmzTngrKocFr4H62fVLREopZq9YAPBpcCtwP7GmBIgDrg4ZLlSDdYqIZZnzxmB\nI0a47PW5lFUGGF/gLBFUV8AnV8Jr43wHgnvS4M3T/d+nrBA+uBCKfaytrJSKesEGggOBlcaYAhE5\nD7gTqOdwVhUuvTqk8L+zRrBxRwm3f7LYf+Oxwy4RfHSp61hFse+0a6ZbcxD5Mu81WPYpzHqywXlW\nSkVOsIHgeaBERIYBNwFrgDdCliu1144c0JEbxvbj4/mbeeqHLHYW+5jOIS657rHPrvZ/022LAr9p\nML2VlFJRJ9hAUGWsr5WnAM8aYyYBqaHLlmoM1x7dh5OGdeHp6Vns98D3dUsGvhanyc+ytrdurHvO\n32yl7v6cAm+fWe+8KqUiJ9hAUCQit2F1G/1KRGKw2glUFBMR/jt+X7qkJWIMnDdlDtXu01Y7AnQa\nS0zzfJ3cHpZ95u+dXLtf3QhZ3zY4z0qp8As2EEwEyrHGE2wDugGPhixXqtEkxTv48Z9H0C8jhVmr\n83ntt/W+E96xDToMsPZT7XF/p022tgNOhC7DocCrlJCzzJq4Dq0SUqopCyoQ2A//t4E0ETkRKDPG\naBtBE5EY5+DL/zuUzHbJ/OebFWS5dyu9aSVctxDikmDoGdYx56pmzhJDjAPiW0HFbtj0J0y/z+py\nOuUYmPkklO2q+6aB2gvcF73ZukjbFpSKsGCnmDgT+AOYAJwJzBGRM0KZMdW44mNjePnCUcQIXPPO\nX5RU2D2AUjt5Lm3pzvmAdsRDfArkrYIpY+HXx2HRB1ZgANcgNPcHeo2fHkYAxq1L64uHwl9vNeh3\nUko1jmCrhu7AGkNwoTHmAqzJ4u4KXbZUKPTpmMoL540ka3sR1777l2d7AcDAk63tkPHWtv84GDoB\nxt5bt4fR59e49n2NPfA3oylAjdfYhq0Lg/sFlFIhEWwgiDHGuK9rmF+Pa1UUOaJ/R+45eTA/LN/O\nfV8s9exJ1KE/3FPoWuc4PhnGvwxpXeu2D7hzzlVU6rYOUaBAYKoDv1ZKhVWwD/NvRORbEblIRC4C\nvgJ0mckm6oIDM7nskJ68PnsDPW+b5tlm4M+Yq/yfK7EDwMJ3XceqA5UIvKqNdMlKpSIq2Mbim4HJ\nwL72z2RjzL9CmTEVWrePG8iw7ukAPPx1ECuF9T7KKi2kda97zlfVTqASgfeDP77Vnt9fKRUyQVfv\nGGM+MsbcaP98EspMqdCLiRE+u/pgjhrQkZ9X5fLzqiCXA71hCfz9N89juzbXTRcoEHiXFkp1aWul\nIilgIBCRIhHZ5eOnSER89BlUTc3TZw2nX0Yq/3hrHku3BDl9VMZgq3Rw907of4LvNIGqhpxB4tQX\noMsIKN1Zv0wrpRpVwEBgjEk1xrT28ZNqjGkdrkyq0ElNjOO1i/cnLSmOi1/9k80F9ZiGOibGf9fT\njy6H1dN9n6uutLax8ZCYbs1eqpSKGO35o8honcirF4+muLyKgx/5kXNf/p05a4OcUjrdrc1g2DnQ\n6whrP28lvHeO72ucpQVHAiSlQ2lBQ7OulGoEGggUAP07pfL8eSMBmLU6n4mTfw/uQueYg+4HwGnP\nw5F3us4lt/czxsBuLI5NsOY0KtNAoFQkaSBQtQ7r14Gvrzu09vXWwiCqiVI6wq2b4ILPrdfOVc8A\ndmXDg508p5QAayEcsEYsO6uGdJoJpSJGA4HyMLBza76/4TBSE2I556U5bN8VxBrGia0hzl7tzJFQ\n9/zOdVDpdp/aqqF4q2qoukKXyFQqgjQQqDr6ZqTy2iX7sz6/mNEPTWfTjpLgL3b4mJ38mf3gwQzX\n6me1VUPxrplO81fvXaaVUg2mgUD5NHKftrxotxlc957bJHV7EuujROD0UBdYPNWtaijBNZ3Fny/v\nRW6VUntDA4Hy69jBnXj+3P1YsKmAy9+YS1llEHMC+aoacvfRpa5A4GwsBpj/+t5lVu2dohz46ibY\ntSXSOVERoIFABfS3oZ357xnDmLU6n7+/NY/yqj0EA++qofFT6qapLHGlTdDhKFFh8QdWqWx+lC4z\nsuZHuCcNCjZFOifNkgYCtUdnjOzGQ6cNZcbKXP7vnb+orK7xn9i7aqjTULhlneex2c9ZW0eCtc6B\nijznmhKBpgaJJOeaFRuD7Nas6kUDgQrKOQf04N8nDeK7ZTlc//4CqvwFA0e85+u4ZEhuCzetch3L\nXW5tYxOs0cm9j4ZO+4Ym4yo4zt67IgGTRUxskrWtCqIXm6o3DQQqaBcf3JPb/jaArxZt5baPF3uu\nZeAU4/B87VzQJjUDLvKaudzNSRE/AAAgAElEQVRZeohPdk07oSKkNhJENBd+Of+vaCAICQ0Eql6u\nPLw31x7dlw/nZXPvF8t8B4PzP3Xtx7utbJZ5MHS1eiIR18oVJBwJ+gceac5/x2gtEcRpiSCUYkN1\nYxF5BTgR2G6MGWIfawu8D2QC64EzjTE69WQTc8PYvhSXVzFl5jrKKqt58LShOGLcHiC9j3TtxyZ6\nXuxeQnA+dGITXT2JVIQ0kRJBpQaCUAhlieA14HivY7cC040xfYHp9mvVxIgId54wkGuO7MN7f27i\nxg8CtBl4f8N0LkKT0sl1LDY+ehspVXSIsXuj1WgVYiiErERgjPlFRDK9Dp8CHGHvvw78BOhKZ02Q\niPDP4/qTFO/g0W9XUlJRzTNnjyAxzhH4wpSO1jYp3XXMkaCBINKivWrImS+dkyokQhYI/Mgwxmy1\n97cBGf4SisgVwBUAPXr0CEPWVENcfWQfkuIc3PflMm76YCHPnjMCEYGDrvVd3XPw9dbSlof903Us\nNiHwQjYqDJyBIFqbDZ0BSgNBKIQ7ENQyxhgR8fuvaoyZjLVOMqNGjdJ//Sh2ySE9ydlVxou/rGXB\nfwqYdu2hpB17v+/E7XrDlb94HotNsIKGMXW/kS79xFov2TkCWYWGcVbtRXmJQIVEuMN/joh0BrC3\n28P8/ipEbv3bAE4Z3oXNBaWc+OyvFJXVoy7XOfbAuwQx73X48CL49B8Ny1TuKms0avbchl3fkkR7\n1RBaNRRK4Q4EnwMX2vsXAp+F+f1ViIgIT581gn+fNIhNO0qZ8MJsZmblBXexs2eRe9fA3JXwxbXW\n/o61DcvUZjsAzJ7kP40xsHNDw+7frER5r6HabGkgCIWQBQIReReYDfQXkWwRuRR4BDhGRLKAsfZr\n1YxcfHBPHp8wjBXbijhvypzglrysHSzkViLY7VZYrAlisjtfElKtbaCuqXOnwNP7wuZ5DXuP5qK2\nRBDZbPinJYJQClkgMMacbYzpbIyJM8Z0M8ZMMcbkG2OONsb0NcaMNcbsCNX7q8gZP7Ib0286HIC7\nPltCYckeqomcXUof62NV5dyTBjVu017XBDkFdh1BPDw2/WFtc1f5T9OiRGkkkACNxV/eAC8eFtbs\nNDfR2kVANXG9O6Tw0gWjWJ9XwsTJs8ktCtArqHXXusfePLXxMmMCTJJX2z+9ocGmuYj2b9oBgvrc\nV6yeaKU6NrWhNBCokDlmUAZTLhrFhvwSJr44my0Ffpaj7DIi8I0CLXYTiLGrlAIGAnvcQ0sfqBTt\njcW14wgC/FvqqOMG00CgQurQvh1489LR5BaVM+GF2azPK66bKLE13L0DblgKA0+ue76hgaAmiEDg\nXD+hWksEligNBMGMI9CxKA2mgUCF3KjMtrxz+RhKKqo488XZrMopqpsoxgFp3WDim3BPIZz7keuc\n93xFwaptZA7w8Iixh9K09KohE+UDyoIZWVyl81U1VJT+q6vmZmi3NN6/8kAAznxxNo98vYKdxQH+\ncPuOde039CEdVNWQMxC09Koh+zOK1qqhYEoqWiJoMA0EKmz6ZaQy9aqDMAZe+HkNI+7/nuqaAN/w\nJtjrGFe4VSdlz4Pv/w01AR7uTs4AEuhbpJYIvERpINASQUhpIFBh1aNdMt9e7+rqd8cni/0vfTn4\nVBh1CezOsV4v+wxePgpmPQWrvt7zm2kbQf1FbYnASdsIQkEDgQq7TmmJrHloHN3bJvHen5voe8fX\n/LIq13fi1C5Qkm/NTvrBBa7j752z5zcKqmpIu48C0T9Qy/lvGCifuqZFg2kgUBHhiBF+/ueRjB1o\nTUt9wSt/sMNXm0GqvW5B0bb6v4mzRBBoimvtPmqL8l5DtQFAq4ZCQQOBipiYGOHlC/fn6bOGA3Dx\nq3+Qt9vroZ3a2doWbfW6OC7wzbcthmn2VNeBljd0Vg3t2uo/TUsQzIM2oux8BSwRaNVQQ2kgUBF3\nyvCuvHzBKFbmFHHqpFlkuXcvbW0HgplPuo71GeuasRTgj5esRmSne9LghUNcrwMFAmd3yUXvQUVJ\nw3+Jpm7xB9Y2WquIaqv3ApUINBA0lAYCFRXGDsrg/SsOpLyqhtOf+41fs+w2A2eJYNU31vaKn6Hz\ncKgqtR5aa360vvm/fJT/mwd6wLu3H+Su2LtfoikrcU4OGK2BIJgSgVYNNZQGAhU1hnVP59OrD6Zr\nmyTOn/IHmbd+xcpCt7WTehwInYdBXKL1AP/kKnjzNNd5Y6DaR11/6Q7/M5i6Hy8r9DxXVQ7f3w3l\nPgbAqfCqDQQ+Gv7FbufREkGDaSBQUaVrehIfXnUgPdomA3Dc07+yJibTqsK54HOre6NzpPGi9zwv\n3rqg7sP8lElQWQJPDrbWHago9nz4uz9YZjzkee2812HW0/Dr443zyzUF0Vo1FKik4m9hIxU0DQQq\n6qQmxvHLLUdyWL8OAIwruYe5F2RBrP0Hn5juecGAE63t5CPg0d6u4yc+CRlDrP2irda6Aw91sdI5\nuQeC7D887+tsW/BVymi2ojQQBBr57Gzw1xJBg2kgUFHrjUtG8/IFoygnnive+oszX5zNB3M3wdAz\nPBMOO8v3DUZdAh0G1D2+bRH8ZK+J5HzApHWHlE6e6ZzjEKJ1/p1QiNYSQTCjw7VE0GAt6H+4aorG\nDsrg+xsOI94Rwx/rdnDL1EWUEQ93bodDb4ILPoO+x1n7TofcANctsvbjEqHvsXVv/NPDMGmMtQWr\nJ5Lxakeo/Rbakv5MojUQBBhQFmjRmkirqYbXT4K1P0U6JwG1pP/hqonqm5HKl9cewqF92wMw5uHp\nvPnnVjj6buh1hFVldPTdcMl3cMRtcNjN0GYf1w3Ofh/2v8zaT+/hOp673NpKjLVKmnfvIud8Rs5B\nZy1BtJYICNBYHM3LWJbuhHW/wNRLIp2TgDQQqCahfUoCb156APefOoSCkkru+mwpny3Y7JmoxwFw\nxK2upS+dYmKgldXeQL+/waH/9DxvaiAuGSqLPSezK9xkbbVEEHnBDHiLxkDQRLSk/+GqGTh/zD48\nPmEYANe9t4B/TV3E9qIgVqZyBgcROPouuHG561xMLMRbvZSoKoXCbHhrPMy3Zz+N1mkXQiFan6XB\nVA0VbgxffpoZDQSqyRk/sht/3jGWUfu04f25mxj94HQKS/fUs8er+qB1F/jXBhh8Oly3EOLsQDHt\nZqur6eofXJd6lzCataiNBF5bH/56Kyw5aY40EKgmqUNqAu9cPoaR+7QBYPzzv9WufPbXxp1sLfRa\nH7m2esftQZKUDhNetVZGc5YIFrxd980ce5jXKJD5b8KiDxt+fTg0hSqVQCWCppD/KKeBQDVZ8bEx\nfPT3g3j7sgMoKKnglGdn8dG8bE577jeOeeIXz8TDz4H+J3j2LnIXaBrqvRlH8Pk18PFlDb8+HNwf\npNH6UA04xUSU5rkJ0UCgmryD+7Rn2rWHsm+3NG76cCEAu8urKKt06w6alA5nv+Oa1tpbp32t7f6X\nWwHjrHfhtmzrWKgHKhnjfwqMcPDoiROlD9VAk85Fa/BqQjQQqGahY+tE3r7sAP5xhGtk8cQXZwde\nCtNd1/3g7p1wwmNWwBgwDhJSre6mG2ZZaYyBxVPhyxtg5Te+7+PeBTXQN+1tS+CL6615jGY+Cfe1\nrTs9Rtg0gRKBk5YIQkIDgWo2Yh0x3HL8ABbefSzd2iSxMLuQ3rdP46tFQa41EOPjzyHzMNi6EBZ9\nAEs+go8uhbmvwLsT66ZdPwse6mz1GwfPKqV702Gl2/Kavz4G816FLQtg3mvWsdoZQMPM4+EapQ/V\nQN1Hoz14NQEaCFSzk5Ycx8x/HcV/x1vVPVe/M5/MW7/i/979i5pgSwhOKR2hrAA+vhy+uK7u+eoq\na9RowSZYM906tnGOta30GqA28ynXfnGetY2GJTLdq4ai9aFa21jsa0CZW54jWcXmS7R+nl40EKhm\n68z9u/PnHWMZndkWgC8WbuGLRVvqd5OEVNd+xe6657+7E944BZ4a4pqldOG71sC0Sq+eS5t+d+1H\n1fw4TaBEEGiFMvdDUfF5uovWz9OTBgLVrHVITeD9K8dww9h+ANz84SKe/TGLyuoAC9q7S25X99hB\n11rbtT/DnOfrnt+xBu5rA1nf+r+vc9qKqjJqHxY1QeapsTWJXkOBVihzOxZtgcBnCSb6RCQQiMh6\nEVksIgtEZG4k8qBaDhHhurF9mXvnWI4ZlMFj363ipGdmsii7YM8X73Nw3WN5Wdb2jZMDX1unKklc\nD3vnuAb3BddrKqHQa9qMcGgSvYaCWKEMom/K8GgNrF4iWSI40hgz3BgzKoJ5UC1I+5QEJp27Hy+e\nP5KdJRWcOmkW57z0O89Mz/J/kXPyusGnW/MUjTgP4pI805z1Loy8GIaM30MOjKtB2Fk1VFXmevbO\nfxOeHBSBEbJNqETg6xu2R4km2r6BR+nn6SV2z0mUal6OG9yJMb3a8cjXK3j3j438tiafgtJKThrW\nhW5tkmifkuBK7IiD65dYk9bF2SujbV8BSz+29sdcbXU1HTAOirZZPYt8iU+FiiIo2gIpHVwlgupy\nah8Wv0+ytqunWwEnXKL14e8h0BQTURwImsRnG7kSgQG+E5F5InKFrwQicoWIzBWRubm5uWHOnmru\n0pLiePj0oUy50CqQTpm5jlMnzWLUAz9gvP9407u7ggBAxwHQro+1n5rhOu5cMtHbP1fDOfaymtPv\ns7a+qoac3BulN/1pBZhQaupVQ1HdxhFt+fEtUoHgEGPMfsDfgKtF5DDvBMaYycaYUcaYUR06dAh/\nDlWLcPTADNY8NI4LDnStX3DNO3/VnavI2yXfwcHXu9Y5ANcsmOKAv/8G46fAeR9bJQDnkpmrf4Bp\nt8Dyz63XVT5mTi0vcu1PGQvPH9SA36w+ovlBagu2sVhLBA0SkUBgjNlsb7cDnwCjI5EPpQAcMcJ9\npwxh+X3Hc8Vhvfh+eQ5HP/4zk2asprC00nOqCqdW7eCYez1nJk1Mt+Yy+vssyBhsLanZ52jrXFI6\nHHaLtf/Hi65rqitc6x44lXt1Uw31QLNoG1D2zCh471yvgwEWptlTG8HrJ8GkAxote/USbYHJj7AH\nAhFpJSKpzn3gWGBJuPOhlLekeAe3jxvI9BsP55A+7Xn025UMu/c7Btz1DVsK9lBCAHutg7uh40Df\n5wefWveY+7d/pwofx0Ip2qpW8rNgxZeexwLNPrqnEsG6XyB3RaNlr36i4PMMQiRKBBnATBFZCPwB\nfGWM8TNxi1Lh171tMpMvGMWL54+sPXbof2fw8q9rKa3Yi5GrGYPh9Jc8j81+tm467xJByEVZicDJ\nvW3Ema2s7z3TGOM1KWAU5R+iI7AGIeyBwBiz1hgzzP4ZbIx5MNx5UCoYxw3uxLqHx/HsOSPYr0c6\nD3y1nIF3f8Odny4OfjI7b/ueCdcvDpzG2Rc+XA+RaJ1iomSHa9+Zx2qvmWB/fYyobiOItsDkh44s\nVioAEeHEfbvw4VUH8Zi9ROZbv29kzMPTefbHLKqCHaHsLr0HXBygEGzsUke4HmrR9PB357FWtJ88\nLnzf83W0/S7Rlh8/NBAoFaQzRnYj68G/cf+pQyitqOax71Yx5uEf+WDupj1f7G2fA/2fc05EF7YJ\n6aKsjcBJ3NaK9ldqSUr3vCZQ8Azn71a2y+r620RoIFCqHuIcMZw/Zh/+uvsYHj1jX/J2l3PL1EWc\nP2UOc9fv2PMN3F3qVt/tcBvEFu5AEHXVKTb3EoH7Q9x9hlHvsRuBHvahXmDI3TtnWl1/a99TAiaP\nNA0ESjVAnCOGCaO6s/y+47l93ACWb93FGS/MZsxD0/lmyba6g9J86dDftZ/S0bUf9kBQz8ZiY2DO\ni6Fv1PZXNWTcAoH3tNOBglrBxkbJVlA2zra2tZPgRVFJywcNBErthaR4B1cc1ptfbzmK28cNoKis\nkqvemsdpz/3G9OU5LMou8L8GQmIaHHkndB4Gad3rng/b3Pr1rBrK+h6+vgW+uyN0WfLm/oB3/1y8\nH/yBAsFLRzVunoIRDetNBEEDgVKNwBkQ/rr7WB48bQh5u8u59PW5nPzsLA75z4+szfXz7fnwm+HK\nXzzXPXAKVyCo7xQTzp47u7eHJDuurPhpF3AvEZh6lAjCPT4Dom82VD80ECjViOJjYzj3gH2Y8c8j\nePQMa4W0LYVlHPPkLzz41TL/U1d0Hen5umgbFIf4QevkUf/u9g12xsPwyVV108fEWdtQP+T8ffN3\nP14nDwECmXN+qIa4Jw3ePL3+19U0jUCgs48qFQLONoQJo7oze00+k39Zw5SZ63h11nr2z2xLtzZJ\ndE5L5MZj7XaCg6+Fnx5y3eDx/r5vbIxnb5pgVZbCzvV+Rj27PTzd++7//Ii1Pe0Fz+QOZyAIwSIw\nfqeL8HN82yKv632UCJLbQ0kedBy0d3lzLkVaH02kRKCBQKkQO7B3Ow7s3Y5NO0p4ddZ6Xpm1rvZc\n9s5Sisqr+PsRvdnv3wXWA/u3/1lrIC9wW5fgnjTX/plvwiB7UZyiHFgyFUZf4XpA+/Lp32HpJ3Dr\nJkhs7XnO/eFZbM/0u2ur/3s5A1Eo6r/d7+lRNeSnRODNVyBw3jMSD+Xa99ReQ0oprKkr7j5pECvu\nP56bj7O+8X/812a+X5bD6c/9xgu/rOWrFYVMMmfAqZPgjm3QqmPdG31wPnx2NexYC/NehW9vh6Wf\nBn7zdb9a260L6p5z/xbunOBuV4CV0qpD+GD1CATuVUN+qq+8BQoE25ftXd4aorZqKLp7DWmJQKkw\nS4xzcPWRfbj6yD5s2lHC+VPmsD6/hEe+dk2M1jE1gXFDO9Pq5ixrecvty6wRyT/8G+a+Yq1i5r6S\nWd7KwG/qXCP59ZPggs9gmt1IHZcESz52pXOWCNxnPK2u9CxtOKuEQlE15K9E4M67gdjjnI8HrvOe\nBRuszzImjN9/81eH7732gpYIlIqg7m2T+enmI1n38Di+/L9DOGlYFwBunrqI4fd9xwEP/cDD365k\nc2Jvq0rnxCetNQ5ad/W80bzXYNcW/90/3fvkv3EK5K2CHXYVVaGzf71YAaCmBipLXOmXfuJ5L+e3\n3KisGvLx+7uXXHyt/9DY3KvVfrgn9O/XCLREoFQUEBGGdE3jmbNH8PTE4czdsJMvFm7hzd838OLP\na3nx57UM657OuCGdOG7wGDJvtKs5cpZaJYQ/X4YnBlo9etpkWt/gUzKg77FwwJVQ5KPOP2ep9VBv\n3896feQdMOMB+OgSyHUrYUy72Zosz8k5WjYUVUPVboGgJkD30Ypi+O2ZutfXGVdgPEsQa2fAgBMa\nJ6/+PDEgtPcPAQ0ESkWZmBhhdM+2jO7ZlvtOGcyCTQX8tiafaYu38vDXK3j46xUM6JTKsYM78fPK\nnazdfjSLrjwPWfElrP4e8rKsb/Tbl1kPvm9v8/1GH9urq7WyVwBsk2ltPUoA4jrulLPU2sYmUC/V\nVZCzGLqM8J8mqBJBDfzyKMx8su713oHAu9Ty3jlwT2HweW4hNBAoFcVEhBE92jCiR5vaNoVvl27j\nu6U5PPNjVu0X5WPe38U5o89j7Bk30aNdsnUwbzU8O9L3jeNTXQOsnO0CA0+CNj1hp6tXE6Muhrmv\nWtVOra1qq9oups5AsHO9NcHavhMC/zL3t7O2l3wLPcb4TuO3jcCrROCvV5N3IHCWWjoMhNzlruOV\npVbpydHIj0C/I7Oju9eQBgKlmpDubZO57NBeXHZoL/J2l/P27xt59bd1lFdVc9+Xy7jvy2X0at+K\nw/t34PB+HTjg9h0kUW71AopxQFIbyJ4HXfezvlXv2gzLPrPaHuIS4boFkL8GXjneGtA26lKr6mn5\nl3DAFVYmKoutbYXdjvDyMVbaNT/Cac/7zniVW8PyhlkBAoFbdZMJMKDM18puVkKv+9mBpSTPLYmB\nBztZ+7dv8VxudG/5ndhOew0ppUKgfUoC143ty3Vj+2KMYV1eMT+tzOXnVbm8M2cjr85aT7wjhv32\nSWdAp9Zktktm/MgUWvU+mhpjiD3+YeuhWlUO8cmuG7frDTdnWfvGQNdR8PXN1vrL7Xq7AoAzIDhH\nQC98x38gcH8Q567y/0v5G03s/k27stR3N1jva8AqyYBVveUs+cx9xXX+8QFw+C1w0P/5z1N9uDey\ne+TLwDsTYfTl0Gds47xXI9JAoFQzICL06pBCrw4pXHJIT0orqvlj/Q5mrc5jZlYer/22HoAHvlpO\nVY0hJSGWJ84cxv6ZbWnTKpmP5mXTOT2Rg3q3974xHHMfvDYOntkPOgxwrf+7c701liEYzocweFY9\neQumaqgkz/84B+9A8Jy9aP2wsyDbXh9g0x+u8+W74Ls7YfDpkObVE6v2nvX4Nu8vEFSVw6pvYM0M\nuCtMU4fUgwYCpZqhpHgHh/ezqocACksqWZBdwO9r83n+pzXsLq/iijfnAdb8SBVV1gP0X8cPYFj3\nNApLKjmod3vSkuMg82AYPwVmPWXVq4vDVW3zP6+G35fHQtteVpBI624FkdTOMPMp63z7/lbVk7N3\nkHcdvXtPJO/SQUoG7M7xXMvY2+xJMO912P8yWP+r67g44KqZ8MIhvru97tocIBDUY72GCn+BwJ5j\nqiHTg4SBBgKlWoC05LjawPCv4wdQVlnN4s2F/LFuB3+u38FPK61v7P/5ZoXHdQM6pTKoS2sGdBpB\n/6M+YUCnVBZuyGNwtzZ0KV6B5Cyxeidt/N164G+YZT3oS3fApjnW9BeOeNfgs37HWVNoOBuOW3eF\noROsIBOfChPfcL25e327MdYo6905rhKJBwEMZH1nvVz6sefphFToNNTKy+6cupdvXQjdR/v+8Ooz\nC6yzusxbbTDRQKCUihKJcQ72z2zL/pltPY7n7S5n6ZZdfLVoC6mJcazKKWJmVh4fz69bFTOgUypH\nDzyAipr9+SR3LJOO2Y99z0gnKd7BQ58vomrNDO4+pJU1b1L+aohLhqPvBgxs/guS28DyL6wgAFYv\npjdPc73Be2dD76OhzT6w7mfoYS/v+ftzrjTigLPesb5pv+M21sHd6CtgyHhrv7rCs6TgtMVuc8hZ\navUwch99XJ8ZRP2VCGrzG51jeDUQKKVqtU9J8KhSctpZXMHKnCJWbitiysx1bNxRwoptRWRt3021\nvfDOxMm/A9CpdSLbdpUBfYjN7UW/jFSemr+KzQWlXBS/ig6pF1HV40L+fkRv4hwxrFw0h16d2xG3\ncy1k/0l1dSXztlUzsnIejpJ82DLfykSnodC2t2syvqtmUdlhEAc+/CMPH5nCMf5+qVGX7LlKZsFb\nVmP41Ivte8+03g/gl8d8X1OYDZvnWQFu3S9w7P1WQ3YgURoIJKgl9SJs1KhRZu7cuZHOhlLKS0VV\nDevzi9m+q5ydJRWszytmfX4JH83PDur6Xu1bsTavGBGr9ufQvu35bU0+1TWGk4d14e6TBtE2OZ7F\n67bw/sJ87j91KMYYdm1dTdtu/Vibu5ujHv8ZgDk3H0RGuzbWjbevsJb/rCxlZ2wH2rSy1zb+4V6Y\n+YS1321/VwOyHzVtexOzY43rwBU/wYJ3rEZh97meAK79C769A1ZO83/DxHT413r/gamqAn68D9L3\nsdo59rJNQUTmGWNG7TGdBgKlVKhUVNWwuaCUBZt2Eu9wkL2zhNyict79YyNHDOjI5p2lLNhUQGpi\nLEVlvucuinMIldV1n1Pd2iSRvbPuN/DD+nWga3oSAzunMmnGanJ2lXNgr3bceeJAYkwN/bZ9iaPP\nUa7G4eI8eLS3z/deX5NBZoyPNoW9EZtkzRsVm2Cto5y+j1X9tW2J1bbirv8JcMYr1hiPBtBAoJRq\nUiqqaigqq6SwtBJHjLB86y62FZaRU1TOtsIyfliWw6H92rNwUyGbC0o5Yd/O/Loql11eASQ9OY6q\nasPucv+T4nVNTyItKY70ZOsnLSme9OQ4fluTz8JNBQAcP7gT3yzdxitx/2VWzWDuinsbgE29z6Fm\n9JW06dKL1lIG2xbz31ff4zTHLPrGuNpSsk17sjqdwO9mMKe0zeabxVs4a0R7uqTGQsVuqz0h+w+r\nIbuyDPKzfGf2zDdg0CkN+kw1ECilWqyq6hq27SqjoKSSxLgY5q7fiQjMWJFLRusEisqrKCyppKC0\nkoKSCgpLKykoqaSqpn7Pw/Yp1jQbebutHk6dyKeMeJ6Le5r/VJ3FQlN3ecxHTh9KckIs8Y4YEuJi\nSIiNISHWQWV1DXPW7uCKw3qRFO+w6sq2L4OMwQ3+HDQQKKVUPRhjKCipJC0pDoP1cM/bXU5KQizp\nyfHsKq1k5bYiVuYUsaWglNLKahJiHRSWVjBtcYCxDQ2QmhBLq4RYWiU4eOi0oRzQq12D7hNsINBe\nQ0ophTU6u7ZRGchonUhGa1fdfFpSHN3bJjN2UEa9711RVUNsjFBWVU3+7grKq2oor6qmvKqGiqoa\nyqtqKKusZmZWHm1axVNUVklxeRXF5dWkJgZYgrSRRCQQiMjxwNOAA3jZGPNIJPKhlFLhEB9rdRtN\njo8lua3/x+5xgzuFK0sewt6pVUQcwCTgb8Ag4GwRGRTufCillLJEYnTDaGC1MWatMaYCeA9oWJO4\nUkqpvRaJQNAV2OT2Ots+5kFErhCRuSIyNzc31/u0UkqpRhKd450BY8xkY8woY8yoDh067PkCpZRS\nDRKJQLAZ6O72upt9TCmlVAREIhD8CfQVkZ4iEg+cBXwegXwopZQiAt1HjTFVInIN8C1W99FXjDFL\nw50PpZRSloiMIzDGTAMCTNGnlFIqXJrEFBMikgtsaODl7YG8PaaKrGjPY7TnDzSPjSHa8wfRn8do\ny98+xpg99rZpEoFgb4jI3GDm2oikaM9jtOcPNI+NIdrzB9Gfx2jPnz9R231UKaVUeGggUEqpFq4l\nBILJkc5AEKI9j9GeP9A8NoZozx9Efx6jPX8+Nfs2AqWUUoG1hBKBUkqpADQQKKVUC9esA4GIHC8i\nK0VktYjcGqE8dBeRGSe03/EAAAZSSURBVCKyTESWish19vG2IvK9iGTZ2zb2cRGR/9l5XiQi+4Up\nnw4R+UtEvrRf9xSROXY+3renA0FEEuzXq+3zmWHKX7qITBWRFSKyXEQOjMLP8Ab733iJiLwrIomR\n/hxF5BUR2S4iS9yO1ftzE5EL7fRZInJhiPP3qP3vvEhEPhGRdLdzt9n5Wykix7kdD9nfuq88up27\nSUSMiLS3X4f9M2wUxphm+YM1fcUaoBcQDywEBkUgH52B/ez9VGAV1oI8/wVutY/fCvzH3h8HfA0I\nMAaYE6Z83gi8A3xpv/4AOMvefwH4u73/D+AFe/8s4P0w5e914DJ7Px5Ij6bPEGsq9XVAktvnd1Gk\nP0fgMGA/YInbsXp9bkBbYK29bWPvtwlh/o4FYu39/7jlb5D9d5wA9LT/vh2h/lv3lUf7eHesqXI2\nAO0j9Rk2yu8Y6QyE7BeDA4Fv3V7fBtwWBfn6DDgGWAl0to91Blba+y8CZ7ulr00Xwjx1A6YDRwFf\n2v+J89z+GGs/S/s//oH2fqydTkKcvzT7IStex6PpM3Sus9HW/ly+BI6Lhs8RyPR60NbrcwPOBl50\nO+6RrrHz53XuNOBte9/jb9j5GYbjb91XHoGpwDBgPa5AEJHPcG9/mnPVUFAL4ISTXfwfAcwBMowx\nW+1T2wDnitiRyPdTwC1Ajf26HVBgjKnykYfa/NnnC+30odQTyAVetauvXhaRVkTRZ2iM2Qw8BmwE\ntmJ9LvOIrs/Rqb6fWyT/li7B+oZNgHyEPX8icgqw2Riz0OtU1OSxPppzIIgqIpICfARcb4zZ5X7O\nWF8RItKPV0ROBLYbY+ZF4v2DFItVNH/eGDMCKMaq0qgVyc8QwK5nPwUraHUBWgHHRyo/wYr05xaI\niNwBVAFvRzov7kQkGbgduDvSeWkszTkQRM0COCIShxUE3jbGfGwfzhGRzvb5zsB2+3i4830wcLKI\nrMdaP/oo4GkgXUScs9O656E2f/b5NCA/hPkD69tTtjFmjv16KlZgiJbPEGAssM4Yk2uMqQQ+xvps\no+lzdKrv5xb2z1NELgJOBM61g1U05a83VsBfaP/ddAPmi0inKMpjvTTnQBAVC+CIiABTgOXGmCfc\nTn0OOHsOXIjVduA8foHd+2AMUOhWjG90xpjbjDHdjDGZWJ/Rj8aYc4EZwBl+8ufM9xl2+pB+ozTG\nbAM2iUh/+9DRwDKi5DO0bQTGiEiy/W/uzGPUfI5u6vu5fQscKyJt7JLPsfaxkBCR47GqKk82xpR4\n5fssu8dVT6Av8Adh/ls3xiw2xnQ0xmTafzfZWB1CthEln2G9RbqRIpQ/WC34q7B6FNwRoTwcglX0\nXgQssH/GYdUHTweygB+AtnZ6ASbZeV4MjApjXo/A1WuoF9Yf2WrgQyDBPp5ov15tn+8VprwNB+ba\nn+OnWD0vouozBO4FVgBLgDexerdE9HME3sVqs6jEemBd2pDPDauufrX9c3GI87caqz7d+ffyglv6\nO+z8rQT+5nY8ZH/rvvLodX49rsbisH+GjfGjU0wopVQL15yrhpRSSgVBA4FSSrVwGgiUUqqF00Cg\nlFItnAYCpZRq4TQQKBViInKE2LO6KhWNNBAopVQLp4FAKZuInCcif4jIAhF5Uaw1GnaLyJNirTMw\nXUQ62GmHi8jvbnPmO+f07yMiP4jIQhGZLyK97duniGs9hbft0cdKRQUNBEoBIjIQmAgcbIwZDlQD\n52JNHjfXGDMY+Bn4t33JG8C/jDH7Yo0gdR5/G5hkjBkGHIQ1IhWsWWevx5pTvxfWPERKRYXYPSdR\nqkU4GhgJ/Gl/WU/CmoytBnjfTvMW8LGIpAHpxpif7eOvAx+KSCrQ1RjzCYAxpgzAvt8fxphs+/UC\nrPntZ4b+11JqzzQQKGUR4HVjzG0eB0Xu8krX0DlZyt32q9G/PRVFtGpIKct04AwR6Qi16/rug/U3\n4pw99BxgpjGmENgpIofax88HfjbGFAHZInKqfY8Ee+56paKafitRCjDGLBORO4HvRCQGa6bJq7EW\nwRltn9uO1Y4A1vTNL9gP+rXAxfbx84EXReQ++x4TwvhrKNUgOvuoUgGIyG5jTEqk86FUKGnVkFJK\ntXBaIlBKqRZOSwRKKdXCaSBQSqkWTgOBUkq1cBoIlFKqhdNAoJRSLdz/A6sdlVulsiw5AAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pXyPUUhR1qHo",
        "colab_type": "code",
        "outputId": "46e8242e-de1e-42d2-c43f-99b4cf7eb601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(1,7):\n",
        "    j=1\n",
        "    while(j<i):\n",
        "      print(j,end =\" \")\n",
        "      j+=1\n",
        "    print('\\n')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "1 \n",
            "\n",
            "1 2 \n",
            "\n",
            "1 2 3 \n",
            "\n",
            "1 2 3 4 \n",
            "\n",
            "1 2 3 4 5 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KTgMUYKE-hdH",
        "colab_type": "code",
        "outputId": "3b682b24-c4d3-4f44-dbbf-7999b015d420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"-\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rh6pFLpJ_dsf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p=[1.0,2.0,3.0]*2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nHzsjlIdiy5D",
        "colab_type": "code",
        "outputId": "a445eab9-c0cc-4126-9654-da6f5db2c386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "300000"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 258
        }
      ]
    },
    {
      "metadata": {
        "id": "-T4034teizlp",
        "colab_type": "code",
        "outputId": "19dc3c4f-f3a1-4722-f0db-61e6ea8a6a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "80000"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 287
        }
      ]
    },
    {
      "metadata": {
        "id": "xW0q0vr6ynYR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}