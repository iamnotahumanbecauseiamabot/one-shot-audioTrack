{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "jiZbNWpg-iZX",
    "outputId": "716ffa94-c039-4ef6-c653-f683c32e3458"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-3b8e316d-4ab3-4bc6-b506-4cb5896bad8c\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-3b8e316d-4ab3-4bc6-b506-4cb5896bad8c\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving speaker1.mp3 to speaker1.mp3\n",
      "Saving speaker2.mp3 to speaker2.mp3\n",
      "Saving speaker3.mp3 to speaker3.mp3\n",
      "Saving speaker4.mp3 to speaker4.mp3\n",
      "Saving speaker5.mp3 to speaker5.mp3\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "-Yh25c-w-yZO",
    "outputId": "8eaf21fc-0986-43b9-b7cd-11cf19d6bfe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /usr/local/lib/python3.6/dist-packages (0.23.1)\n",
      "Collecting soundfile\n",
      "  Downloading https://files.pythonhosted.org/packages/68/64/1191352221e2ec90db7492b4bf0c04fd9d2508de67b3f39cbf093cd6bd86/SoundFile-0.10.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.12.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.19)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub\n",
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pboGkUhm-fqU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh/miniconda3/envs/env3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pydub import silence\n",
    "from pydub import AudioSegment\n",
    "from wave import open as open_wave\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import librosa\n",
    "from librosa import display\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "import time\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ec3KJsBql5h9"
   },
   "outputs": [],
   "source": [
    "segment_length= 5000\n",
    "n_mels = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import shutil\n",
    "import scipy\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=0\n",
    "for file in glob.glob('./data/*'):\n",
    "    i=0\n",
    "    classes+=1\n",
    "    for file1 in glob.glob(file + '/*'):\n",
    "        for file2 in glob.glob(file1 + '/*'):\n",
    "            if \"wav\" not in str(file2):\n",
    "                subprocess.call(['ffmpeg', '-i', file2,'./' + file +'/{}.wav'.format(i)])\n",
    "                os.remove(file2)\n",
    "                i+=1\n",
    "        shutil.rmtree(file1)\n",
    "for file in glob.glob('./data/*'):\n",
    "    sound1 = 0\n",
    "    for file1 in glob.glob(file + '/*'):\n",
    "        sound1 = AudioSegment.from_wav(file1) + sound1\n",
    "        os.remove(file1)\n",
    "    sound1.export(file + '/final.wav', format=\"wav\")\n",
    "for file in glob.glob('./data/*'):\n",
    "    t1 = 0\n",
    "    i=0\n",
    "    for file1 in glob.glob(file + '/*'):\n",
    "        newAudio = AudioSegment.from_wav(file1)\n",
    "        f = sf.SoundFile(file1)\n",
    "        time = len(f) / f.samplerate\n",
    "        segments = (time*1000)/segment_length\n",
    "        for j in range(0,int(segments)): \n",
    "            newAudio1 = newAudio[t1:(t1+segment_length)]\n",
    "            newAudio1.export(file + '/{}.wav'.format(i), format=\"wav\")\n",
    "            t1 = t1 + segment_length\n",
    "            i+=1\n",
    "        os.remove(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SeeIY8lq4lCy"
   },
   "outputs": [],
   "source": [
    "def preprocess_audio_mel(audio, sample_rate=16000, window_size=25,step_size=10, eps=1e-10):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels= n_mels)\n",
    "    mel_db = (librosa.power_to_db(mel_spec, ref=np.max))\n",
    "    return mel_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = glob.glob('./data/84/*')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RROdLgjO-fqy"
   },
   "outputs": [],
   "source": [
    "def data_make(file_path):\n",
    "    dataset = glob.glob(file_path)\n",
    "    data1 = []\n",
    "    segments = len(dataset)\n",
    "    for i in range(0,segments):\n",
    "        y, sr = librosa.load(dataset[i], sr=16000)\n",
    "        y=y.astype(np.float64)\n",
    "        y=y/np.abs(np.max(y))\n",
    "        y = preprocess_audio_mel(audio = y)\n",
    "        y = np.array(y)\n",
    "        #y=np.reshape(y, (250,320))\n",
    "        data1.append(y)\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965
    },
    "colab_type": "code",
    "id": "4jvCW5vpjN0N",
    "outputId": "aa62aad6-0c6b-4fa2-b3f7-f0fca26a83be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/84\n",
      "./data/251\n",
      "./data/174\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "data1 = []\n",
    "i=0\n",
    "for file in glob.glob('./data/*'):\n",
    "    print(file)\n",
    "    data = data_make(file + '/*') + data\n",
    "    data1.append(data_make(file + '/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 96, 128, 157)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = np.asarray(data1)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 128, 157, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0, len(data)):\n",
    "    data[i] = data[i][:, :, None] * np.ones(3, dtype=int)[None, None, :]\n",
    "data = np.asarray(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SNKXcKh0plct",
    "outputId": "55e1c9e1-08d1-46df-ca31-ecb5a97beb08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "96\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "labels_for_pre = []\n",
    "for i in range(0,3):\n",
    "    print((data1[0].shape[0]))\n",
    "    labels_for_pre = ([i] * (data1[0].shape[0])) + labels_for_pre\n",
    "one_hot_labels = keras.utils.to_categorical(labels_for_pre, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16.VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                                       Output Shape                                                Param #                \n",
      "======================================================================================================================================================\n",
      "input_10 (InputLayer)                                              (None, 224, 224, 3)                                         0                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)                                              (None, 224, 224, 64)                                        1792                   \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)                                              (None, 224, 224, 64)                                        36928                  \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)                                         (None, 112, 112, 64)                                        0                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)                                              (None, 112, 112, 128)                                       73856                  \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)                                              (None, 112, 112, 128)                                       147584                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)                                         (None, 56, 56, 128)                                         0                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)                                              (None, 56, 56, 256)                                         295168                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)                                              (None, 56, 56, 256)                                         590080                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)                                              (None, 56, 56, 256)                                         590080                 \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)                                         (None, 28, 28, 256)                                         0                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)                                              (None, 28, 28, 512)                                         1180160                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)                                              (None, 28, 28, 512)                                         2359808                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)                                              (None, 28, 28, 512)                                         2359808                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)                                         (None, 14, 14, 512)                                         0                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)                                              (None, 14, 14, 512)                                         2359808                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)                                              (None, 14, 14, 512)                                         2359808                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)                                              (None, 14, 14, 512)                                         2359808                \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)                                         (None, 7, 7, 512)                                           0                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "flatten (Flatten)                                                  (None, 25088)                                               0                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "fc1 (Dense)                                                        (None, 4096)                                                102764544              \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "fc2 (Dense)                                                        (None, 4096)                                                16781312               \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "predictions (Dense)                                                (None, 1000)                                                4097000                \n",
      "======================================================================================================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "Layer (type)                                                       Output Shape                                                Param #                \n",
      "======================================================================================================================================================\n",
      "input_11 (InputLayer)                                              (None, 128, 157, 3)                                         0                      \n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)                                          (None, 3)                                                   9691459                \n",
      "======================================================================================================================================================\n",
      "Total params: 9,691,459\n",
      "Trainable params: 9,691,459\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.input\n",
    "\n",
    "model.summary(line_length=150)\n",
    "model.layers.pop(0)\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "\n",
    "    # let us say this new InputLayer\n",
    "#newModel = Model(newInput, newOutputs)\n",
    "\n",
    "newinput = Input((128, 157, 3))\n",
    "new_model = Sequential()\n",
    "new_model.add(model)\n",
    "new_model.add(Dense(2048, activation='relu'))\n",
    "new_model.add(Dense(3,activation='softmax'))\n",
    "output = new_model(newinput)\n",
    "mm = Model(inputs=newinput,outputs=output)\n",
    "mm.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh/miniconda3/envs/env3/lib/python3.5/site-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 288 arrays: [array([[-40.15658402, -38.90275168, -38.27398657, ..., -42.62111658,\n        -43.35683613, -38.26441197],\n       [-48.50186516, -43.838476  , -41.05961059, ..., -49.96260389,\n        -46.50417312, -3...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ff33deb39423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env3/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 288 arrays: [array([[-40.15658402, -38.90275168, -38.27398657, ..., -42.62111658,\n        -43.35683613, -38.26441197],\n       [-48.50186516, -43.838476  , -41.05961059, ..., -49.96260389,\n        -46.50417312, -3..."
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr = 0.00006)\n",
    "mm.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])\n",
    "# Train the discriminator\n",
    "d_loss = mm.fit(data,one_hot_labels,validation_split=0.33,nb_epoch=3000,verbose=1,shuffle=True)\n",
    "\n",
    "print( d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1YFMry_gRJ4f",
    "outputId": "14554e10-42a4-4526-8640-23886430bab7"
   },
   "outputs": [],
   "source": [
    "min_index = len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1b51ay-IHYg_"
   },
   "outputs": [],
   "source": [
    "def pair_make(item, datacat1, datacat2):\n",
    "    pair = []\n",
    "    pair1 = []\n",
    "    for i in range(item):\n",
    "        indx = np.random.randint(0, min_index)\n",
    "        indx1 = np.random.randint(0, min_index)\n",
    "        pair.append(datacat1[indx])\n",
    "        pair1.append(datacat2[indx1])\n",
    "    return pair, pair1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InlxSejLrX2t"
   },
   "outputs": [],
   "source": [
    "classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWsnMxRxsZWg"
   },
   "outputs": [],
   "source": [
    "pair = []\n",
    "cor_pair = []\n",
    "for i in range(0, classes):\n",
    "    pair1 = []\n",
    "    pair11 = []\n",
    "    pair1, pair11 = pair_make(100, data[i],data[i])\n",
    "    pair = pair + pair1\n",
    "    cor_pair = cor_pair + pair11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "srL-eAFquqUw"
   },
   "outputs": [],
   "source": [
    "paird = []\n",
    "diff_paird = []\n",
    "for i in range(0,classes):\n",
    "    for j in range(0, classes):\n",
    "        pair1 = []\n",
    "        pair11 = []\n",
    "        if(j<i):\n",
    "            pair1, pair11 = pair_make(5, data[i],data[j])\n",
    "            paird = paird + pair1\n",
    "            diff_paird = diff_paird + pair11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SbqWfZzz2jSx",
    "outputId": "9d516220-51ce-4c21-b01d-cb0c7b27f130"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tssCMLiF0WYE",
    "outputId": "55ffd4f7-dffb-466b-c040-1335f68e8b61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1VQWkShHYl3"
   },
   "outputs": [],
   "source": [
    "label1 = [1]*150\n",
    "label0 = [0]*150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VInHr5AZTQBD"
   },
   "outputs": [],
   "source": [
    "lside = pair + paird\n",
    "rside = cor_pair + diff_paird\n",
    "labels = label1 + label0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EOuYfpZU1KNj",
    "outputId": "63611e97-f4bf-4d2f-cb36-17f1872451d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7900"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "64MzvOxEVNAN"
   },
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ji38BQIYY85J"
   },
   "outputs": [],
   "source": [
    "X_l = np.array(lside)\n",
    "X_r = np.array(rside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aYc1oAuga0pB",
    "outputId": "cee2d12c-8fb1-4c08-82f6-40fdc022cb32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7900, 128, 157)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7CyG0WC_-fro"
   },
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    mm.layers.pop()\n",
    "    model = mm\n",
    "    \n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "7EgTK8z3-frr",
    "outputId": "cf24dfdf-14f1-4169-920a-39831355e84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 128, 157, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 128, 157, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 3)            0           input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 3)            0           model_2[1][0]                    \n",
      "                                                                 model_2[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            4           lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model((128, 157, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLjubBvJ-frw"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88073
    },
    "colab_type": "code",
    "id": "g0b2arIe-fry",
    "outputId": "8491a1ec-0ae2-4768-991f-befa36823849"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 100 samples\n",
      "Epoch 1/3000\n",
      "200/200 [==============================] - 24s 122ms/step - loss: 18.2190 - acc: 0.6000 - val_loss: 18.8661 - val_acc: 0.1000\n",
      "Epoch 2/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 18.0041 - acc: 0.7750 - val_loss: 18.9795 - val_acc: 0.0800\n",
      "Epoch 3/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.8743 - acc: 0.8300 - val_loss: 19.1528 - val_acc: 0.0300\n",
      "Epoch 4/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.7820 - acc: 0.8850 - val_loss: 19.2091 - val_acc: 0.0600\n",
      "Epoch 5/3000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 17.7363 - acc: 0.9150 - val_loss: 19.3111 - val_acc: 0.0900\n",
      "Epoch 6/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.6577 - acc: 0.9600 - val_loss: 19.3488 - val_acc: 0.0600\n",
      "Epoch 7/3000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 17.6186 - acc: 0.9800 - val_loss: 19.3478 - val_acc: 0.0600\n",
      "Epoch 8/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.5982 - acc: 0.9800 - val_loss: 19.3670 - val_acc: 0.0900\n",
      "Epoch 9/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.5897 - acc: 0.9700 - val_loss: 19.3508 - val_acc: 0.0500\n",
      "Epoch 10/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.5922 - acc: 0.9650 - val_loss: 19.3258 - val_acc: 0.1000\n",
      "Epoch 11/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.5577 - acc: 0.9700 - val_loss: 19.2288 - val_acc: 0.1200\n",
      "Epoch 12/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.5183 - acc: 0.9900 - val_loss: 19.2904 - val_acc: 0.1300\n",
      "Epoch 13/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.5238 - acc: 0.9850 - val_loss: 19.3808 - val_acc: 0.0900\n",
      "Epoch 14/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.5146 - acc: 0.9750 - val_loss: 19.4760 - val_acc: 0.0800\n",
      "Epoch 15/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.4624 - acc: 1.0000 - val_loss: 19.3964 - val_acc: 0.0900\n",
      "Epoch 16/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.4589 - acc: 0.9950 - val_loss: 19.2460 - val_acc: 0.1200\n",
      "Epoch 17/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.4622 - acc: 0.9950 - val_loss: 19.3357 - val_acc: 0.1200\n",
      "Epoch 18/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.4456 - acc: 0.9950 - val_loss: 19.4199 - val_acc: 0.0700\n",
      "Epoch 19/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.4154 - acc: 0.9900 - val_loss: 19.4292 - val_acc: 0.0800\n",
      "Epoch 20/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.4098 - acc: 0.9950 - val_loss: 19.4698 - val_acc: 0.1000\n",
      "Epoch 21/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.3998 - acc: 0.9900 - val_loss: 19.5680 - val_acc: 0.0800\n",
      "Epoch 22/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.3806 - acc: 0.9850 - val_loss: 19.3740 - val_acc: 0.0800\n",
      "Epoch 23/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.3698 - acc: 0.9850 - val_loss: 19.2853 - val_acc: 0.0700\n",
      "Epoch 24/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.3583 - acc: 0.9900 - val_loss: 19.4294 - val_acc: 0.0700\n",
      "Epoch 25/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.3396 - acc: 0.9950 - val_loss: 19.4495 - val_acc: 0.0500\n",
      "Epoch 26/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.3485 - acc: 0.9800 - val_loss: 19.3674 - val_acc: 0.0800\n",
      "Epoch 27/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.3046 - acc: 0.9950 - val_loss: 19.2353 - val_acc: 0.0800\n",
      "Epoch 28/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.2898 - acc: 1.0000 - val_loss: 19.2392 - val_acc: 0.0800\n",
      "Epoch 29/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.2744 - acc: 1.0000 - val_loss: 19.3512 - val_acc: 0.0900\n",
      "Epoch 30/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.2609 - acc: 0.9900 - val_loss: 19.4542 - val_acc: 0.1000\n",
      "Epoch 31/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.2479 - acc: 0.9950 - val_loss: 19.5176 - val_acc: 0.0700\n",
      "Epoch 32/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.2254 - acc: 1.0000 - val_loss: 19.5177 - val_acc: 0.0900\n",
      "Epoch 33/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.2162 - acc: 0.9950 - val_loss: 19.6013 - val_acc: 0.0800\n",
      "Epoch 34/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.2135 - acc: 0.9850 - val_loss: 19.4575 - val_acc: 0.1000\n",
      "Epoch 35/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.1802 - acc: 1.0000 - val_loss: 19.1599 - val_acc: 0.1200\n",
      "Epoch 36/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.1795 - acc: 0.9950 - val_loss: 19.1143 - val_acc: 0.1400\n",
      "Epoch 37/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.1862 - acc: 0.9950 - val_loss: 19.0312 - val_acc: 0.1300\n",
      "Epoch 38/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.1601 - acc: 1.0000 - val_loss: 18.8783 - val_acc: 0.1300\n",
      "Epoch 39/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.1466 - acc: 0.9950 - val_loss: 18.9414 - val_acc: 0.1300\n",
      "Epoch 40/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.1410 - acc: 0.9950 - val_loss: 19.0513 - val_acc: 0.0800\n",
      "Epoch 41/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.1059 - acc: 1.0000 - val_loss: 19.0798 - val_acc: 0.1100\n",
      "Epoch 42/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.0800 - acc: 1.0000 - val_loss: 19.1762 - val_acc: 0.1300\n",
      "Epoch 43/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.0741 - acc: 1.0000 - val_loss: 19.2276 - val_acc: 0.1200\n",
      "Epoch 44/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.0486 - acc: 1.0000 - val_loss: 19.3127 - val_acc: 0.1100\n",
      "Epoch 45/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.0371 - acc: 1.0000 - val_loss: 19.2993 - val_acc: 0.1100\n",
      "Epoch 46/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 17.0110 - acc: 1.0000 - val_loss: 19.2328 - val_acc: 0.1400\n",
      "Epoch 47/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.9930 - acc: 1.0000 - val_loss: 19.3494 - val_acc: 0.1300\n",
      "Epoch 48/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.9763 - acc: 1.0000 - val_loss: 19.4274 - val_acc: 0.1300\n",
      "Epoch 49/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.9632 - acc: 0.9950 - val_loss: 19.4563 - val_acc: 0.1500\n",
      "Epoch 50/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.9390 - acc: 1.0000 - val_loss: 19.4740 - val_acc: 0.1400\n",
      "Epoch 51/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.9335 - acc: 1.0000 - val_loss: 19.3994 - val_acc: 0.1500\n",
      "Epoch 52/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.9079 - acc: 1.0000 - val_loss: 19.3899 - val_acc: 0.1500\n",
      "Epoch 53/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.8943 - acc: 1.0000 - val_loss: 19.2749 - val_acc: 0.1600\n",
      "Epoch 54/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.8736 - acc: 1.0000 - val_loss: 19.3323 - val_acc: 0.1700\n",
      "Epoch 55/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.8577 - acc: 1.0000 - val_loss: 19.3509 - val_acc: 0.2000\n",
      "Epoch 56/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.8386 - acc: 1.0000 - val_loss: 19.4866 - val_acc: 0.1900\n",
      "Epoch 57/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.8194 - acc: 1.0000 - val_loss: 19.4341 - val_acc: 0.1500\n",
      "Epoch 58/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.8084 - acc: 1.0000 - val_loss: 19.4644 - val_acc: 0.1600\n",
      "Epoch 59/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.7843 - acc: 1.0000 - val_loss: 19.2913 - val_acc: 0.1800\n",
      "Epoch 60/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.7686 - acc: 1.0000 - val_loss: 19.2820 - val_acc: 0.1700\n",
      "Epoch 61/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.7542 - acc: 0.9950 - val_loss: 19.3391 - val_acc: 0.1700\n",
      "Epoch 62/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.7369 - acc: 0.9950 - val_loss: 19.3565 - val_acc: 0.1600\n",
      "Epoch 63/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.7244 - acc: 0.9900 - val_loss: 19.1448 - val_acc: 0.1400\n",
      "Epoch 64/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.7081 - acc: 1.0000 - val_loss: 18.7116 - val_acc: 0.1600\n",
      "Epoch 65/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.6910 - acc: 1.0000 - val_loss: 18.5575 - val_acc: 0.1200\n",
      "Epoch 66/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.6882 - acc: 0.9950 - val_loss: 18.7811 - val_acc: 0.1100\n",
      "Epoch 67/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.6592 - acc: 1.0000 - val_loss: 18.8844 - val_acc: 0.1300\n",
      "Epoch 68/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.6408 - acc: 0.9950 - val_loss: 18.8848 - val_acc: 0.1500\n",
      "Epoch 69/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.6162 - acc: 1.0000 - val_loss: 18.8191 - val_acc: 0.1900\n",
      "Epoch 70/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.6026 - acc: 1.0000 - val_loss: 18.6516 - val_acc: 0.2000\n",
      "Epoch 71/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.5817 - acc: 1.0000 - val_loss: 18.6959 - val_acc: 0.1800\n",
      "Epoch 72/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.5594 - acc: 1.0000 - val_loss: 18.7272 - val_acc: 0.1900\n",
      "Epoch 73/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.5451 - acc: 1.0000 - val_loss: 18.7875 - val_acc: 0.1500\n",
      "Epoch 74/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.5261 - acc: 0.9950 - val_loss: 18.8002 - val_acc: 0.1700\n",
      "Epoch 75/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.5093 - acc: 1.0000 - val_loss: 18.8438 - val_acc: 0.1600\n",
      "Epoch 76/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.4919 - acc: 0.9950 - val_loss: 18.9577 - val_acc: 0.1600\n",
      "Epoch 77/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.4662 - acc: 1.0000 - val_loss: 19.0853 - val_acc: 0.1200\n",
      "Epoch 78/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.4654 - acc: 0.9950 - val_loss: 18.9740 - val_acc: 0.1000\n",
      "Epoch 79/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.4314 - acc: 1.0000 - val_loss: 18.6845 - val_acc: 0.1300\n",
      "Epoch 80/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.4145 - acc: 1.0000 - val_loss: 18.5790 - val_acc: 0.1400\n",
      "Epoch 81/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.4014 - acc: 1.0000 - val_loss: 18.6946 - val_acc: 0.1400\n",
      "Epoch 82/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.3725 - acc: 1.0000 - val_loss: 18.6954 - val_acc: 0.1400\n",
      "Epoch 83/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.3499 - acc: 1.0000 - val_loss: 18.8662 - val_acc: 0.1500\n",
      "Epoch 84/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.3348 - acc: 0.9950 - val_loss: 18.8440 - val_acc: 0.1200\n",
      "Epoch 85/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.3132 - acc: 0.9950 - val_loss: 18.8484 - val_acc: 0.1400\n",
      "Epoch 86/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.2929 - acc: 1.0000 - val_loss: 18.7637 - val_acc: 0.1500\n",
      "Epoch 87/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.2703 - acc: 1.0000 - val_loss: 18.6790 - val_acc: 0.1600\n",
      "Epoch 88/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.2535 - acc: 1.0000 - val_loss: 18.5945 - val_acc: 0.1700\n",
      "Epoch 89/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.2308 - acc: 1.0000 - val_loss: 18.6231 - val_acc: 0.1600\n",
      "Epoch 90/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.2099 - acc: 1.0000 - val_loss: 18.6417 - val_acc: 0.1600\n",
      "Epoch 91/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.1881 - acc: 1.0000 - val_loss: 18.7002 - val_acc: 0.1800\n",
      "Epoch 92/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.1697 - acc: 1.0000 - val_loss: 18.6477 - val_acc: 0.1800\n",
      "Epoch 93/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.1468 - acc: 1.0000 - val_loss: 18.6308 - val_acc: 0.1900\n",
      "Epoch 94/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.1294 - acc: 0.9950 - val_loss: 18.5843 - val_acc: 0.2000\n",
      "Epoch 95/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.1055 - acc: 1.0000 - val_loss: 18.4516 - val_acc: 0.1900\n",
      "Epoch 96/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.0865 - acc: 1.0000 - val_loss: 18.3506 - val_acc: 0.1900\n",
      "Epoch 97/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.0661 - acc: 1.0000 - val_loss: 18.4846 - val_acc: 0.1800\n",
      "Epoch 98/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.0438 - acc: 1.0000 - val_loss: 18.5276 - val_acc: 0.1900\n",
      "Epoch 99/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.0242 - acc: 1.0000 - val_loss: 18.5491 - val_acc: 0.1700\n",
      "Epoch 100/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 16.0101 - acc: 0.9950 - val_loss: 18.5953 - val_acc: 0.1800\n",
      "Epoch 101/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.9799 - acc: 1.0000 - val_loss: 18.4300 - val_acc: 0.2100\n",
      "Epoch 102/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.9682 - acc: 1.0000 - val_loss: 18.2881 - val_acc: 0.2400\n",
      "Epoch 103/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.9418 - acc: 1.0000 - val_loss: 18.3099 - val_acc: 0.1700\n",
      "Epoch 104/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.9262 - acc: 0.9950 - val_loss: 18.5048 - val_acc: 0.1400\n",
      "Epoch 105/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.9047 - acc: 0.9950 - val_loss: 18.5416 - val_acc: 0.1200\n",
      "Epoch 106/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.8766 - acc: 1.0000 - val_loss: 18.5136 - val_acc: 0.1200\n",
      "Epoch 107/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.8554 - acc: 1.0000 - val_loss: 18.4801 - val_acc: 0.1400\n",
      "Epoch 108/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.8351 - acc: 1.0000 - val_loss: 18.5596 - val_acc: 0.1500\n",
      "Epoch 109/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.8121 - acc: 1.0000 - val_loss: 18.6203 - val_acc: 0.1500\n",
      "Epoch 110/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.7916 - acc: 1.0000 - val_loss: 18.5681 - val_acc: 0.1600\n",
      "Epoch 111/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.7715 - acc: 1.0000 - val_loss: 18.4524 - val_acc: 0.1600\n",
      "Epoch 112/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.7437 - acc: 1.0000 - val_loss: 18.4383 - val_acc: 0.1700\n",
      "Epoch 113/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.7222 - acc: 1.0000 - val_loss: 18.4938 - val_acc: 0.1500\n",
      "Epoch 114/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.7119 - acc: 0.9950 - val_loss: 18.3995 - val_acc: 0.1600\n",
      "Epoch 115/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.6864 - acc: 0.9950 - val_loss: 18.1462 - val_acc: 0.1300\n",
      "Epoch 116/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.6649 - acc: 1.0000 - val_loss: 17.9636 - val_acc: 0.1800\n",
      "Epoch 117/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.6454 - acc: 1.0000 - val_loss: 17.8004 - val_acc: 0.1700\n",
      "Epoch 118/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.6411 - acc: 0.9950 - val_loss: 18.0329 - val_acc: 0.1400\n",
      "Epoch 119/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.6033 - acc: 1.0000 - val_loss: 18.0526 - val_acc: 0.1300\n",
      "Epoch 120/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.5953 - acc: 0.9950 - val_loss: 18.2072 - val_acc: 0.1400\n",
      "Epoch 121/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.5659 - acc: 0.9950 - val_loss: 18.2306 - val_acc: 0.1600\n",
      "Epoch 122/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.5404 - acc: 1.0000 - val_loss: 18.1778 - val_acc: 0.1500\n",
      "Epoch 123/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.5148 - acc: 1.0000 - val_loss: 18.0570 - val_acc: 0.1200\n",
      "Epoch 124/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.4947 - acc: 1.0000 - val_loss: 18.1327 - val_acc: 0.1200\n",
      "Epoch 125/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.4697 - acc: 1.0000 - val_loss: 18.2137 - val_acc: 0.1300\n",
      "Epoch 126/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.4489 - acc: 1.0000 - val_loss: 18.1843 - val_acc: 0.1400\n",
      "Epoch 127/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.4230 - acc: 1.0000 - val_loss: 17.9014 - val_acc: 0.1600\n",
      "Epoch 128/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.4005 - acc: 1.0000 - val_loss: 17.7207 - val_acc: 0.1700\n",
      "Epoch 129/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.3777 - acc: 1.0000 - val_loss: 17.6652 - val_acc: 0.1700\n",
      "Epoch 130/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.3559 - acc: 1.0000 - val_loss: 17.7861 - val_acc: 0.1400\n",
      "Epoch 131/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.3333 - acc: 1.0000 - val_loss: 17.8169 - val_acc: 0.1500\n",
      "Epoch 132/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.3093 - acc: 1.0000 - val_loss: 18.1696 - val_acc: 0.1300\n",
      "Epoch 133/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.2843 - acc: 1.0000 - val_loss: 17.9972 - val_acc: 0.1500\n",
      "Epoch 134/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.2612 - acc: 1.0000 - val_loss: 18.2171 - val_acc: 0.1300\n",
      "Epoch 135/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.2381 - acc: 1.0000 - val_loss: 18.1177 - val_acc: 0.1300\n",
      "Epoch 136/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.2152 - acc: 1.0000 - val_loss: 18.1673 - val_acc: 0.1200\n",
      "Epoch 137/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.1918 - acc: 1.0000 - val_loss: 18.1236 - val_acc: 0.1400\n",
      "Epoch 138/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.1671 - acc: 1.0000 - val_loss: 18.0581 - val_acc: 0.1500\n",
      "Epoch 139/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.1436 - acc: 1.0000 - val_loss: 17.9574 - val_acc: 0.1500\n",
      "Epoch 140/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.1223 - acc: 1.0000 - val_loss: 17.9196 - val_acc: 0.1600\n",
      "Epoch 141/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.0966 - acc: 1.0000 - val_loss: 17.9759 - val_acc: 0.1500\n",
      "Epoch 142/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.0752 - acc: 1.0000 - val_loss: 17.9539 - val_acc: 0.1500\n",
      "Epoch 143/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.0494 - acc: 1.0000 - val_loss: 18.0105 - val_acc: 0.1500\n",
      "Epoch 144/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.0263 - acc: 1.0000 - val_loss: 17.9944 - val_acc: 0.1400\n",
      "Epoch 145/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 15.0016 - acc: 1.0000 - val_loss: 17.9339 - val_acc: 0.1600\n",
      "Epoch 146/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.9783 - acc: 1.0000 - val_loss: 17.9706 - val_acc: 0.1500\n",
      "Epoch 147/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.9553 - acc: 1.0000 - val_loss: 17.9798 - val_acc: 0.1400\n",
      "Epoch 148/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.9315 - acc: 1.0000 - val_loss: 17.8616 - val_acc: 0.1700\n",
      "Epoch 149/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.9063 - acc: 1.0000 - val_loss: 17.7429 - val_acc: 0.1600\n",
      "Epoch 150/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.8866 - acc: 1.0000 - val_loss: 17.6553 - val_acc: 0.1700\n",
      "Epoch 151/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.8585 - acc: 1.0000 - val_loss: 17.4919 - val_acc: 0.1700\n",
      "Epoch 152/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.8365 - acc: 1.0000 - val_loss: 17.4226 - val_acc: 0.1800\n",
      "Epoch 153/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.8169 - acc: 1.0000 - val_loss: 17.6042 - val_acc: 0.1800\n",
      "Epoch 154/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.7896 - acc: 1.0000 - val_loss: 17.8088 - val_acc: 0.1200\n",
      "Epoch 155/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.7657 - acc: 1.0000 - val_loss: 18.0326 - val_acc: 0.1000\n",
      "Epoch 156/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.7389 - acc: 1.0000 - val_loss: 17.8665 - val_acc: 0.1000\n",
      "Epoch 157/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.7166 - acc: 1.0000 - val_loss: 17.7314 - val_acc: 0.1100\n",
      "Epoch 158/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.6939 - acc: 1.0000 - val_loss: 17.3810 - val_acc: 0.1400\n",
      "Epoch 159/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.6829 - acc: 0.9900 - val_loss: 17.3179 - val_acc: 0.1800\n",
      "Epoch 160/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.6464 - acc: 1.0000 - val_loss: 17.0473 - val_acc: 0.1800\n",
      "Epoch 161/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.6244 - acc: 1.0000 - val_loss: 16.9248 - val_acc: 0.1500\n",
      "Epoch 162/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.6036 - acc: 1.0000 - val_loss: 17.0022 - val_acc: 0.1300\n",
      "Epoch 163/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.5805 - acc: 1.0000 - val_loss: 17.1513 - val_acc: 0.1400\n",
      "Epoch 164/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.5630 - acc: 0.9950 - val_loss: 17.1273 - val_acc: 0.1300\n",
      "Epoch 165/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.5496 - acc: 0.9900 - val_loss: 17.1965 - val_acc: 0.1200\n",
      "Epoch 166/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.5148 - acc: 1.0000 - val_loss: 16.9013 - val_acc: 0.1200\n",
      "Epoch 167/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.4981 - acc: 1.0000 - val_loss: 17.1185 - val_acc: 0.1000\n",
      "Epoch 168/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.4767 - acc: 1.0000 - val_loss: 17.2031 - val_acc: 0.1100\n",
      "Epoch 169/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.4464 - acc: 1.0000 - val_loss: 17.3545 - val_acc: 0.0900\n",
      "Epoch 170/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.4268 - acc: 1.0000 - val_loss: 17.4446 - val_acc: 0.1200\n",
      "Epoch 171/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.3973 - acc: 1.0000 - val_loss: 17.2201 - val_acc: 0.1500\n",
      "Epoch 172/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.3697 - acc: 1.0000 - val_loss: 17.2386 - val_acc: 0.1400\n",
      "Epoch 173/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.3448 - acc: 1.0000 - val_loss: 17.5552 - val_acc: 0.1100\n",
      "Epoch 174/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.3172 - acc: 1.0000 - val_loss: 17.5879 - val_acc: 0.1000\n",
      "Epoch 175/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.2940 - acc: 1.0000 - val_loss: 17.6984 - val_acc: 0.0900\n",
      "Epoch 176/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.2672 - acc: 1.0000 - val_loss: 18.0087 - val_acc: 0.0800\n",
      "Epoch 177/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.2455 - acc: 1.0000 - val_loss: 17.7636 - val_acc: 0.0800\n",
      "Epoch 178/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.2263 - acc: 0.9950 - val_loss: 17.6435 - val_acc: 0.1100\n",
      "Epoch 179/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.1966 - acc: 1.0000 - val_loss: 17.2343 - val_acc: 0.1900\n",
      "Epoch 180/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.1910 - acc: 0.9900 - val_loss: 17.2807 - val_acc: 0.1800\n",
      "Epoch 181/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.1553 - acc: 0.9950 - val_loss: 17.1163 - val_acc: 0.1600\n",
      "Epoch 182/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.1280 - acc: 1.0000 - val_loss: 16.7447 - val_acc: 0.1500\n",
      "Epoch 183/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.1069 - acc: 1.0000 - val_loss: 16.6174 - val_acc: 0.1500\n",
      "Epoch 184/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.0813 - acc: 1.0000 - val_loss: 16.5983 - val_acc: 0.1600\n",
      "Epoch 185/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.0545 - acc: 1.0000 - val_loss: 16.5974 - val_acc: 0.1400\n",
      "Epoch 186/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.0367 - acc: 0.9950 - val_loss: 16.4968 - val_acc: 0.1900\n",
      "Epoch 187/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 14.0179 - acc: 0.9950 - val_loss: 16.5293 - val_acc: 0.1500\n",
      "Epoch 188/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.9881 - acc: 0.9950 - val_loss: 16.7967 - val_acc: 0.1300\n",
      "Epoch 189/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.9599 - acc: 1.0000 - val_loss: 16.6620 - val_acc: 0.1400\n",
      "Epoch 190/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.9373 - acc: 1.0000 - val_loss: 16.5095 - val_acc: 0.1600\n",
      "Epoch 191/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.9263 - acc: 1.0000 - val_loss: 16.3062 - val_acc: 0.1600\n",
      "Epoch 192/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.8984 - acc: 1.0000 - val_loss: 15.9171 - val_acc: 0.2200\n",
      "Epoch 193/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.9022 - acc: 0.9900 - val_loss: 16.1748 - val_acc: 0.1500\n",
      "Epoch 194/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.8766 - acc: 0.9850 - val_loss: 16.3001 - val_acc: 0.1300\n",
      "Epoch 195/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.8378 - acc: 1.0000 - val_loss: 15.9265 - val_acc: 0.2100\n",
      "Epoch 196/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.8192 - acc: 0.9950 - val_loss: 15.8673 - val_acc: 0.2100\n",
      "Epoch 197/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.7925 - acc: 1.0000 - val_loss: 16.1699 - val_acc: 0.1500\n",
      "Epoch 198/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.7748 - acc: 0.9900 - val_loss: 16.1756 - val_acc: 0.1600\n",
      "Epoch 199/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.7456 - acc: 1.0000 - val_loss: 15.6202 - val_acc: 0.2700\n",
      "Epoch 200/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.7200 - acc: 1.0000 - val_loss: 15.5737 - val_acc: 0.2800\n",
      "Epoch 201/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.7034 - acc: 0.9950 - val_loss: 15.9014 - val_acc: 0.2000\n",
      "Epoch 202/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.6797 - acc: 0.9950 - val_loss: 16.0952 - val_acc: 0.2000\n",
      "Epoch 203/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.6528 - acc: 1.0000 - val_loss: 16.1898 - val_acc: 0.1900\n",
      "Epoch 204/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.6428 - acc: 0.9950 - val_loss: 15.9704 - val_acc: 0.1900\n",
      "Epoch 205/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.6045 - acc: 1.0000 - val_loss: 15.9122 - val_acc: 0.2200\n",
      "Epoch 206/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.5778 - acc: 1.0000 - val_loss: 16.1990 - val_acc: 0.2200\n",
      "Epoch 207/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.5556 - acc: 1.0000 - val_loss: 16.4950 - val_acc: 0.1800\n",
      "Epoch 208/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.5314 - acc: 1.0000 - val_loss: 16.5279 - val_acc: 0.1800\n",
      "Epoch 209/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.5113 - acc: 0.9950 - val_loss: 16.4399 - val_acc: 0.1800\n",
      "Epoch 210/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.4825 - acc: 1.0000 - val_loss: 16.3479 - val_acc: 0.1800\n",
      "Epoch 211/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.4600 - acc: 1.0000 - val_loss: 16.1202 - val_acc: 0.2300\n",
      "Epoch 212/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.4399 - acc: 0.9950 - val_loss: 16.0415 - val_acc: 0.2300\n",
      "Epoch 213/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.4153 - acc: 0.9950 - val_loss: 16.2241 - val_acc: 0.2200\n",
      "Epoch 214/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.3917 - acc: 1.0000 - val_loss: 16.2411 - val_acc: 0.2100\n",
      "Epoch 215/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.3644 - acc: 1.0000 - val_loss: 16.5720 - val_acc: 0.2000\n",
      "Epoch 216/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.3449 - acc: 1.0000 - val_loss: 16.4889 - val_acc: 0.1800\n",
      "Epoch 217/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.3188 - acc: 1.0000 - val_loss: 16.5012 - val_acc: 0.2200\n",
      "Epoch 218/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.3002 - acc: 0.9950 - val_loss: 16.2827 - val_acc: 0.2200\n",
      "Epoch 219/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.2746 - acc: 1.0000 - val_loss: 16.2252 - val_acc: 0.2200\n",
      "Epoch 220/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.2531 - acc: 1.0000 - val_loss: 16.1857 - val_acc: 0.2500\n",
      "Epoch 221/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.2245 - acc: 1.0000 - val_loss: 15.7734 - val_acc: 0.2400\n",
      "Epoch 222/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.2028 - acc: 1.0000 - val_loss: 15.7710 - val_acc: 0.2500\n",
      "Epoch 223/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.1773 - acc: 1.0000 - val_loss: 15.9595 - val_acc: 0.2600\n",
      "Epoch 224/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.1621 - acc: 0.9950 - val_loss: 16.1203 - val_acc: 0.2300\n",
      "Epoch 225/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.1295 - acc: 1.0000 - val_loss: 16.4341 - val_acc: 0.2100\n",
      "Epoch 226/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.1065 - acc: 1.0000 - val_loss: 16.3834 - val_acc: 0.2200\n",
      "Epoch 227/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.0833 - acc: 1.0000 - val_loss: 16.3386 - val_acc: 0.2300\n",
      "Epoch 228/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.0577 - acc: 1.0000 - val_loss: 16.0568 - val_acc: 0.2100\n",
      "Epoch 229/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.0362 - acc: 1.0000 - val_loss: 16.0815 - val_acc: 0.2200\n",
      "Epoch 230/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 13.0119 - acc: 1.0000 - val_loss: 16.1153 - val_acc: 0.2000\n",
      "Epoch 231/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.9862 - acc: 1.0000 - val_loss: 16.2213 - val_acc: 0.1900\n",
      "Epoch 232/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.9617 - acc: 1.0000 - val_loss: 16.2532 - val_acc: 0.2000\n",
      "Epoch 233/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.9382 - acc: 1.0000 - val_loss: 16.2202 - val_acc: 0.1900\n",
      "Epoch 234/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.9143 - acc: 1.0000 - val_loss: 16.2293 - val_acc: 0.2100\n",
      "Epoch 235/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.8911 - acc: 1.0000 - val_loss: 16.4009 - val_acc: 0.2200\n",
      "Epoch 236/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.8685 - acc: 1.0000 - val_loss: 16.2840 - val_acc: 0.2300\n",
      "Epoch 237/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.8423 - acc: 1.0000 - val_loss: 16.6135 - val_acc: 0.1800\n",
      "Epoch 238/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.8196 - acc: 1.0000 - val_loss: 16.5476 - val_acc: 0.1800\n",
      "Epoch 239/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.7986 - acc: 1.0000 - val_loss: 16.5193 - val_acc: 0.1900\n",
      "Epoch 240/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.7758 - acc: 0.9950 - val_loss: 16.2976 - val_acc: 0.2200\n",
      "Epoch 241/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.7472 - acc: 1.0000 - val_loss: 15.5192 - val_acc: 0.2500\n",
      "Epoch 242/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.7323 - acc: 0.9950 - val_loss: 15.4163 - val_acc: 0.2300\n",
      "Epoch 243/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.7089 - acc: 0.9950 - val_loss: 15.3453 - val_acc: 0.2200\n",
      "Epoch 244/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.6823 - acc: 1.0000 - val_loss: 15.5834 - val_acc: 0.1900\n",
      "Epoch 245/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.6808 - acc: 0.9950 - val_loss: 15.6691 - val_acc: 0.1800\n",
      "Epoch 246/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.6381 - acc: 0.9950 - val_loss: 15.7379 - val_acc: 0.1700\n",
      "Epoch 247/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.6234 - acc: 0.9950 - val_loss: 15.7737 - val_acc: 0.1400\n",
      "Epoch 248/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.6055 - acc: 0.9950 - val_loss: 15.9603 - val_acc: 0.1500\n",
      "Epoch 249/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.5817 - acc: 0.9950 - val_loss: 16.1455 - val_acc: 0.1500\n",
      "Epoch 250/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.5512 - acc: 1.0000 - val_loss: 15.7450 - val_acc: 0.2100\n",
      "Epoch 251/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.5273 - acc: 1.0000 - val_loss: 15.8637 - val_acc: 0.2100\n",
      "Epoch 252/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.5009 - acc: 1.0000 - val_loss: 16.1299 - val_acc: 0.1700\n",
      "Epoch 253/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.4779 - acc: 1.0000 - val_loss: 16.2350 - val_acc: 0.1600\n",
      "Epoch 254/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.4503 - acc: 1.0000 - val_loss: 16.2547 - val_acc: 0.1800\n",
      "Epoch 255/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.4318 - acc: 1.0000 - val_loss: 16.0477 - val_acc: 0.2000\n",
      "Epoch 256/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.4048 - acc: 1.0000 - val_loss: 15.8328 - val_acc: 0.2300\n",
      "Epoch 257/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.3850 - acc: 1.0000 - val_loss: 15.6653 - val_acc: 0.2100\n",
      "Epoch 258/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.3568 - acc: 1.0000 - val_loss: 15.9181 - val_acc: 0.1900\n",
      "Epoch 259/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.3320 - acc: 1.0000 - val_loss: 15.9729 - val_acc: 0.1900\n",
      "Epoch 260/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.3081 - acc: 1.0000 - val_loss: 15.9871 - val_acc: 0.1600\n",
      "Epoch 261/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.2940 - acc: 0.9950 - val_loss: 15.9678 - val_acc: 0.1600\n",
      "Epoch 262/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.2617 - acc: 1.0000 - val_loss: 15.6632 - val_acc: 0.2000\n",
      "Epoch 263/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.2415 - acc: 1.0000 - val_loss: 15.4406 - val_acc: 0.2200\n",
      "Epoch 264/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.2216 - acc: 1.0000 - val_loss: 15.5141 - val_acc: 0.1500\n",
      "Epoch 265/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.1977 - acc: 1.0000 - val_loss: 15.4301 - val_acc: 0.1700\n",
      "Epoch 266/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.1764 - acc: 1.0000 - val_loss: 15.3425 - val_acc: 0.1600\n",
      "Epoch 267/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.1527 - acc: 1.0000 - val_loss: 15.2423 - val_acc: 0.1500\n",
      "Epoch 268/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.1323 - acc: 1.0000 - val_loss: 15.3115 - val_acc: 0.1500\n",
      "Epoch 269/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.1032 - acc: 1.0000 - val_loss: 15.4476 - val_acc: 0.1600\n",
      "Epoch 270/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.0802 - acc: 1.0000 - val_loss: 15.5869 - val_acc: 0.1600\n",
      "Epoch 271/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.0582 - acc: 1.0000 - val_loss: 15.5833 - val_acc: 0.1300\n",
      "Epoch 272/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.0350 - acc: 1.0000 - val_loss: 15.5390 - val_acc: 0.1800\n",
      "Epoch 273/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 12.0099 - acc: 1.0000 - val_loss: 15.1371 - val_acc: 0.2500\n",
      "Epoch 274/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.9979 - acc: 0.9950 - val_loss: 15.1608 - val_acc: 0.2300\n",
      "Epoch 275/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.9691 - acc: 0.9950 - val_loss: 15.3556 - val_acc: 0.2100\n",
      "Epoch 276/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.9418 - acc: 1.0000 - val_loss: 15.3896 - val_acc: 0.1900\n",
      "Epoch 277/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.9174 - acc: 1.0000 - val_loss: 15.2270 - val_acc: 0.1900\n",
      "Epoch 278/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.8947 - acc: 1.0000 - val_loss: 15.1879 - val_acc: 0.1800\n",
      "Epoch 279/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.8692 - acc: 1.0000 - val_loss: 15.2565 - val_acc: 0.1800\n",
      "Epoch 280/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.8480 - acc: 1.0000 - val_loss: 15.3063 - val_acc: 0.1800\n",
      "Epoch 281/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.8222 - acc: 1.0000 - val_loss: 15.4220 - val_acc: 0.1900\n",
      "Epoch 282/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.8004 - acc: 1.0000 - val_loss: 15.5091 - val_acc: 0.1800\n",
      "Epoch 283/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.7764 - acc: 1.0000 - val_loss: 15.2198 - val_acc: 0.1800\n",
      "Epoch 284/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.7518 - acc: 1.0000 - val_loss: 15.0931 - val_acc: 0.1800\n",
      "Epoch 285/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.7323 - acc: 1.0000 - val_loss: 15.0267 - val_acc: 0.1700\n",
      "Epoch 286/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.7047 - acc: 1.0000 - val_loss: 14.9812 - val_acc: 0.2000\n",
      "Epoch 287/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.6815 - acc: 1.0000 - val_loss: 14.9993 - val_acc: 0.2000\n",
      "Epoch 288/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.6571 - acc: 1.0000 - val_loss: 14.9018 - val_acc: 0.1900\n",
      "Epoch 289/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.6370 - acc: 1.0000 - val_loss: 14.8807 - val_acc: 0.2200\n",
      "Epoch 290/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.6141 - acc: 0.9950 - val_loss: 14.3890 - val_acc: 0.2400\n",
      "Epoch 291/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.5882 - acc: 1.0000 - val_loss: 14.3639 - val_acc: 0.2500\n",
      "Epoch 292/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.5643 - acc: 1.0000 - val_loss: 14.5235 - val_acc: 0.2600\n",
      "Epoch 293/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.5400 - acc: 1.0000 - val_loss: 14.5516 - val_acc: 0.2400\n",
      "Epoch 294/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.5169 - acc: 1.0000 - val_loss: 14.6430 - val_acc: 0.2300\n",
      "Epoch 295/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.4922 - acc: 1.0000 - val_loss: 14.8178 - val_acc: 0.2000\n",
      "Epoch 296/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.4681 - acc: 1.0000 - val_loss: 14.9749 - val_acc: 0.2400\n",
      "Epoch 297/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.4447 - acc: 1.0000 - val_loss: 14.8502 - val_acc: 0.2100\n",
      "Epoch 298/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.4211 - acc: 1.0000 - val_loss: 14.9013 - val_acc: 0.2200\n",
      "Epoch 299/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.3970 - acc: 1.0000 - val_loss: 14.7535 - val_acc: 0.2100\n",
      "Epoch 300/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.3731 - acc: 1.0000 - val_loss: 14.7972 - val_acc: 0.2200\n",
      "Epoch 301/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.3495 - acc: 1.0000 - val_loss: 14.7822 - val_acc: 0.2400\n",
      "Epoch 302/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.3253 - acc: 1.0000 - val_loss: 14.7937 - val_acc: 0.2300\n",
      "Epoch 303/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.3011 - acc: 1.0000 - val_loss: 14.8649 - val_acc: 0.2600\n",
      "Epoch 304/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.2770 - acc: 1.0000 - val_loss: 14.9199 - val_acc: 0.2600\n",
      "Epoch 305/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.2536 - acc: 1.0000 - val_loss: 14.9678 - val_acc: 0.2500\n",
      "Epoch 306/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.2290 - acc: 1.0000 - val_loss: 14.8193 - val_acc: 0.2700\n",
      "Epoch 307/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.2059 - acc: 1.0000 - val_loss: 14.6420 - val_acc: 0.2800\n",
      "Epoch 308/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.1813 - acc: 1.0000 - val_loss: 14.6199 - val_acc: 0.2600\n",
      "Epoch 309/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.1567 - acc: 1.0000 - val_loss: 14.7256 - val_acc: 0.2700\n",
      "Epoch 310/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.1325 - acc: 1.0000 - val_loss: 14.6753 - val_acc: 0.2400\n",
      "Epoch 311/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.1087 - acc: 1.0000 - val_loss: 14.6947 - val_acc: 0.2400\n",
      "Epoch 312/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.0841 - acc: 1.0000 - val_loss: 14.9715 - val_acc: 0.2400\n",
      "Epoch 313/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.0602 - acc: 1.0000 - val_loss: 14.8224 - val_acc: 0.2400\n",
      "Epoch 314/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.0355 - acc: 1.0000 - val_loss: 14.6963 - val_acc: 0.2300\n",
      "Epoch 315/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 11.0113 - acc: 1.0000 - val_loss: 14.7972 - val_acc: 0.2400\n",
      "Epoch 316/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.9871 - acc: 1.0000 - val_loss: 14.5877 - val_acc: 0.2400\n",
      "Epoch 317/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.9627 - acc: 1.0000 - val_loss: 14.6122 - val_acc: 0.2500\n",
      "Epoch 318/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.9382 - acc: 1.0000 - val_loss: 14.6286 - val_acc: 0.2400\n",
      "Epoch 319/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.9140 - acc: 1.0000 - val_loss: 14.6507 - val_acc: 0.2500\n",
      "Epoch 320/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.8895 - acc: 1.0000 - val_loss: 14.8130 - val_acc: 0.2300\n",
      "Epoch 321/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.8651 - acc: 1.0000 - val_loss: 14.5694 - val_acc: 0.2500\n",
      "Epoch 322/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.8407 - acc: 1.0000 - val_loss: 14.4944 - val_acc: 0.2500\n",
      "Epoch 323/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.8161 - acc: 1.0000 - val_loss: 14.6922 - val_acc: 0.2300\n",
      "Epoch 324/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.7918 - acc: 1.0000 - val_loss: 14.5503 - val_acc: 0.2300\n",
      "Epoch 325/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.7674 - acc: 1.0000 - val_loss: 14.4944 - val_acc: 0.2500\n",
      "Epoch 326/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.7429 - acc: 1.0000 - val_loss: 14.7314 - val_acc: 0.2300\n",
      "Epoch 327/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.7181 - acc: 1.0000 - val_loss: 14.7327 - val_acc: 0.2300\n",
      "Epoch 328/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.6935 - acc: 1.0000 - val_loss: 14.4657 - val_acc: 0.2400\n",
      "Epoch 329/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.6690 - acc: 1.0000 - val_loss: 14.4963 - val_acc: 0.2400\n",
      "Epoch 330/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.6449 - acc: 1.0000 - val_loss: 14.5970 - val_acc: 0.2400\n",
      "Epoch 331/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.6199 - acc: 1.0000 - val_loss: 14.4551 - val_acc: 0.2500\n",
      "Epoch 332/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.5954 - acc: 1.0000 - val_loss: 14.3692 - val_acc: 0.2600\n",
      "Epoch 333/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.5707 - acc: 1.0000 - val_loss: 14.3428 - val_acc: 0.2500\n",
      "Epoch 334/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.5463 - acc: 1.0000 - val_loss: 14.1222 - val_acc: 0.2400\n",
      "Epoch 335/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.5218 - acc: 1.0000 - val_loss: 14.1443 - val_acc: 0.2100\n",
      "Epoch 336/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.4968 - acc: 1.0000 - val_loss: 14.1646 - val_acc: 0.2300\n",
      "Epoch 337/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.4723 - acc: 1.0000 - val_loss: 14.1804 - val_acc: 0.2300\n",
      "Epoch 338/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.4473 - acc: 1.0000 - val_loss: 14.0435 - val_acc: 0.2100\n",
      "Epoch 339/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.4226 - acc: 1.0000 - val_loss: 13.9014 - val_acc: 0.2300\n",
      "Epoch 340/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.4003 - acc: 1.0000 - val_loss: 13.8783 - val_acc: 0.2700\n",
      "Epoch 341/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.3737 - acc: 1.0000 - val_loss: 13.5406 - val_acc: 0.2800\n",
      "Epoch 342/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.3519 - acc: 1.0000 - val_loss: 13.5235 - val_acc: 0.2800\n",
      "Epoch 343/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.3304 - acc: 1.0000 - val_loss: 13.6321 - val_acc: 0.2600\n",
      "Epoch 344/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.3026 - acc: 1.0000 - val_loss: 13.5560 - val_acc: 0.2300\n",
      "Epoch 345/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.2775 - acc: 1.0000 - val_loss: 13.9036 - val_acc: 0.2200\n",
      "Epoch 346/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.2535 - acc: 1.0000 - val_loss: 13.8190 - val_acc: 0.2200\n",
      "Epoch 347/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.2282 - acc: 1.0000 - val_loss: 13.8745 - val_acc: 0.1800\n",
      "Epoch 348/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.2035 - acc: 1.0000 - val_loss: 13.7153 - val_acc: 0.1600\n",
      "Epoch 349/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.1794 - acc: 1.0000 - val_loss: 13.8206 - val_acc: 0.1600\n",
      "Epoch 350/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.1541 - acc: 1.0000 - val_loss: 13.7279 - val_acc: 0.2000\n",
      "Epoch 351/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.1308 - acc: 1.0000 - val_loss: 13.6102 - val_acc: 0.2000\n",
      "Epoch 352/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.1062 - acc: 1.0000 - val_loss: 13.7110 - val_acc: 0.1900\n",
      "Epoch 353/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.0807 - acc: 1.0000 - val_loss: 13.6864 - val_acc: 0.2000\n",
      "Epoch 354/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.0570 - acc: 1.0000 - val_loss: 13.7964 - val_acc: 0.1900\n",
      "Epoch 355/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.0314 - acc: 1.0000 - val_loss: 13.5653 - val_acc: 0.2000\n",
      "Epoch 356/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.0068 - acc: 1.0000 - val_loss: 13.6398 - val_acc: 0.2100\n",
      "Epoch 357/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.9825 - acc: 1.0000 - val_loss: 13.5524 - val_acc: 0.2000\n",
      "Epoch 358/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.9573 - acc: 1.0000 - val_loss: 13.4673 - val_acc: 0.2000\n",
      "Epoch 359/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.9334 - acc: 1.0000 - val_loss: 13.6100 - val_acc: 0.2000\n",
      "Epoch 360/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.9079 - acc: 1.0000 - val_loss: 13.3139 - val_acc: 0.2500\n",
      "Epoch 361/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.8832 - acc: 1.0000 - val_loss: 13.2493 - val_acc: 0.2600\n",
      "Epoch 362/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.8590 - acc: 1.0000 - val_loss: 13.2365 - val_acc: 0.2400\n",
      "Epoch 363/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.8340 - acc: 1.0000 - val_loss: 13.3538 - val_acc: 0.2200\n",
      "Epoch 364/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.8094 - acc: 1.0000 - val_loss: 13.4339 - val_acc: 0.2300\n",
      "Epoch 365/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.7846 - acc: 1.0000 - val_loss: 13.3640 - val_acc: 0.2200\n",
      "Epoch 366/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.7600 - acc: 1.0000 - val_loss: 13.4808 - val_acc: 0.2400\n",
      "Epoch 367/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.7351 - acc: 1.0000 - val_loss: 13.4577 - val_acc: 0.2500\n",
      "Epoch 368/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.7102 - acc: 1.0000 - val_loss: 13.4165 - val_acc: 0.2600\n",
      "Epoch 369/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.6867 - acc: 1.0000 - val_loss: 13.4305 - val_acc: 0.2500\n",
      "Epoch 370/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.6607 - acc: 1.0000 - val_loss: 13.4197 - val_acc: 0.2500\n",
      "Epoch 371/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.6357 - acc: 1.0000 - val_loss: 13.3949 - val_acc: 0.2600\n",
      "Epoch 372/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.6114 - acc: 1.0000 - val_loss: 13.4224 - val_acc: 0.2400\n",
      "Epoch 373/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.5862 - acc: 1.0000 - val_loss: 13.6434 - val_acc: 0.2400\n",
      "Epoch 374/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.5668 - acc: 0.9950 - val_loss: 13.3381 - val_acc: 0.2400\n",
      "Epoch 375/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.5427 - acc: 1.0000 - val_loss: 13.0004 - val_acc: 0.3500\n",
      "Epoch 376/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.5216 - acc: 1.0000 - val_loss: 13.4945 - val_acc: 0.2700\n",
      "Epoch 377/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.5347 - acc: 0.9900 - val_loss: 14.2726 - val_acc: 0.1300\n",
      "Epoch 378/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.4719 - acc: 1.0000 - val_loss: 14.1995 - val_acc: 0.1600\n",
      "Epoch 379/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.4509 - acc: 1.0000 - val_loss: 13.4929 - val_acc: 0.1700\n",
      "Epoch 380/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.4225 - acc: 1.0000 - val_loss: 13.3592 - val_acc: 0.2100\n",
      "Epoch 381/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.3993 - acc: 1.0000 - val_loss: 13.2996 - val_acc: 0.1600\n",
      "Epoch 382/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.3746 - acc: 1.0000 - val_loss: 13.4986 - val_acc: 0.1500\n",
      "Epoch 383/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.3519 - acc: 1.0000 - val_loss: 13.6358 - val_acc: 0.1400\n",
      "Epoch 384/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.3268 - acc: 1.0000 - val_loss: 13.1438 - val_acc: 0.1400\n",
      "Epoch 385/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.3029 - acc: 1.0000 - val_loss: 12.5447 - val_acc: 0.2300\n",
      "Epoch 386/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.2782 - acc: 1.0000 - val_loss: 13.2483 - val_acc: 0.1700\n",
      "Epoch 387/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.2575 - acc: 1.0000 - val_loss: 13.5224 - val_acc: 0.1600\n",
      "Epoch 388/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.2407 - acc: 0.9950 - val_loss: 11.8202 - val_acc: 0.3000\n",
      "Epoch 389/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.2105 - acc: 1.0000 - val_loss: 11.6012 - val_acc: 0.3400\n",
      "Epoch 390/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.1905 - acc: 0.9950 - val_loss: 11.9581 - val_acc: 0.2700\n",
      "Epoch 391/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.1611 - acc: 1.0000 - val_loss: 12.1882 - val_acc: 0.2600\n",
      "Epoch 392/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.1374 - acc: 1.0000 - val_loss: 12.0201 - val_acc: 0.2800\n",
      "Epoch 393/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.1121 - acc: 1.0000 - val_loss: 12.1474 - val_acc: 0.2600\n",
      "Epoch 394/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.0900 - acc: 1.0000 - val_loss: 12.2817 - val_acc: 0.2600\n",
      "Epoch 395/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.0646 - acc: 1.0000 - val_loss: 12.2919 - val_acc: 0.2300\n",
      "Epoch 396/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.0402 - acc: 1.0000 - val_loss: 12.3407 - val_acc: 0.2300\n",
      "Epoch 397/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 9.0168 - acc: 1.0000 - val_loss: 12.3560 - val_acc: 0.2400\n",
      "Epoch 398/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.9933 - acc: 1.0000 - val_loss: 12.3681 - val_acc: 0.2800\n",
      "Epoch 399/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.9698 - acc: 1.0000 - val_loss: 11.7217 - val_acc: 0.3200\n",
      "Epoch 400/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.9449 - acc: 1.0000 - val_loss: 11.5507 - val_acc: 0.3400\n",
      "Epoch 401/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.9329 - acc: 0.9950 - val_loss: 12.1313 - val_acc: 0.2200\n",
      "Epoch 402/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.9006 - acc: 1.0000 - val_loss: 12.2545 - val_acc: 0.2200\n",
      "Epoch 403/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.8744 - acc: 1.0000 - val_loss: 12.1299 - val_acc: 0.1500\n",
      "Epoch 404/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.8511 - acc: 1.0000 - val_loss: 12.0695 - val_acc: 0.1200\n",
      "Epoch 405/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.8283 - acc: 1.0000 - val_loss: 12.0688 - val_acc: 0.1900\n",
      "Epoch 406/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.8170 - acc: 0.9950 - val_loss: 12.0559 - val_acc: 0.1800\n",
      "Epoch 407/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.7808 - acc: 1.0000 - val_loss: 12.0194 - val_acc: 0.1800\n",
      "Epoch 408/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.7587 - acc: 1.0000 - val_loss: 12.2821 - val_acc: 0.2000\n",
      "Epoch 409/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.7350 - acc: 1.0000 - val_loss: 12.1174 - val_acc: 0.1800\n",
      "Epoch 410/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.7114 - acc: 1.0000 - val_loss: 12.4047 - val_acc: 0.2200\n",
      "Epoch 411/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.6880 - acc: 1.0000 - val_loss: 12.2053 - val_acc: 0.1700\n",
      "Epoch 412/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.6638 - acc: 1.0000 - val_loss: 12.1832 - val_acc: 0.2000\n",
      "Epoch 413/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.6404 - acc: 1.0000 - val_loss: 12.2091 - val_acc: 0.1800\n",
      "Epoch 414/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.6164 - acc: 1.0000 - val_loss: 12.2843 - val_acc: 0.1900\n",
      "Epoch 415/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.5928 - acc: 1.0000 - val_loss: 12.2908 - val_acc: 0.1900\n",
      "Epoch 416/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.5699 - acc: 1.0000 - val_loss: 12.1062 - val_acc: 0.2000\n",
      "Epoch 417/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.5456 - acc: 1.0000 - val_loss: 12.1468 - val_acc: 0.2000\n",
      "Epoch 418/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.5331 - acc: 0.9950 - val_loss: 11.5914 - val_acc: 0.2200\n",
      "Epoch 419/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.5024 - acc: 1.0000 - val_loss: 10.6845 - val_acc: 0.3400\n",
      "Epoch 420/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.5423 - acc: 0.9800 - val_loss: 10.8941 - val_acc: 0.2800\n",
      "Epoch 421/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.5064 - acc: 0.9900 - val_loss: 11.4123 - val_acc: 0.2200\n",
      "Epoch 422/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.4795 - acc: 0.9850 - val_loss: 11.2452 - val_acc: 0.2500\n",
      "Epoch 423/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.4595 - acc: 0.9950 - val_loss: 10.7178 - val_acc: 0.3600\n",
      "Epoch 424/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.4233 - acc: 1.0000 - val_loss: 11.2706 - val_acc: 0.3000\n",
      "Epoch 425/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.4066 - acc: 0.9950 - val_loss: 11.6985 - val_acc: 0.2200\n",
      "Epoch 426/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.4016 - acc: 0.9850 - val_loss: 11.0918 - val_acc: 0.3300\n",
      "Epoch 427/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.3821 - acc: 0.9900 - val_loss: 11.4261 - val_acc: 0.2400\n",
      "Epoch 428/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.3658 - acc: 0.9900 - val_loss: 10.6100 - val_acc: 0.3500\n",
      "Epoch 429/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.3564 - acc: 0.9900 - val_loss: 11.8044 - val_acc: 0.2300\n",
      "Epoch 430/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.3453 - acc: 0.9900 - val_loss: 11.1667 - val_acc: 0.2900\n",
      "Epoch 431/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.3503 - acc: 0.9900 - val_loss: 11.8296 - val_acc: 0.1700\n",
      "Epoch 432/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.2686 - acc: 1.0000 - val_loss: 12.1273 - val_acc: 0.1800\n",
      "Epoch 433/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.2506 - acc: 0.9950 - val_loss: 12.1810 - val_acc: 0.1900\n",
      "Epoch 434/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.2281 - acc: 1.0000 - val_loss: 12.4659 - val_acc: 0.1800\n",
      "Epoch 435/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.2088 - acc: 1.0000 - val_loss: 11.9260 - val_acc: 0.2200\n",
      "Epoch 436/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.1869 - acc: 1.0000 - val_loss: 11.6668 - val_acc: 0.2400\n",
      "Epoch 437/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.1657 - acc: 1.0000 - val_loss: 11.7735 - val_acc: 0.2500\n",
      "Epoch 438/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.1454 - acc: 1.0000 - val_loss: 11.9111 - val_acc: 0.2400\n",
      "Epoch 439/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.1291 - acc: 1.0000 - val_loss: 11.8083 - val_acc: 0.2400\n",
      "Epoch 440/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.1346 - acc: 0.9900 - val_loss: 11.9682 - val_acc: 0.2200\n",
      "Epoch 441/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.0894 - acc: 1.0000 - val_loss: 10.9862 - val_acc: 0.3300\n",
      "Epoch 442/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.0696 - acc: 1.0000 - val_loss: 11.0458 - val_acc: 0.3000\n",
      "Epoch 443/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.0528 - acc: 1.0000 - val_loss: 11.2099 - val_acc: 0.2200\n",
      "Epoch 444/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.0399 - acc: 0.9950 - val_loss: 11.6200 - val_acc: 0.2200\n",
      "Epoch 445/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 8.0220 - acc: 0.9900 - val_loss: 11.5123 - val_acc: 0.2200\n",
      "Epoch 446/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.9898 - acc: 1.0000 - val_loss: 11.5760 - val_acc: 0.2300\n",
      "Epoch 447/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.9741 - acc: 1.0000 - val_loss: 11.8086 - val_acc: 0.2300\n",
      "Epoch 448/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.9509 - acc: 1.0000 - val_loss: 11.8115 - val_acc: 0.2300\n",
      "Epoch 449/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.9310 - acc: 1.0000 - val_loss: 11.7116 - val_acc: 0.2200\n",
      "Epoch 450/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.9112 - acc: 1.0000 - val_loss: 11.6087 - val_acc: 0.2300\n",
      "Epoch 451/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.8919 - acc: 1.0000 - val_loss: 11.5214 - val_acc: 0.2400\n",
      "Epoch 452/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.8708 - acc: 1.0000 - val_loss: 11.6638 - val_acc: 0.2500\n",
      "Epoch 453/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.8509 - acc: 1.0000 - val_loss: 11.6452 - val_acc: 0.2400\n",
      "Epoch 454/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.8312 - acc: 1.0000 - val_loss: 11.5984 - val_acc: 0.2400\n",
      "Epoch 455/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.8119 - acc: 1.0000 - val_loss: 11.6186 - val_acc: 0.2500\n",
      "Epoch 456/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.7917 - acc: 1.0000 - val_loss: 11.4979 - val_acc: 0.2700\n",
      "Epoch 457/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.7717 - acc: 1.0000 - val_loss: 11.4819 - val_acc: 0.2700\n",
      "Epoch 458/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.7520 - acc: 1.0000 - val_loss: 11.3236 - val_acc: 0.2800\n",
      "Epoch 459/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.7318 - acc: 1.0000 - val_loss: 11.2549 - val_acc: 0.2900\n",
      "Epoch 460/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.7121 - acc: 1.0000 - val_loss: 11.2804 - val_acc: 0.3000\n",
      "Epoch 461/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.6929 - acc: 1.0000 - val_loss: 11.2390 - val_acc: 0.2900\n",
      "Epoch 462/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.6730 - acc: 1.0000 - val_loss: 10.8862 - val_acc: 0.3000\n",
      "Epoch 463/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.6524 - acc: 1.0000 - val_loss: 10.9149 - val_acc: 0.3000\n",
      "Epoch 464/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.6327 - acc: 1.0000 - val_loss: 10.6634 - val_acc: 0.3000\n",
      "Epoch 465/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.6129 - acc: 1.0000 - val_loss: 10.7232 - val_acc: 0.2700\n",
      "Epoch 466/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.5928 - acc: 1.0000 - val_loss: 10.6332 - val_acc: 0.2800\n",
      "Epoch 467/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.5746 - acc: 1.0000 - val_loss: 10.7499 - val_acc: 0.2800\n",
      "Epoch 468/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.5532 - acc: 1.0000 - val_loss: 10.8750 - val_acc: 0.2600\n",
      "Epoch 469/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.5333 - acc: 1.0000 - val_loss: 10.8350 - val_acc: 0.2700\n",
      "Epoch 470/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.5135 - acc: 1.0000 - val_loss: 10.8925 - val_acc: 0.2500\n",
      "Epoch 471/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4934 - acc: 1.0000 - val_loss: 10.9455 - val_acc: 0.2600\n",
      "Epoch 472/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4737 - acc: 1.0000 - val_loss: 10.9232 - val_acc: 0.2500\n",
      "Epoch 473/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4541 - acc: 1.0000 - val_loss: 10.8243 - val_acc: 0.2500\n",
      "Epoch 474/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4342 - acc: 1.0000 - val_loss: 10.8851 - val_acc: 0.2700\n",
      "Epoch 475/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4183 - acc: 0.9950 - val_loss: 10.6693 - val_acc: 0.2700\n",
      "Epoch 476/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4061 - acc: 0.9950 - val_loss: 10.6265 - val_acc: 0.3000\n",
      "Epoch 477/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4248 - acc: 0.9900 - val_loss: 10.9052 - val_acc: 0.2600\n",
      "Epoch 478/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4198 - acc: 0.9850 - val_loss: 11.4011 - val_acc: 0.2600\n",
      "Epoch 479/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4344 - acc: 0.9750 - val_loss: 10.8578 - val_acc: 0.3100\n",
      "Epoch 480/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4630 - acc: 0.9550 - val_loss: 13.2576 - val_acc: 0.2100\n",
      "Epoch 481/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.6150 - acc: 0.9550 - val_loss: 14.3346 - val_acc: 0.2000\n",
      "Epoch 482/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4526 - acc: 0.9500 - val_loss: 12.4112 - val_acc: 0.2700\n",
      "Epoch 483/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4594 - acc: 0.9600 - val_loss: 13.9308 - val_acc: 0.1600\n",
      "Epoch 484/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4063 - acc: 0.9600 - val_loss: 11.8244 - val_acc: 0.2000\n",
      "Epoch 485/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.3400 - acc: 0.9800 - val_loss: 11.5345 - val_acc: 0.2200\n",
      "Epoch 486/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.3184 - acc: 0.9850 - val_loss: 11.2353 - val_acc: 0.1500\n",
      "Epoch 487/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.2809 - acc: 0.9850 - val_loss: 10.6135 - val_acc: 0.1700\n",
      "Epoch 488/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.2475 - acc: 0.9900 - val_loss: 11.0148 - val_acc: 0.1600\n",
      "Epoch 489/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.2434 - acc: 0.9800 - val_loss: 11.5952 - val_acc: 0.2000\n",
      "Epoch 490/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.2336 - acc: 0.9850 - val_loss: 11.8061 - val_acc: 0.1300\n",
      "Epoch 491/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.2393 - acc: 0.9800 - val_loss: 11.7416 - val_acc: 0.1800\n",
      "Epoch 492/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.1822 - acc: 0.9950 - val_loss: 11.6632 - val_acc: 0.2500\n",
      "Epoch 493/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.1823 - acc: 0.9850 - val_loss: 11.9910 - val_acc: 0.2400\n",
      "Epoch 494/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.1768 - acc: 0.9950 - val_loss: 11.3175 - val_acc: 0.2300\n",
      "Epoch 495/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.1698 - acc: 0.9900 - val_loss: 10.2544 - val_acc: 0.2900\n",
      "Epoch 496/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.1335 - acc: 0.9900 - val_loss: 10.7048 - val_acc: 0.2200\n",
      "Epoch 497/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.1105 - acc: 0.9950 - val_loss: 10.6643 - val_acc: 0.2800\n",
      "Epoch 498/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.0931 - acc: 0.9900 - val_loss: 10.7398 - val_acc: 0.2400\n",
      "Epoch 499/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.0830 - acc: 0.9900 - val_loss: 10.7428 - val_acc: 0.2800\n",
      "Epoch 500/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.0463 - acc: 1.0000 - val_loss: 10.9739 - val_acc: 0.2200\n",
      "Epoch 501/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.0370 - acc: 0.9950 - val_loss: 10.8649 - val_acc: 0.1900\n",
      "Epoch 502/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.0200 - acc: 0.9950 - val_loss: 10.8866 - val_acc: 0.2000\n",
      "Epoch 503/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.9987 - acc: 1.0000 - val_loss: 10.1193 - val_acc: 0.3400\n",
      "Epoch 504/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.0183 - acc: 0.9850 - val_loss: 10.9869 - val_acc: 0.2100\n",
      "Epoch 505/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.9910 - acc: 0.9950 - val_loss: 12.3363 - val_acc: 0.1500\n",
      "Epoch 506/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.9761 - acc: 0.9850 - val_loss: 11.9219 - val_acc: 0.2200\n",
      "Epoch 507/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.9474 - acc: 1.0000 - val_loss: 11.8728 - val_acc: 0.2400\n",
      "Epoch 508/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.9384 - acc: 0.9950 - val_loss: 11.4855 - val_acc: 0.2400\n",
      "Epoch 509/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.9167 - acc: 0.9950 - val_loss: 10.2953 - val_acc: 0.2600\n",
      "Epoch 510/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.9663 - acc: 0.9800 - val_loss: 9.8975 - val_acc: 0.3400\n",
      "Epoch 511/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.9956 - acc: 0.9750 - val_loss: 10.3827 - val_acc: 0.3600\n",
      "Epoch 512/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.8730 - acc: 1.0000 - val_loss: 11.8493 - val_acc: 0.2300\n",
      "Epoch 513/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.9078 - acc: 0.9800 - val_loss: 11.2116 - val_acc: 0.2700\n",
      "Epoch 514/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.8534 - acc: 0.9900 - val_loss: 10.6608 - val_acc: 0.3000\n",
      "Epoch 515/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.8810 - acc: 0.9850 - val_loss: 10.4240 - val_acc: 0.3600\n",
      "Epoch 516/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.8341 - acc: 0.9900 - val_loss: 11.0811 - val_acc: 0.3100\n",
      "Epoch 517/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.8223 - acc: 0.9900 - val_loss: 10.0913 - val_acc: 0.3900\n",
      "Epoch 518/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.7930 - acc: 1.0000 - val_loss: 9.2347 - val_acc: 0.5200\n",
      "Epoch 519/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.7916 - acc: 0.9900 - val_loss: 9.6999 - val_acc: 0.4200\n",
      "Epoch 520/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.7857 - acc: 0.9950 - val_loss: 10.1084 - val_acc: 0.3300\n",
      "Epoch 521/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.8023 - acc: 0.9800 - val_loss: 10.0100 - val_acc: 0.3800\n",
      "Epoch 522/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.7280 - acc: 1.0000 - val_loss: 10.6047 - val_acc: 0.3200\n",
      "Epoch 523/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.7176 - acc: 0.9950 - val_loss: 10.7566 - val_acc: 0.3100\n",
      "Epoch 524/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.6980 - acc: 1.0000 - val_loss: 10.9982 - val_acc: 0.2900\n",
      "Epoch 525/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.6837 - acc: 1.0000 - val_loss: 11.0650 - val_acc: 0.2900\n",
      "Epoch 526/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.6653 - acc: 1.0000 - val_loss: 11.0248 - val_acc: 0.3000\n",
      "Epoch 527/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.6547 - acc: 1.0000 - val_loss: 11.0513 - val_acc: 0.2500\n",
      "Epoch 528/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.6379 - acc: 1.0000 - val_loss: 10.8173 - val_acc: 0.3600\n",
      "Epoch 529/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.6205 - acc: 1.0000 - val_loss: 10.7897 - val_acc: 0.3400\n",
      "Epoch 530/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.6082 - acc: 1.0000 - val_loss: 10.7279 - val_acc: 0.3500\n",
      "Epoch 531/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.5914 - acc: 1.0000 - val_loss: 10.8648 - val_acc: 0.3600\n",
      "Epoch 532/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.5753 - acc: 1.0000 - val_loss: 11.0966 - val_acc: 0.3100\n",
      "Epoch 533/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.5609 - acc: 1.0000 - val_loss: 11.2424 - val_acc: 0.3000\n",
      "Epoch 534/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.5452 - acc: 1.0000 - val_loss: 11.2606 - val_acc: 0.2900\n",
      "Epoch 535/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.5302 - acc: 1.0000 - val_loss: 11.1131 - val_acc: 0.2900\n",
      "Epoch 536/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.5153 - acc: 1.0000 - val_loss: 11.2257 - val_acc: 0.2800\n",
      "Epoch 537/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.5003 - acc: 1.0000 - val_loss: 11.2524 - val_acc: 0.2700\n",
      "Epoch 538/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.4858 - acc: 1.0000 - val_loss: 11.2149 - val_acc: 0.2700\n",
      "Epoch 539/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.4700 - acc: 1.0000 - val_loss: 11.1330 - val_acc: 0.2600\n",
      "Epoch 540/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.4549 - acc: 1.0000 - val_loss: 10.9526 - val_acc: 0.2800\n",
      "Epoch 541/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.4403 - acc: 1.0000 - val_loss: 11.2183 - val_acc: 0.2500\n",
      "Epoch 542/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.4246 - acc: 1.0000 - val_loss: 10.9713 - val_acc: 0.2700\n",
      "Epoch 543/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.4111 - acc: 1.0000 - val_loss: 10.9650 - val_acc: 0.2800\n",
      "Epoch 544/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.3944 - acc: 1.0000 - val_loss: 11.0436 - val_acc: 0.2700\n",
      "Epoch 545/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.3796 - acc: 1.0000 - val_loss: 11.1703 - val_acc: 0.2700\n",
      "Epoch 546/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.3642 - acc: 1.0000 - val_loss: 11.2189 - val_acc: 0.2700\n",
      "Epoch 547/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.3496 - acc: 1.0000 - val_loss: 11.3990 - val_acc: 0.2300\n",
      "Epoch 548/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.3357 - acc: 1.0000 - val_loss: 11.1370 - val_acc: 0.2400\n",
      "Epoch 549/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.3188 - acc: 1.0000 - val_loss: 10.8895 - val_acc: 0.2400\n",
      "Epoch 550/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.3047 - acc: 1.0000 - val_loss: 10.6908 - val_acc: 0.2500\n",
      "Epoch 551/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.2888 - acc: 1.0000 - val_loss: 10.7707 - val_acc: 0.2500\n",
      "Epoch 552/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.2735 - acc: 1.0000 - val_loss: 10.5967 - val_acc: 0.3000\n",
      "Epoch 553/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.2597 - acc: 1.0000 - val_loss: 10.6565 - val_acc: 0.2200\n",
      "Epoch 554/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.2437 - acc: 1.0000 - val_loss: 10.9859 - val_acc: 0.2000\n",
      "Epoch 555/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.2311 - acc: 1.0000 - val_loss: 11.1654 - val_acc: 0.2100\n",
      "Epoch 556/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.2126 - acc: 1.0000 - val_loss: 11.4076 - val_acc: 0.2300\n",
      "Epoch 557/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.1976 - acc: 1.0000 - val_loss: 11.2441 - val_acc: 0.2200\n",
      "Epoch 558/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.1825 - acc: 1.0000 - val_loss: 11.5916 - val_acc: 0.2000\n",
      "Epoch 559/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.1673 - acc: 1.0000 - val_loss: 11.3428 - val_acc: 0.2400\n",
      "Epoch 560/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.1520 - acc: 1.0000 - val_loss: 11.3372 - val_acc: 0.2400\n",
      "Epoch 561/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.1382 - acc: 1.0000 - val_loss: 11.1684 - val_acc: 0.2600\n",
      "Epoch 562/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.1235 - acc: 1.0000 - val_loss: 10.5936 - val_acc: 0.2800\n",
      "Epoch 563/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.1067 - acc: 1.0000 - val_loss: 10.3984 - val_acc: 0.2700\n",
      "Epoch 564/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.0913 - acc: 1.0000 - val_loss: 10.2502 - val_acc: 0.3100\n",
      "Epoch 565/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.0767 - acc: 1.0000 - val_loss: 9.7751 - val_acc: 0.3400\n",
      "Epoch 566/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.0631 - acc: 1.0000 - val_loss: 10.0611 - val_acc: 0.2900\n",
      "Epoch 567/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.0489 - acc: 1.0000 - val_loss: 10.2248 - val_acc: 0.2800\n",
      "Epoch 568/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.0314 - acc: 1.0000 - val_loss: 10.3211 - val_acc: 0.2800\n",
      "Epoch 569/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 6.0150 - acc: 1.0000 - val_loss: 10.6396 - val_acc: 0.2700\n",
      "Epoch 570/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.9997 - acc: 1.0000 - val_loss: 10.6131 - val_acc: 0.2600\n",
      "Epoch 571/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.9849 - acc: 1.0000 - val_loss: 10.7039 - val_acc: 0.2600\n",
      "Epoch 572/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.9694 - acc: 1.0000 - val_loss: 10.7461 - val_acc: 0.2500\n",
      "Epoch 573/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.9540 - acc: 1.0000 - val_loss: 10.7365 - val_acc: 0.2600\n",
      "Epoch 574/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.9386 - acc: 1.0000 - val_loss: 10.5235 - val_acc: 0.2600\n",
      "Epoch 575/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.9234 - acc: 1.0000 - val_loss: 10.6763 - val_acc: 0.2500\n",
      "Epoch 576/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.9082 - acc: 1.0000 - val_loss: 10.7149 - val_acc: 0.2500\n",
      "Epoch 577/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.8929 - acc: 1.0000 - val_loss: 10.6632 - val_acc: 0.2600\n",
      "Epoch 578/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.8773 - acc: 1.0000 - val_loss: 10.5823 - val_acc: 0.2600\n",
      "Epoch 579/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.8621 - acc: 1.0000 - val_loss: 10.6168 - val_acc: 0.2600\n",
      "Epoch 580/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.8473 - acc: 1.0000 - val_loss: 10.5396 - val_acc: 0.2600\n",
      "Epoch 581/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.8318 - acc: 1.0000 - val_loss: 10.6655 - val_acc: 0.2400\n",
      "Epoch 582/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.8161 - acc: 1.0000 - val_loss: 10.8351 - val_acc: 0.2500\n",
      "Epoch 583/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.8008 - acc: 1.0000 - val_loss: 10.6989 - val_acc: 0.2600\n",
      "Epoch 584/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.7855 - acc: 1.0000 - val_loss: 10.7546 - val_acc: 0.2500\n",
      "Epoch 585/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.8417 - acc: 0.9850 - val_loss: 10.3493 - val_acc: 0.2800\n",
      "Epoch 586/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.8320 - acc: 0.9800 - val_loss: 9.2244 - val_acc: 0.2900\n",
      "Epoch 587/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.8611 - acc: 0.9800 - val_loss: 8.9789 - val_acc: 0.3000\n",
      "Epoch 588/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.8063 - acc: 0.9800 - val_loss: 8.5347 - val_acc: 0.3400\n",
      "Epoch 589/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.7562 - acc: 0.9900 - val_loss: 8.2491 - val_acc: 0.4300\n",
      "Epoch 590/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.7200 - acc: 0.9950 - val_loss: 8.7283 - val_acc: 0.4000\n",
      "Epoch 591/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.7123 - acc: 0.9900 - val_loss: 8.6602 - val_acc: 0.4400\n",
      "Epoch 592/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.6971 - acc: 0.9950 - val_loss: 8.1197 - val_acc: 0.4900\n",
      "Epoch 593/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.6751 - acc: 1.0000 - val_loss: 8.0853 - val_acc: 0.5200\n",
      "Epoch 594/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.6603 - acc: 1.0000 - val_loss: 8.2682 - val_acc: 0.5100\n",
      "Epoch 595/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.6661 - acc: 0.9950 - val_loss: 8.1548 - val_acc: 0.4700\n",
      "Epoch 596/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.6295 - acc: 1.0000 - val_loss: 8.4337 - val_acc: 0.4200\n",
      "Epoch 597/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.6173 - acc: 1.0000 - val_loss: 8.4070 - val_acc: 0.4300\n",
      "Epoch 598/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.6073 - acc: 1.0000 - val_loss: 8.6205 - val_acc: 0.3700\n",
      "Epoch 599/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.5863 - acc: 1.0000 - val_loss: 8.5526 - val_acc: 0.4300\n",
      "Epoch 600/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.5818 - acc: 0.9950 - val_loss: 8.7947 - val_acc: 0.4200\n",
      "Epoch 601/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.5654 - acc: 0.9950 - val_loss: 8.8914 - val_acc: 0.4200\n",
      "Epoch 602/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.5589 - acc: 0.9900 - val_loss: 9.7918 - val_acc: 0.3300\n",
      "Epoch 603/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.5379 - acc: 0.9950 - val_loss: 10.4614 - val_acc: 0.2800\n",
      "Epoch 604/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.5168 - acc: 1.0000 - val_loss: 11.0006 - val_acc: 0.2400\n",
      "Epoch 605/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.5023 - acc: 1.0000 - val_loss: 11.0755 - val_acc: 0.2300\n",
      "Epoch 606/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.4900 - acc: 1.0000 - val_loss: 11.0585 - val_acc: 0.2100\n",
      "Epoch 607/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.4743 - acc: 1.0000 - val_loss: 10.8637 - val_acc: 0.2100\n",
      "Epoch 608/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.4631 - acc: 1.0000 - val_loss: 10.5889 - val_acc: 0.2400\n",
      "Epoch 609/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.4507 - acc: 1.0000 - val_loss: 9.7262 - val_acc: 0.2900\n",
      "Epoch 610/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.4351 - acc: 1.0000 - val_loss: 9.6766 - val_acc: 0.3100\n",
      "Epoch 611/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.4187 - acc: 1.0000 - val_loss: 9.7750 - val_acc: 0.3100\n",
      "Epoch 612/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.4064 - acc: 1.0000 - val_loss: 10.0025 - val_acc: 0.3000\n",
      "Epoch 613/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.3913 - acc: 1.0000 - val_loss: 9.8572 - val_acc: 0.3000\n",
      "Epoch 614/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.3799 - acc: 1.0000 - val_loss: 9.4225 - val_acc: 0.4000\n",
      "Epoch 615/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.3911 - acc: 0.9950 - val_loss: 9.2531 - val_acc: 0.4000\n",
      "Epoch 616/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.3513 - acc: 1.0000 - val_loss: 9.5984 - val_acc: 0.3700\n",
      "Epoch 617/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.3546 - acc: 0.9950 - val_loss: 9.4978 - val_acc: 0.3400\n",
      "Epoch 618/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.3233 - acc: 1.0000 - val_loss: 9.7227 - val_acc: 0.3500\n",
      "Epoch 619/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.3123 - acc: 0.9950 - val_loss: 9.4245 - val_acc: 0.3800\n",
      "Epoch 620/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.2931 - acc: 1.0000 - val_loss: 8.7876 - val_acc: 0.4100\n",
      "Epoch 621/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.2791 - acc: 1.0000 - val_loss: 8.9399 - val_acc: 0.4200\n",
      "Epoch 622/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.2648 - acc: 1.0000 - val_loss: 8.8788 - val_acc: 0.4100\n",
      "Epoch 623/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.2509 - acc: 1.0000 - val_loss: 8.7521 - val_acc: 0.4200\n",
      "Epoch 624/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.2370 - acc: 1.0000 - val_loss: 8.8319 - val_acc: 0.4100\n",
      "Epoch 625/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.2226 - acc: 1.0000 - val_loss: 8.9566 - val_acc: 0.4300\n",
      "Epoch 626/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.2080 - acc: 1.0000 - val_loss: 9.0553 - val_acc: 0.4200\n",
      "Epoch 627/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.1949 - acc: 1.0000 - val_loss: 8.8391 - val_acc: 0.4200\n",
      "Epoch 628/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.1797 - acc: 1.0000 - val_loss: 8.9431 - val_acc: 0.4200\n",
      "Epoch 629/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.1653 - acc: 1.0000 - val_loss: 8.8295 - val_acc: 0.4200\n",
      "Epoch 630/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.1514 - acc: 1.0000 - val_loss: 8.9303 - val_acc: 0.4100\n",
      "Epoch 631/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.1398 - acc: 1.0000 - val_loss: 8.6846 - val_acc: 0.4300\n",
      "Epoch 632/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.1261 - acc: 1.0000 - val_loss: 7.8361 - val_acc: 0.4800\n",
      "Epoch 633/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.1161 - acc: 1.0000 - val_loss: 8.0154 - val_acc: 0.4800\n",
      "Epoch 634/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.1407 - acc: 0.9800 - val_loss: 8.5630 - val_acc: 0.4000\n",
      "Epoch 635/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.0931 - acc: 0.9950 - val_loss: 8.3966 - val_acc: 0.4200\n",
      "Epoch 636/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.0734 - acc: 1.0000 - val_loss: 8.5101 - val_acc: 0.3700\n",
      "Epoch 637/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.0586 - acc: 1.0000 - val_loss: 8.6398 - val_acc: 0.3700\n",
      "Epoch 638/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.0458 - acc: 1.0000 - val_loss: 8.5369 - val_acc: 0.3700\n",
      "Epoch 639/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.0294 - acc: 1.0000 - val_loss: 8.3163 - val_acc: 0.3900\n",
      "Epoch 640/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.0158 - acc: 1.0000 - val_loss: 8.4506 - val_acc: 0.3900\n",
      "Epoch 641/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.0049 - acc: 1.0000 - val_loss: 8.5448 - val_acc: 0.3800\n",
      "Epoch 642/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.9963 - acc: 0.9950 - val_loss: 8.7412 - val_acc: 0.3700\n",
      "Epoch 643/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.9796 - acc: 1.0000 - val_loss: 8.1543 - val_acc: 0.4600\n",
      "Epoch 644/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.9686 - acc: 0.9950 - val_loss: 8.8345 - val_acc: 0.3700\n",
      "Epoch 645/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.9600 - acc: 0.9950 - val_loss: 8.8319 - val_acc: 0.3200\n",
      "Epoch 646/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.9385 - acc: 1.0000 - val_loss: 8.8841 - val_acc: 0.3100\n",
      "Epoch 647/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.9245 - acc: 1.0000 - val_loss: 9.0753 - val_acc: 0.3000\n",
      "Epoch 648/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.9091 - acc: 1.0000 - val_loss: 9.0660 - val_acc: 0.3000\n",
      "Epoch 649/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.8978 - acc: 1.0000 - val_loss: 8.8646 - val_acc: 0.3300\n",
      "Epoch 650/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.8815 - acc: 1.0000 - val_loss: 8.3879 - val_acc: 0.3600\n",
      "Epoch 651/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.8694 - acc: 1.0000 - val_loss: 8.5519 - val_acc: 0.3500\n",
      "Epoch 652/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.8578 - acc: 0.9950 - val_loss: 8.5895 - val_acc: 0.3300\n",
      "Epoch 653/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.8400 - acc: 1.0000 - val_loss: 8.5608 - val_acc: 0.3300\n",
      "Epoch 654/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.8359 - acc: 1.0000 - val_loss: 8.1971 - val_acc: 0.3600\n",
      "Epoch 655/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.8181 - acc: 0.9950 - val_loss: 7.8659 - val_acc: 0.3700\n",
      "Epoch 656/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.8227 - acc: 0.9900 - val_loss: 7.9184 - val_acc: 0.3700\n",
      "Epoch 657/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.7962 - acc: 1.0000 - val_loss: 8.8316 - val_acc: 0.2800\n",
      "Epoch 658/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.8475 - acc: 0.9700 - val_loss: 7.7532 - val_acc: 0.3900\n",
      "Epoch 659/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.7746 - acc: 1.0000 - val_loss: 7.4771 - val_acc: 0.4900\n",
      "Epoch 660/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.7872 - acc: 0.9850 - val_loss: 7.3503 - val_acc: 0.5100\n",
      "Epoch 661/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.7481 - acc: 1.0000 - val_loss: 7.7697 - val_acc: 0.4200\n",
      "Epoch 662/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.7942 - acc: 0.9750 - val_loss: 7.7273 - val_acc: 0.4300\n",
      "Epoch 663/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.7426 - acc: 0.9950 - val_loss: 6.7976 - val_acc: 0.4900\n",
      "Epoch 664/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.7639 - acc: 0.9750 - val_loss: 6.9723 - val_acc: 0.5200\n",
      "Epoch 665/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.7580 - acc: 0.9800 - val_loss: 6.7200 - val_acc: 0.6000\n",
      "Epoch 666/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.7463 - acc: 0.9700 - val_loss: 6.6541 - val_acc: 0.6300\n",
      "Epoch 667/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.7705 - acc: 0.9750 - val_loss: 7.6685 - val_acc: 0.4700\n",
      "Epoch 668/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.6870 - acc: 0.9900 - val_loss: 7.9252 - val_acc: 0.4000\n",
      "Epoch 669/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.6781 - acc: 0.9900 - val_loss: 8.1390 - val_acc: 0.4200\n",
      "Epoch 670/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.6803 - acc: 0.9850 - val_loss: 7.3502 - val_acc: 0.5000\n",
      "Epoch 671/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.6427 - acc: 0.9950 - val_loss: 7.0621 - val_acc: 0.5200\n",
      "Epoch 672/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.6236 - acc: 1.0000 - val_loss: 6.8925 - val_acc: 0.5300\n",
      "Epoch 673/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.6102 - acc: 1.0000 - val_loss: 7.2136 - val_acc: 0.5100\n",
      "Epoch 674/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.5967 - acc: 1.0000 - val_loss: 7.4783 - val_acc: 0.4900\n",
      "Epoch 675/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.5957 - acc: 0.9950 - val_loss: 7.4676 - val_acc: 0.4900\n",
      "Epoch 676/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.5735 - acc: 1.0000 - val_loss: 7.6811 - val_acc: 0.5300\n",
      "Epoch 677/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.5921 - acc: 0.9800 - val_loss: 7.8017 - val_acc: 0.4900\n",
      "Epoch 678/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.5634 - acc: 0.9950 - val_loss: 6.9605 - val_acc: 0.5100\n",
      "Epoch 679/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.5603 - acc: 0.9950 - val_loss: 7.9225 - val_acc: 0.2800\n",
      "Epoch 680/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.5377 - acc: 1.0000 - val_loss: 8.2932 - val_acc: 0.2300\n",
      "Epoch 681/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.5254 - acc: 0.9950 - val_loss: 7.9897 - val_acc: 0.2700\n",
      "Epoch 682/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.5042 - acc: 1.0000 - val_loss: 7.5233 - val_acc: 0.4200\n",
      "Epoch 683/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.4914 - acc: 1.0000 - val_loss: 7.7998 - val_acc: 0.4100\n",
      "Epoch 684/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.4835 - acc: 1.0000 - val_loss: 7.4909 - val_acc: 0.4600\n",
      "Epoch 685/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.4672 - acc: 1.0000 - val_loss: 7.8239 - val_acc: 0.4000\n",
      "Epoch 686/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.4565 - acc: 1.0000 - val_loss: 7.7599 - val_acc: 0.4400\n",
      "Epoch 687/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.4610 - acc: 0.9950 - val_loss: 7.6250 - val_acc: 0.4300\n",
      "Epoch 688/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.4373 - acc: 1.0000 - val_loss: 7.2925 - val_acc: 0.4800\n",
      "Epoch 689/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.4613 - acc: 0.9950 - val_loss: 7.8609 - val_acc: 0.3900\n",
      "Epoch 690/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.4104 - acc: 1.0000 - val_loss: 8.3211 - val_acc: 0.3400\n",
      "Epoch 691/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.4044 - acc: 0.9950 - val_loss: 8.1289 - val_acc: 0.4100\n",
      "Epoch 692/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.3930 - acc: 0.9950 - val_loss: 8.2202 - val_acc: 0.3900\n",
      "Epoch 693/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.3874 - acc: 0.9900 - val_loss: 7.8841 - val_acc: 0.3900\n",
      "Epoch 694/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.3875 - acc: 0.9900 - val_loss: 7.0797 - val_acc: 0.4400\n",
      "Epoch 695/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.3825 - acc: 0.9950 - val_loss: 6.7781 - val_acc: 0.5200\n",
      "Epoch 696/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.3695 - acc: 0.9850 - val_loss: 5.5696 - val_acc: 0.7100\n",
      "Epoch 697/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.4179 - acc: 0.9700 - val_loss: 6.5422 - val_acc: 0.5300\n",
      "Epoch 698/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.3614 - acc: 0.9900 - val_loss: 8.2442 - val_acc: 0.3100\n",
      "Epoch 699/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.3585 - acc: 0.9750 - val_loss: 7.5445 - val_acc: 0.4000\n",
      "Epoch 700/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.3169 - acc: 0.9950 - val_loss: 6.7285 - val_acc: 0.5300\n",
      "Epoch 701/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.2942 - acc: 1.0000 - val_loss: 6.5000 - val_acc: 0.5900\n",
      "Epoch 702/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.2795 - acc: 1.0000 - val_loss: 6.5781 - val_acc: 0.5900\n",
      "Epoch 703/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.2710 - acc: 1.0000 - val_loss: 6.6547 - val_acc: 0.5800\n",
      "Epoch 704/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.2572 - acc: 1.0000 - val_loss: 6.6964 - val_acc: 0.5400\n",
      "Epoch 705/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.2450 - acc: 1.0000 - val_loss: 6.8649 - val_acc: 0.5500\n",
      "Epoch 706/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.2349 - acc: 1.0000 - val_loss: 6.9632 - val_acc: 0.5500\n",
      "Epoch 707/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.2223 - acc: 1.0000 - val_loss: 6.6718 - val_acc: 0.5900\n",
      "Epoch 708/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.2102 - acc: 1.0000 - val_loss: 6.6329 - val_acc: 0.5900\n",
      "Epoch 709/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.1993 - acc: 1.0000 - val_loss: 6.5277 - val_acc: 0.6100\n",
      "Epoch 710/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.1875 - acc: 1.0000 - val_loss: 6.3709 - val_acc: 0.6100\n",
      "Epoch 711/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.1791 - acc: 1.0000 - val_loss: 6.4573 - val_acc: 0.6100\n",
      "Epoch 712/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.1676 - acc: 1.0000 - val_loss: 6.3984 - val_acc: 0.6200\n",
      "Epoch 713/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.2060 - acc: 0.9900 - val_loss: 6.5496 - val_acc: 0.5800\n",
      "Epoch 714/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.1553 - acc: 0.9950 - val_loss: 6.7499 - val_acc: 0.5500\n",
      "Epoch 715/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.1374 - acc: 0.9950 - val_loss: 6.9317 - val_acc: 0.5400\n",
      "Epoch 716/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.1210 - acc: 1.0000 - val_loss: 7.1265 - val_acc: 0.5000\n",
      "Epoch 717/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.1094 - acc: 1.0000 - val_loss: 6.9720 - val_acc: 0.5400\n",
      "Epoch 718/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.0976 - acc: 1.0000 - val_loss: 7.1118 - val_acc: 0.4800\n",
      "Epoch 719/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.0880 - acc: 1.0000 - val_loss: 6.8836 - val_acc: 0.5400\n",
      "Epoch 720/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.0740 - acc: 1.0000 - val_loss: 7.0512 - val_acc: 0.5300\n",
      "Epoch 721/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.0625 - acc: 1.0000 - val_loss: 6.7567 - val_acc: 0.5300\n",
      "Epoch 722/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.0573 - acc: 0.9950 - val_loss: 6.8276 - val_acc: 0.5400\n",
      "Epoch 723/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.0397 - acc: 1.0000 - val_loss: 6.7625 - val_acc: 0.5900\n",
      "Epoch 724/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.0318 - acc: 1.0000 - val_loss: 6.9324 - val_acc: 0.6000\n",
      "Epoch 725/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.0379 - acc: 0.9900 - val_loss: 6.9261 - val_acc: 0.5700\n",
      "Epoch 726/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.0143 - acc: 1.0000 - val_loss: 7.0822 - val_acc: 0.4600\n",
      "Epoch 727/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.0375 - acc: 0.9850 - val_loss: 6.6361 - val_acc: 0.5000\n",
      "Epoch 728/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9914 - acc: 1.0000 - val_loss: 7.8151 - val_acc: 0.3700\n",
      "Epoch 729/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 4.0033 - acc: 0.9900 - val_loss: 8.1907 - val_acc: 0.3600\n",
      "Epoch 730/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9940 - acc: 0.9900 - val_loss: 8.0462 - val_acc: 0.3900\n",
      "Epoch 731/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9599 - acc: 1.0000 - val_loss: 8.8518 - val_acc: 0.3500\n",
      "Epoch 732/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9613 - acc: 0.9950 - val_loss: 8.2279 - val_acc: 0.3600\n",
      "Epoch 733/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9508 - acc: 0.9900 - val_loss: 9.3381 - val_acc: 0.2600\n",
      "Epoch 734/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9475 - acc: 0.9850 - val_loss: 7.9259 - val_acc: 0.3400\n",
      "Epoch 735/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9288 - acc: 0.9900 - val_loss: 7.6594 - val_acc: 0.3000\n",
      "Epoch 736/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9269 - acc: 0.9950 - val_loss: 7.8537 - val_acc: 0.2600\n",
      "Epoch 737/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9103 - acc: 0.9950 - val_loss: 7.6418 - val_acc: 0.3200\n",
      "Epoch 738/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.8913 - acc: 1.0000 - val_loss: 7.1755 - val_acc: 0.4100\n",
      "Epoch 739/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.8792 - acc: 0.9950 - val_loss: 6.7519 - val_acc: 0.4800\n",
      "Epoch 740/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.8780 - acc: 0.9950 - val_loss: 6.5383 - val_acc: 0.5400\n",
      "Epoch 741/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.8575 - acc: 0.9950 - val_loss: 6.6650 - val_acc: 0.4800\n",
      "Epoch 742/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9166 - acc: 0.9750 - val_loss: 7.0498 - val_acc: 0.3500\n",
      "Epoch 743/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.8694 - acc: 0.9900 - val_loss: 7.1704 - val_acc: 0.2200\n",
      "Epoch 744/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9256 - acc: 0.9800 - val_loss: 8.8188 - val_acc: 0.0300\n",
      "Epoch 745/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.8595 - acc: 0.9900 - val_loss: 7.6515 - val_acc: 0.1700\n",
      "Epoch 746/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.9295 - acc: 0.9600 - val_loss: 6.8085 - val_acc: 0.3300\n",
      "Epoch 747/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.8927 - acc: 0.9800 - val_loss: 6.2342 - val_acc: 0.4000\n",
      "Epoch 748/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.8069 - acc: 1.0000 - val_loss: 5.9048 - val_acc: 0.5300\n",
      "Epoch 749/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.8068 - acc: 0.9950 - val_loss: 6.5489 - val_acc: 0.4700\n",
      "Epoch 750/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.8117 - acc: 0.9900 - val_loss: 7.0366 - val_acc: 0.4200\n",
      "Epoch 751/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.7776 - acc: 1.0000 - val_loss: 7.5954 - val_acc: 0.3600\n",
      "Epoch 752/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.8014 - acc: 0.9900 - val_loss: 6.6158 - val_acc: 0.4600\n",
      "Epoch 753/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.7706 - acc: 0.9900 - val_loss: 6.3248 - val_acc: 0.5100\n",
      "Epoch 754/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.7564 - acc: 0.9900 - val_loss: 6.3366 - val_acc: 0.5200\n",
      "Epoch 755/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.7344 - acc: 1.0000 - val_loss: 6.8867 - val_acc: 0.4200\n",
      "Epoch 756/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.7460 - acc: 0.9950 - val_loss: 6.8678 - val_acc: 0.4300\n",
      "Epoch 757/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.7468 - acc: 0.9950 - val_loss: 6.2604 - val_acc: 0.4900\n",
      "Epoch 758/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.7176 - acc: 0.9950 - val_loss: 5.8551 - val_acc: 0.6000\n",
      "Epoch 759/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.7082 - acc: 0.9950 - val_loss: 5.7055 - val_acc: 0.6300\n",
      "Epoch 760/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.7275 - acc: 0.9800 - val_loss: 5.4541 - val_acc: 0.6400\n",
      "Epoch 761/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.6995 - acc: 0.9900 - val_loss: 5.4565 - val_acc: 0.6700\n",
      "Epoch 762/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.6702 - acc: 1.0000 - val_loss: 5.3892 - val_acc: 0.6400\n",
      "Epoch 763/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.6951 - acc: 0.9900 - val_loss: 5.5991 - val_acc: 0.6500\n",
      "Epoch 764/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.6505 - acc: 1.0000 - val_loss: 5.7674 - val_acc: 0.6700\n",
      "Epoch 765/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.6538 - acc: 1.0000 - val_loss: 5.7721 - val_acc: 0.5900\n",
      "Epoch 766/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.6434 - acc: 0.9950 - val_loss: 6.0866 - val_acc: 0.5300\n",
      "Epoch 767/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.6221 - acc: 1.0000 - val_loss: 6.4941 - val_acc: 0.5300\n",
      "Epoch 768/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.6296 - acc: 0.9900 - val_loss: 6.4078 - val_acc: 0.5100\n",
      "Epoch 769/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.6044 - acc: 1.0000 - val_loss: 6.6452 - val_acc: 0.4400\n",
      "Epoch 770/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.6081 - acc: 0.9950 - val_loss: 6.9227 - val_acc: 0.3600\n",
      "Epoch 771/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.5871 - acc: 1.0000 - val_loss: 7.0203 - val_acc: 0.2900\n",
      "Epoch 772/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.5871 - acc: 0.9950 - val_loss: 6.6256 - val_acc: 0.3100\n",
      "Epoch 773/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.5709 - acc: 0.9950 - val_loss: 6.2204 - val_acc: 0.3900\n",
      "Epoch 774/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.5613 - acc: 1.0000 - val_loss: 6.2354 - val_acc: 0.3900\n",
      "Epoch 775/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.5509 - acc: 1.0000 - val_loss: 6.0871 - val_acc: 0.4800\n",
      "Epoch 776/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.5417 - acc: 1.0000 - val_loss: 6.2468 - val_acc: 0.4600\n",
      "Epoch 777/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.5346 - acc: 1.0000 - val_loss: 6.3007 - val_acc: 0.4300\n",
      "Epoch 778/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.5214 - acc: 1.0000 - val_loss: 6.1293 - val_acc: 0.4500\n",
      "Epoch 779/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.5120 - acc: 1.0000 - val_loss: 5.9223 - val_acc: 0.5000\n",
      "Epoch 780/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.4998 - acc: 1.0000 - val_loss: 6.0943 - val_acc: 0.4600\n",
      "Epoch 781/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.4907 - acc: 1.0000 - val_loss: 5.9155 - val_acc: 0.5100\n",
      "Epoch 782/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.4796 - acc: 1.0000 - val_loss: 6.1652 - val_acc: 0.4600\n",
      "Epoch 783/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.4714 - acc: 1.0000 - val_loss: 5.9333 - val_acc: 0.5300\n",
      "Epoch 784/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.4613 - acc: 1.0000 - val_loss: 5.9111 - val_acc: 0.4900\n",
      "Epoch 785/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.4537 - acc: 1.0000 - val_loss: 6.2462 - val_acc: 0.4600\n",
      "Epoch 786/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.4410 - acc: 1.0000 - val_loss: 6.3848 - val_acc: 0.4400\n",
      "Epoch 787/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.4329 - acc: 1.0000 - val_loss: 6.3973 - val_acc: 0.4900\n",
      "Epoch 788/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.4234 - acc: 1.0000 - val_loss: 6.6382 - val_acc: 0.4100\n",
      "Epoch 789/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.4123 - acc: 1.0000 - val_loss: 6.5247 - val_acc: 0.4300\n",
      "Epoch 790/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.4039 - acc: 1.0000 - val_loss: 6.3997 - val_acc: 0.4500\n",
      "Epoch 791/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.3941 - acc: 1.0000 - val_loss: 6.3711 - val_acc: 0.4600\n",
      "Epoch 792/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.3838 - acc: 1.0000 - val_loss: 6.3158 - val_acc: 0.4700\n",
      "Epoch 793/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.3742 - acc: 1.0000 - val_loss: 6.2562 - val_acc: 0.4700\n",
      "Epoch 794/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.3644 - acc: 1.0000 - val_loss: 6.0071 - val_acc: 0.4700\n",
      "Epoch 795/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.3546 - acc: 1.0000 - val_loss: 5.8629 - val_acc: 0.4900\n",
      "Epoch 796/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.3450 - acc: 1.0000 - val_loss: 5.9142 - val_acc: 0.4700\n",
      "Epoch 797/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.3353 - acc: 1.0000 - val_loss: 5.9344 - val_acc: 0.5000\n",
      "Epoch 798/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.3270 - acc: 1.0000 - val_loss: 5.9470 - val_acc: 0.4900\n",
      "Epoch 799/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.3163 - acc: 1.0000 - val_loss: 5.8750 - val_acc: 0.5100\n",
      "Epoch 800/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.3067 - acc: 1.0000 - val_loss: 5.7685 - val_acc: 0.5100\n",
      "Epoch 801/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.2969 - acc: 1.0000 - val_loss: 5.7135 - val_acc: 0.5200\n",
      "Epoch 802/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.2876 - acc: 1.0000 - val_loss: 5.7934 - val_acc: 0.5100\n",
      "Epoch 803/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.2777 - acc: 1.0000 - val_loss: 6.0053 - val_acc: 0.5100\n",
      "Epoch 804/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.2680 - acc: 1.0000 - val_loss: 5.7396 - val_acc: 0.5400\n",
      "Epoch 805/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.2588 - acc: 1.0000 - val_loss: 5.8387 - val_acc: 0.5100\n",
      "Epoch 806/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.2488 - acc: 1.0000 - val_loss: 5.9623 - val_acc: 0.5200\n",
      "Epoch 807/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.2409 - acc: 1.0000 - val_loss: 5.9304 - val_acc: 0.5100\n",
      "Epoch 808/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.2297 - acc: 1.0000 - val_loss: 5.7962 - val_acc: 0.4800\n",
      "Epoch 809/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.2209 - acc: 1.0000 - val_loss: 5.9199 - val_acc: 0.4800\n",
      "Epoch 810/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.2105 - acc: 1.0000 - val_loss: 5.8741 - val_acc: 0.5100\n",
      "Epoch 811/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.2010 - acc: 1.0000 - val_loss: 5.8651 - val_acc: 0.5000\n",
      "Epoch 812/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.1914 - acc: 1.0000 - val_loss: 5.9890 - val_acc: 0.4800\n",
      "Epoch 813/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.1820 - acc: 1.0000 - val_loss: 5.9424 - val_acc: 0.4900\n",
      "Epoch 814/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.1722 - acc: 1.0000 - val_loss: 5.9515 - val_acc: 0.4700\n",
      "Epoch 815/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.1629 - acc: 1.0000 - val_loss: 5.7868 - val_acc: 0.4800\n",
      "Epoch 816/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.1532 - acc: 1.0000 - val_loss: 5.9279 - val_acc: 0.4700\n",
      "Epoch 817/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.1436 - acc: 1.0000 - val_loss: 5.8281 - val_acc: 0.4800\n",
      "Epoch 818/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.1341 - acc: 1.0000 - val_loss: 5.8442 - val_acc: 0.4700\n",
      "Epoch 819/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.1245 - acc: 1.0000 - val_loss: 6.0163 - val_acc: 0.4700\n",
      "Epoch 820/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.1156 - acc: 1.0000 - val_loss: 5.7370 - val_acc: 0.4800\n",
      "Epoch 821/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.1054 - acc: 1.0000 - val_loss: 5.9757 - val_acc: 0.4600\n",
      "Epoch 822/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.0961 - acc: 1.0000 - val_loss: 5.8018 - val_acc: 0.4600\n",
      "Epoch 823/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.0892 - acc: 1.0000 - val_loss: 6.0280 - val_acc: 0.4800\n",
      "Epoch 824/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.0771 - acc: 1.0000 - val_loss: 6.2060 - val_acc: 0.4900\n",
      "Epoch 825/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.0703 - acc: 1.0000 - val_loss: 6.4947 - val_acc: 0.4100\n",
      "Epoch 826/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.0654 - acc: 0.9950 - val_loss: 6.0556 - val_acc: 0.5000\n",
      "Epoch 827/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.0540 - acc: 0.9950 - val_loss: 5.5898 - val_acc: 0.5300\n",
      "Epoch 828/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.0425 - acc: 1.0000 - val_loss: 5.6815 - val_acc: 0.5400\n",
      "Epoch 829/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.0308 - acc: 1.0000 - val_loss: 5.7062 - val_acc: 0.5500\n",
      "Epoch 830/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.0214 - acc: 1.0000 - val_loss: 5.6207 - val_acc: 0.5600\n",
      "Epoch 831/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.0151 - acc: 1.0000 - val_loss: 6.0577 - val_acc: 0.5100\n",
      "Epoch 832/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.0026 - acc: 1.0000 - val_loss: 5.9375 - val_acc: 0.5500\n",
      "Epoch 833/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.9927 - acc: 1.0000 - val_loss: 5.8951 - val_acc: 0.5500\n",
      "Epoch 834/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.9837 - acc: 1.0000 - val_loss: 5.9155 - val_acc: 0.5600\n",
      "Epoch 835/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.9742 - acc: 1.0000 - val_loss: 5.9273 - val_acc: 0.5600\n",
      "Epoch 836/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.9647 - acc: 1.0000 - val_loss: 6.1912 - val_acc: 0.5300\n",
      "Epoch 837/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.9560 - acc: 1.0000 - val_loss: 6.0187 - val_acc: 0.5500\n",
      "Epoch 838/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.9458 - acc: 1.0000 - val_loss: 6.0288 - val_acc: 0.4900\n",
      "Epoch 839/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.9368 - acc: 1.0000 - val_loss: 5.9395 - val_acc: 0.5300\n",
      "Epoch 840/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.9270 - acc: 1.0000 - val_loss: 6.0786 - val_acc: 0.5100\n",
      "Epoch 841/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.9195 - acc: 1.0000 - val_loss: 5.8328 - val_acc: 0.5000\n",
      "Epoch 842/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.9086 - acc: 1.0000 - val_loss: 7.0680 - val_acc: 0.4000\n",
      "Epoch 843/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.8998 - acc: 1.0000 - val_loss: 7.0724 - val_acc: 0.3900\n",
      "Epoch 844/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.8932 - acc: 1.0000 - val_loss: 6.9336 - val_acc: 0.4000\n",
      "Epoch 845/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.8839 - acc: 1.0000 - val_loss: 6.2882 - val_acc: 0.4400\n",
      "Epoch 846/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.8715 - acc: 1.0000 - val_loss: 5.3496 - val_acc: 0.5500\n",
      "Epoch 847/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.8621 - acc: 1.0000 - val_loss: 5.1054 - val_acc: 0.5900\n",
      "Epoch 848/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.8531 - acc: 1.0000 - val_loss: 5.1221 - val_acc: 0.5800\n",
      "Epoch 849/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.8446 - acc: 1.0000 - val_loss: 5.2457 - val_acc: 0.5800\n",
      "Epoch 850/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.8354 - acc: 1.0000 - val_loss: 5.2112 - val_acc: 0.5400\n",
      "Epoch 851/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.8254 - acc: 1.0000 - val_loss: 5.1523 - val_acc: 0.5400\n",
      "Epoch 852/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.8159 - acc: 1.0000 - val_loss: 5.2092 - val_acc: 0.5400\n",
      "Epoch 853/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.8077 - acc: 1.0000 - val_loss: 5.1690 - val_acc: 0.5200\n",
      "Epoch 854/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7973 - acc: 1.0000 - val_loss: 5.5278 - val_acc: 0.4900\n",
      "Epoch 855/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7884 - acc: 1.0000 - val_loss: 5.6911 - val_acc: 0.4800\n",
      "Epoch 856/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7795 - acc: 1.0000 - val_loss: 5.4382 - val_acc: 0.4900\n",
      "Epoch 857/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7694 - acc: 1.0000 - val_loss: 5.6655 - val_acc: 0.4600\n",
      "Epoch 858/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7650 - acc: 0.9950 - val_loss: 5.8496 - val_acc: 0.4700\n",
      "Epoch 859/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7523 - acc: 1.0000 - val_loss: 5.8196 - val_acc: 0.4700\n",
      "Epoch 860/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7420 - acc: 1.0000 - val_loss: 4.9865 - val_acc: 0.6000\n",
      "Epoch 861/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7335 - acc: 1.0000 - val_loss: 4.6658 - val_acc: 0.6500\n",
      "Epoch 862/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7256 - acc: 1.0000 - val_loss: 4.6355 - val_acc: 0.6300\n",
      "Epoch 863/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7154 - acc: 1.0000 - val_loss: 4.6511 - val_acc: 0.6500\n",
      "Epoch 864/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7057 - acc: 1.0000 - val_loss: 4.7884 - val_acc: 0.6200\n",
      "Epoch 865/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7023 - acc: 0.9950 - val_loss: 4.7027 - val_acc: 0.6100\n",
      "Epoch 866/3000\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.6895 - acc: 1.0000 - val_loss: 4.6781 - val_acc: 0.5500\n",
      "Epoch 867/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.7080 - acc: 0.9950 - val_loss: 4.5133 - val_acc: 0.5900\n",
      "Epoch 868/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6813 - acc: 0.9950 - val_loss: 4.5926 - val_acc: 0.5300\n",
      "Epoch 869/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6666 - acc: 1.0000 - val_loss: 5.0118 - val_acc: 0.5000\n",
      "Epoch 870/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6563 - acc: 1.0000 - val_loss: 4.9878 - val_acc: 0.5200\n",
      "Epoch 871/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6474 - acc: 1.0000 - val_loss: 5.0705 - val_acc: 0.5300\n",
      "Epoch 872/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6402 - acc: 1.0000 - val_loss: 4.9861 - val_acc: 0.5500\n",
      "Epoch 873/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6300 - acc: 1.0000 - val_loss: 5.1788 - val_acc: 0.5000\n",
      "Epoch 874/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6254 - acc: 1.0000 - val_loss: 4.8014 - val_acc: 0.5700\n",
      "Epoch 875/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6187 - acc: 0.9950 - val_loss: 4.0858 - val_acc: 0.6800\n",
      "Epoch 876/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6124 - acc: 0.9950 - val_loss: 4.2874 - val_acc: 0.6700\n",
      "Epoch 877/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6182 - acc: 0.9950 - val_loss: 4.3955 - val_acc: 0.6500\n",
      "Epoch 878/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5900 - acc: 1.0000 - val_loss: 4.4458 - val_acc: 0.6500\n",
      "Epoch 879/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5826 - acc: 1.0000 - val_loss: 4.6911 - val_acc: 0.5500\n",
      "Epoch 880/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5726 - acc: 1.0000 - val_loss: 4.3051 - val_acc: 0.6400\n",
      "Epoch 881/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5774 - acc: 0.9950 - val_loss: 4.3013 - val_acc: 0.6200\n",
      "Epoch 882/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5546 - acc: 1.0000 - val_loss: 3.9923 - val_acc: 0.6200\n",
      "Epoch 883/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5472 - acc: 1.0000 - val_loss: 3.7775 - val_acc: 0.6600\n",
      "Epoch 884/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5432 - acc: 1.0000 - val_loss: 4.0670 - val_acc: 0.6300\n",
      "Epoch 885/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5289 - acc: 1.0000 - val_loss: 4.0305 - val_acc: 0.6600\n",
      "Epoch 886/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5216 - acc: 1.0000 - val_loss: 4.3052 - val_acc: 0.6300\n",
      "Epoch 887/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5138 - acc: 1.0000 - val_loss: 4.1713 - val_acc: 0.6000\n",
      "Epoch 888/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5043 - acc: 1.0000 - val_loss: 3.5418 - val_acc: 0.7300\n",
      "Epoch 889/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5027 - acc: 0.9950 - val_loss: 3.2481 - val_acc: 0.8200\n",
      "Epoch 890/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4928 - acc: 0.9950 - val_loss: 3.6238 - val_acc: 0.7300\n",
      "Epoch 891/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4786 - acc: 1.0000 - val_loss: 3.8280 - val_acc: 0.6900\n",
      "Epoch 892/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4697 - acc: 1.0000 - val_loss: 3.9239 - val_acc: 0.6900\n",
      "Epoch 893/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4614 - acc: 1.0000 - val_loss: 3.9406 - val_acc: 0.6900\n",
      "Epoch 894/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4601 - acc: 0.9950 - val_loss: 4.0182 - val_acc: 0.6800\n",
      "Epoch 895/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4610 - acc: 0.9900 - val_loss: 4.0394 - val_acc: 0.6800\n",
      "Epoch 896/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5033 - acc: 0.9850 - val_loss: 5.5421 - val_acc: 0.4800\n",
      "Epoch 897/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.6210 - acc: 0.9450 - val_loss: 7.9602 - val_acc: 0.3000\n",
      "Epoch 898/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5441 - acc: 0.9550 - val_loss: 6.7033 - val_acc: 0.3300\n",
      "Epoch 899/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5388 - acc: 0.9700 - val_loss: 5.9416 - val_acc: 0.3400\n",
      "Epoch 900/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4437 - acc: 0.9950 - val_loss: 3.8793 - val_acc: 0.6100\n",
      "Epoch 901/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.5136 - acc: 0.9900 - val_loss: 4.1577 - val_acc: 0.6200\n",
      "Epoch 902/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4695 - acc: 0.9900 - val_loss: 4.0857 - val_acc: 0.6400\n",
      "Epoch 903/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4628 - acc: 0.9850 - val_loss: 4.5455 - val_acc: 0.5700\n",
      "Epoch 904/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4249 - acc: 0.9950 - val_loss: 5.1826 - val_acc: 0.4300\n",
      "Epoch 905/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4370 - acc: 0.9900 - val_loss: 4.5815 - val_acc: 0.5600\n",
      "Epoch 906/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.4116 - acc: 0.9900 - val_loss: 3.8256 - val_acc: 0.6200\n",
      "Epoch 907/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3866 - acc: 1.0000 - val_loss: 3.9189 - val_acc: 0.5700\n",
      "Epoch 908/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3831 - acc: 1.0000 - val_loss: 4.1112 - val_acc: 0.5300\n",
      "Epoch 909/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3943 - acc: 0.9900 - val_loss: 4.1414 - val_acc: 0.5700\n",
      "Epoch 910/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3774 - acc: 0.9900 - val_loss: 3.8334 - val_acc: 0.6000\n",
      "Epoch 911/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3887 - acc: 0.9850 - val_loss: 3.8608 - val_acc: 0.6500\n",
      "Epoch 912/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3712 - acc: 0.9950 - val_loss: 3.8903 - val_acc: 0.6300\n",
      "Epoch 913/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3778 - acc: 0.9900 - val_loss: 3.5946 - val_acc: 0.6900\n",
      "Epoch 914/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3433 - acc: 1.0000 - val_loss: 4.2769 - val_acc: 0.6100\n",
      "Epoch 915/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3386 - acc: 0.9950 - val_loss: 4.8636 - val_acc: 0.5300\n",
      "Epoch 916/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3337 - acc: 0.9950 - val_loss: 4.9427 - val_acc: 0.5300\n",
      "Epoch 917/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3198 - acc: 1.0000 - val_loss: 4.8596 - val_acc: 0.5300\n",
      "Epoch 918/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3111 - acc: 1.0000 - val_loss: 4.5586 - val_acc: 0.5600\n",
      "Epoch 919/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3037 - acc: 1.0000 - val_loss: 4.5080 - val_acc: 0.5600\n",
      "Epoch 920/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2974 - acc: 1.0000 - val_loss: 4.7646 - val_acc: 0.5600\n",
      "Epoch 921/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2903 - acc: 1.0000 - val_loss: 4.5661 - val_acc: 0.5700\n",
      "Epoch 922/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2824 - acc: 1.0000 - val_loss: 4.7144 - val_acc: 0.5500\n",
      "Epoch 923/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2963 - acc: 0.9950 - val_loss: 5.0117 - val_acc: 0.5400\n",
      "Epoch 924/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2990 - acc: 0.9850 - val_loss: 6.0921 - val_acc: 0.4700\n",
      "Epoch 925/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2924 - acc: 0.9850 - val_loss: 5.5189 - val_acc: 0.5100\n",
      "Epoch 926/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2692 - acc: 0.9950 - val_loss: 5.3322 - val_acc: 0.4500\n",
      "Epoch 927/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2808 - acc: 0.9950 - val_loss: 5.5399 - val_acc: 0.4200\n",
      "Epoch 928/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3104 - acc: 0.9800 - val_loss: 5.1413 - val_acc: 0.4500\n",
      "Epoch 929/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3317 - acc: 0.9750 - val_loss: 4.4994 - val_acc: 0.5700\n",
      "Epoch 930/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2935 - acc: 0.9900 - val_loss: 4.7719 - val_acc: 0.5100\n",
      "Epoch 931/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2961 - acc: 0.9700 - val_loss: 5.4121 - val_acc: 0.4800\n",
      "Epoch 932/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2718 - acc: 0.9750 - val_loss: 5.0702 - val_acc: 0.5800\n",
      "Epoch 933/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2473 - acc: 0.9900 - val_loss: 4.1516 - val_acc: 0.7100\n",
      "Epoch 934/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2557 - acc: 0.9900 - val_loss: 3.4530 - val_acc: 0.7700\n",
      "Epoch 935/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2221 - acc: 1.0000 - val_loss: 3.2822 - val_acc: 0.7500\n",
      "Epoch 936/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2128 - acc: 1.0000 - val_loss: 3.4546 - val_acc: 0.6900\n",
      "Epoch 937/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2178 - acc: 0.9950 - val_loss: 3.3437 - val_acc: 0.6900\n",
      "Epoch 938/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2004 - acc: 1.0000 - val_loss: 3.7131 - val_acc: 0.6700\n",
      "Epoch 939/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2009 - acc: 1.0000 - val_loss: 3.9447 - val_acc: 0.6500\n",
      "Epoch 940/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1962 - acc: 0.9950 - val_loss: 4.3145 - val_acc: 0.5800\n",
      "Epoch 941/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1821 - acc: 1.0000 - val_loss: 4.8034 - val_acc: 0.5500\n",
      "Epoch 942/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1782 - acc: 1.0000 - val_loss: 4.9088 - val_acc: 0.5500\n",
      "Epoch 943/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1798 - acc: 0.9950 - val_loss: 4.6494 - val_acc: 0.5500\n",
      "Epoch 944/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1626 - acc: 1.0000 - val_loss: 4.5025 - val_acc: 0.5700\n",
      "Epoch 945/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1583 - acc: 1.0000 - val_loss: 4.7020 - val_acc: 0.5700\n",
      "Epoch 946/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1493 - acc: 1.0000 - val_loss: 4.4678 - val_acc: 0.5800\n",
      "Epoch 947/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1433 - acc: 1.0000 - val_loss: 4.3884 - val_acc: 0.5800\n",
      "Epoch 948/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1362 - acc: 1.0000 - val_loss: 4.8789 - val_acc: 0.5500\n",
      "Epoch 949/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1327 - acc: 1.0000 - val_loss: 4.5878 - val_acc: 0.5500\n",
      "Epoch 950/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1276 - acc: 1.0000 - val_loss: 4.7387 - val_acc: 0.5500\n",
      "Epoch 951/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1205 - acc: 1.0000 - val_loss: 5.0682 - val_acc: 0.5400\n",
      "Epoch 952/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1138 - acc: 1.0000 - val_loss: 4.5778 - val_acc: 0.5600\n",
      "Epoch 953/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1050 - acc: 1.0000 - val_loss: 4.8933 - val_acc: 0.5500\n",
      "Epoch 954/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1015 - acc: 1.0000 - val_loss: 4.9382 - val_acc: 0.5600\n",
      "Epoch 955/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0973 - acc: 1.0000 - val_loss: 4.9288 - val_acc: 0.5500\n",
      "Epoch 956/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1111 - acc: 0.9900 - val_loss: 4.8390 - val_acc: 0.5600\n",
      "Epoch 957/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1568 - acc: 0.9950 - val_loss: 4.1748 - val_acc: 0.6200\n",
      "Epoch 958/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.3374 - acc: 0.9350 - val_loss: 5.3598 - val_acc: 0.4800\n",
      "Epoch 959/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.2404 - acc: 0.9350 - val_loss: 4.7824 - val_acc: 0.5200\n",
      "Epoch 960/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1292 - acc: 0.9800 - val_loss: 3.9611 - val_acc: 0.6100\n",
      "Epoch 961/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0662 - acc: 1.0000 - val_loss: 3.8389 - val_acc: 0.6400\n",
      "Epoch 962/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0644 - acc: 1.0000 - val_loss: 3.7821 - val_acc: 0.6400\n",
      "Epoch 963/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0645 - acc: 0.9950 - val_loss: 4.0025 - val_acc: 0.5900\n",
      "Epoch 964/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0527 - acc: 1.0000 - val_loss: 4.4901 - val_acc: 0.5300\n",
      "Epoch 965/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0468 - acc: 1.0000 - val_loss: 4.3813 - val_acc: 0.5500\n",
      "Epoch 966/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0407 - acc: 1.0000 - val_loss: 4.3769 - val_acc: 0.5400\n",
      "Epoch 967/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0314 - acc: 1.0000 - val_loss: 4.2465 - val_acc: 0.5600\n",
      "Epoch 968/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0246 - acc: 1.0000 - val_loss: 4.4572 - val_acc: 0.5300\n",
      "Epoch 969/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0183 - acc: 1.0000 - val_loss: 4.0812 - val_acc: 0.5700\n",
      "Epoch 970/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0139 - acc: 1.0000 - val_loss: 4.1787 - val_acc: 0.5500\n",
      "Epoch 971/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0083 - acc: 1.0000 - val_loss: 4.1103 - val_acc: 0.5500\n",
      "Epoch 972/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.0002 - acc: 1.0000 - val_loss: 4.0711 - val_acc: 0.5500\n",
      "Epoch 973/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9945 - acc: 1.0000 - val_loss: 4.1752 - val_acc: 0.5100\n",
      "Epoch 974/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9884 - acc: 1.0000 - val_loss: 3.9315 - val_acc: 0.5500\n",
      "Epoch 975/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9825 - acc: 1.0000 - val_loss: 4.0398 - val_acc: 0.5500\n",
      "Epoch 976/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9775 - acc: 1.0000 - val_loss: 4.0674 - val_acc: 0.5300\n",
      "Epoch 977/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9707 - acc: 1.0000 - val_loss: 4.1976 - val_acc: 0.5300\n",
      "Epoch 978/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9670 - acc: 1.0000 - val_loss: 4.0364 - val_acc: 0.6000\n",
      "Epoch 979/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9593 - acc: 1.0000 - val_loss: 4.0410 - val_acc: 0.5900\n",
      "Epoch 980/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9536 - acc: 1.0000 - val_loss: 3.9668 - val_acc: 0.6000\n",
      "Epoch 981/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9470 - acc: 1.0000 - val_loss: 4.1929 - val_acc: 0.5600\n",
      "Epoch 982/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9417 - acc: 1.0000 - val_loss: 4.0780 - val_acc: 0.5600\n",
      "Epoch 983/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9361 - acc: 1.0000 - val_loss: 3.9903 - val_acc: 0.5900\n",
      "Epoch 984/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9295 - acc: 1.0000 - val_loss: 4.2794 - val_acc: 0.5400\n",
      "Epoch 985/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9234 - acc: 1.0000 - val_loss: 4.3700 - val_acc: 0.5600\n",
      "Epoch 986/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9174 - acc: 1.0000 - val_loss: 4.2288 - val_acc: 0.5600\n",
      "Epoch 987/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9115 - acc: 1.0000 - val_loss: 4.2407 - val_acc: 0.5500\n",
      "Epoch 988/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.9061 - acc: 1.0000 - val_loss: 4.3398 - val_acc: 0.5300\n",
      "Epoch 989/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8999 - acc: 1.0000 - val_loss: 4.2868 - val_acc: 0.5400\n",
      "Epoch 990/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8946 - acc: 1.0000 - val_loss: 4.0660 - val_acc: 0.5900\n",
      "Epoch 991/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8880 - acc: 1.0000 - val_loss: 4.0861 - val_acc: 0.5700\n",
      "Epoch 992/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8825 - acc: 1.0000 - val_loss: 4.0757 - val_acc: 0.5600\n",
      "Epoch 993/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8764 - acc: 1.0000 - val_loss: 4.0649 - val_acc: 0.5400\n",
      "Epoch 994/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8711 - acc: 1.0000 - val_loss: 4.0492 - val_acc: 0.5400\n",
      "Epoch 995/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8649 - acc: 1.0000 - val_loss: 4.0396 - val_acc: 0.5600\n",
      "Epoch 996/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8592 - acc: 1.0000 - val_loss: 4.0978 - val_acc: 0.5300\n",
      "Epoch 997/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8531 - acc: 1.0000 - val_loss: 3.9866 - val_acc: 0.5500\n",
      "Epoch 998/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8471 - acc: 1.0000 - val_loss: 3.9009 - val_acc: 0.5600\n",
      "Epoch 999/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8418 - acc: 1.0000 - val_loss: 4.1810 - val_acc: 0.5200\n",
      "Epoch 1000/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8356 - acc: 1.0000 - val_loss: 3.9800 - val_acc: 0.5600\n",
      "Epoch 1001/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8298 - acc: 1.0000 - val_loss: 3.9958 - val_acc: 0.5600\n",
      "Epoch 1002/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8238 - acc: 1.0000 - val_loss: 3.8881 - val_acc: 0.5600\n",
      "Epoch 1003/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8182 - acc: 1.0000 - val_loss: 4.0218 - val_acc: 0.5600\n",
      "Epoch 1004/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8319 - acc: 0.9950 - val_loss: 3.6473 - val_acc: 0.5900\n",
      "Epoch 1005/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8104 - acc: 1.0000 - val_loss: 2.3526 - val_acc: 0.8100\n",
      "Epoch 1006/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8139 - acc: 0.9950 - val_loss: 2.2915 - val_acc: 0.8100\n",
      "Epoch 1007/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8259 - acc: 0.9950 - val_loss: 2.3894 - val_acc: 0.8200\n",
      "Epoch 1008/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8065 - acc: 0.9900 - val_loss: 2.8704 - val_acc: 0.6600\n",
      "Epoch 1009/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7980 - acc: 0.9950 - val_loss: 3.2175 - val_acc: 0.6200\n",
      "Epoch 1010/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7910 - acc: 0.9950 - val_loss: 3.2654 - val_acc: 0.6500\n",
      "Epoch 1011/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7836 - acc: 1.0000 - val_loss: 3.3235 - val_acc: 0.6500\n",
      "Epoch 1012/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8010 - acc: 0.9950 - val_loss: 3.1391 - val_acc: 0.7000\n",
      "Epoch 1013/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7901 - acc: 0.9900 - val_loss: 2.8899 - val_acc: 0.7800\n",
      "Epoch 1014/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7663 - acc: 1.0000 - val_loss: 3.3230 - val_acc: 0.6900\n",
      "Epoch 1015/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7598 - acc: 1.0000 - val_loss: 3.3678 - val_acc: 0.6800\n",
      "Epoch 1016/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7574 - acc: 1.0000 - val_loss: 3.2075 - val_acc: 0.6900\n",
      "Epoch 1017/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7527 - acc: 1.0000 - val_loss: 3.3149 - val_acc: 0.6100\n",
      "Epoch 1018/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7458 - acc: 1.0000 - val_loss: 3.3191 - val_acc: 0.6400\n",
      "Epoch 1019/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7461 - acc: 0.9950 - val_loss: 3.3533 - val_acc: 0.6300\n",
      "Epoch 1020/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7512 - acc: 0.9900 - val_loss: 3.6184 - val_acc: 0.5900\n",
      "Epoch 1021/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7711 - acc: 0.9850 - val_loss: 3.5000 - val_acc: 0.6200\n",
      "Epoch 1022/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7622 - acc: 0.9900 - val_loss: 4.1879 - val_acc: 0.5800\n",
      "Epoch 1023/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7997 - acc: 0.9650 - val_loss: 2.5504 - val_acc: 0.6700\n",
      "Epoch 1024/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7399 - acc: 0.9950 - val_loss: 3.1786 - val_acc: 0.5700\n",
      "Epoch 1025/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7399 - acc: 0.9950 - val_loss: 3.6729 - val_acc: 0.5500\n",
      "Epoch 1026/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7180 - acc: 1.0000 - val_loss: 3.9923 - val_acc: 0.5300\n",
      "Epoch 1027/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7122 - acc: 1.0000 - val_loss: 3.8264 - val_acc: 0.5300\n",
      "Epoch 1028/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7142 - acc: 0.9950 - val_loss: 4.0635 - val_acc: 0.5200\n",
      "Epoch 1029/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7395 - acc: 0.9900 - val_loss: 3.5887 - val_acc: 0.5300\n",
      "Epoch 1030/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6993 - acc: 1.0000 - val_loss: 3.6159 - val_acc: 0.5400\n",
      "Epoch 1031/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7912 - acc: 0.9700 - val_loss: 3.8649 - val_acc: 0.5200\n",
      "Epoch 1032/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6893 - acc: 1.0000 - val_loss: 3.8759 - val_acc: 0.5300\n",
      "Epoch 1033/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6833 - acc: 1.0000 - val_loss: 3.4085 - val_acc: 0.5700\n",
      "Epoch 1034/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6807 - acc: 1.0000 - val_loss: 3.8211 - val_acc: 0.5300\n",
      "Epoch 1035/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6854 - acc: 0.9950 - val_loss: 3.5479 - val_acc: 0.5600\n",
      "Epoch 1036/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6725 - acc: 1.0000 - val_loss: 3.3465 - val_acc: 0.5600\n",
      "Epoch 1037/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6659 - acc: 1.0000 - val_loss: 3.6230 - val_acc: 0.5400\n",
      "Epoch 1038/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6574 - acc: 1.0000 - val_loss: 3.4572 - val_acc: 0.5400\n",
      "Epoch 1039/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6551 - acc: 1.0000 - val_loss: 3.3110 - val_acc: 0.5400\n",
      "Epoch 1040/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6477 - acc: 1.0000 - val_loss: 3.1909 - val_acc: 0.5400\n",
      "Epoch 1041/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6419 - acc: 1.0000 - val_loss: 3.5058 - val_acc: 0.5200\n",
      "Epoch 1042/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6371 - acc: 1.0000 - val_loss: 3.6105 - val_acc: 0.5200\n",
      "Epoch 1043/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6318 - acc: 1.0000 - val_loss: 3.6511 - val_acc: 0.5100\n",
      "Epoch 1044/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6279 - acc: 1.0000 - val_loss: 3.6274 - val_acc: 0.5200\n",
      "Epoch 1045/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6217 - acc: 1.0000 - val_loss: 3.4669 - val_acc: 0.5100\n",
      "Epoch 1046/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6237 - acc: 0.9950 - val_loss: 3.5978 - val_acc: 0.5000\n",
      "Epoch 1047/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6117 - acc: 1.0000 - val_loss: 3.4519 - val_acc: 0.5100\n",
      "Epoch 1048/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6086 - acc: 1.0000 - val_loss: 3.2365 - val_acc: 0.5300\n",
      "Epoch 1049/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6034 - acc: 1.0000 - val_loss: 3.4872 - val_acc: 0.5100\n",
      "Epoch 1050/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6026 - acc: 0.9950 - val_loss: 3.5123 - val_acc: 0.5200\n",
      "Epoch 1051/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5934 - acc: 1.0000 - val_loss: 3.4337 - val_acc: 0.5200\n",
      "Epoch 1052/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5878 - acc: 1.0000 - val_loss: 3.5325 - val_acc: 0.5200\n",
      "Epoch 1053/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.7006 - acc: 0.9850 - val_loss: 3.4898 - val_acc: 0.5200\n",
      "Epoch 1054/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6504 - acc: 0.9900 - val_loss: 2.8336 - val_acc: 0.6500\n",
      "Epoch 1055/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6917 - acc: 0.9800 - val_loss: 2.9328 - val_acc: 0.6500\n",
      "Epoch 1056/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5973 - acc: 0.9900 - val_loss: 3.1820 - val_acc: 0.6200\n",
      "Epoch 1057/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5692 - acc: 1.0000 - val_loss: 2.8618 - val_acc: 0.6700\n",
      "Epoch 1058/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5659 - acc: 1.0000 - val_loss: 2.8881 - val_acc: 0.6700\n",
      "Epoch 1059/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5609 - acc: 1.0000 - val_loss: 3.4722 - val_acc: 0.6100\n",
      "Epoch 1060/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5819 - acc: 0.9950 - val_loss: 3.8706 - val_acc: 0.5800\n",
      "Epoch 1061/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5866 - acc: 0.9850 - val_loss: 4.2152 - val_acc: 0.5400\n",
      "Epoch 1062/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5655 - acc: 0.9950 - val_loss: 4.2398 - val_acc: 0.5100\n",
      "Epoch 1063/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5432 - acc: 1.0000 - val_loss: 3.7865 - val_acc: 0.5300\n",
      "Epoch 1064/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5426 - acc: 1.0000 - val_loss: 4.2782 - val_acc: 0.5000\n",
      "Epoch 1065/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5504 - acc: 0.9950 - val_loss: 4.0527 - val_acc: 0.5100\n",
      "Epoch 1066/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5292 - acc: 1.0000 - val_loss: 4.0699 - val_acc: 0.5300\n",
      "Epoch 1067/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5450 - acc: 0.9950 - val_loss: 3.7352 - val_acc: 0.5300\n",
      "Epoch 1068/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5216 - acc: 1.0000 - val_loss: 2.2552 - val_acc: 0.6500\n",
      "Epoch 1069/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5173 - acc: 1.0000 - val_loss: 2.2409 - val_acc: 0.6300\n",
      "Epoch 1070/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5139 - acc: 1.0000 - val_loss: 2.4289 - val_acc: 0.6000\n",
      "Epoch 1071/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5160 - acc: 0.9950 - val_loss: 2.4885 - val_acc: 0.5900\n",
      "Epoch 1072/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5024 - acc: 1.0000 - val_loss: 2.8491 - val_acc: 0.5500\n",
      "Epoch 1073/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4987 - acc: 1.0000 - val_loss: 2.9154 - val_acc: 0.5400\n",
      "Epoch 1074/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4937 - acc: 1.0000 - val_loss: 3.0521 - val_acc: 0.5400\n",
      "Epoch 1075/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4944 - acc: 1.0000 - val_loss: 2.9585 - val_acc: 0.5300\n",
      "Epoch 1076/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4844 - acc: 1.0000 - val_loss: 3.2611 - val_acc: 0.5000\n",
      "Epoch 1077/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4795 - acc: 1.0000 - val_loss: 3.1212 - val_acc: 0.5200\n",
      "Epoch 1078/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4752 - acc: 1.0000 - val_loss: 3.4747 - val_acc: 0.5200\n",
      "Epoch 1079/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5177 - acc: 0.9950 - val_loss: 2.8186 - val_acc: 0.5100\n",
      "Epoch 1080/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8816 - acc: 0.8900 - val_loss: 11.7666 - val_acc: 0.2800\n",
      "Epoch 1081/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8851 - acc: 0.8950 - val_loss: 13.3681 - val_acc: 0.2100\n",
      "Epoch 1082/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.8727 - acc: 0.8800 - val_loss: 15.4044 - val_acc: 0.1200\n",
      "Epoch 1083/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.6009 - acc: 0.9550 - val_loss: 17.0791 - val_acc: 0.0100\n",
      "Epoch 1084/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5618 - acc: 0.9700 - val_loss: 16.9518 - val_acc: 0.0000e+00\n",
      "Epoch 1085/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5037 - acc: 0.9900 - val_loss: 15.3499 - val_acc: 0.0500\n",
      "Epoch 1086/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4841 - acc: 1.0000 - val_loss: 14.3301 - val_acc: 0.1200\n",
      "Epoch 1087/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4852 - acc: 0.9950 - val_loss: 14.0648 - val_acc: 0.1100\n",
      "Epoch 1088/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4890 - acc: 0.9950 - val_loss: 13.8807 - val_acc: 0.1100\n",
      "Epoch 1089/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4773 - acc: 0.9950 - val_loss: 13.7248 - val_acc: 0.1400\n",
      "Epoch 1090/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4752 - acc: 0.9950 - val_loss: 13.6605 - val_acc: 0.1200\n",
      "Epoch 1091/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4703 - acc: 1.0000 - val_loss: 13.5598 - val_acc: 0.1100\n",
      "Epoch 1092/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5073 - acc: 0.9950 - val_loss: 13.5521 - val_acc: 0.1200\n",
      "Epoch 1093/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4688 - acc: 0.9950 - val_loss: 13.6750 - val_acc: 0.0900\n",
      "Epoch 1094/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4955 - acc: 0.9800 - val_loss: 13.0653 - val_acc: 0.1200\n",
      "Epoch 1095/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4640 - acc: 0.9900 - val_loss: 12.3520 - val_acc: 0.0900\n",
      "Epoch 1096/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4885 - acc: 0.9900 - val_loss: 12.2494 - val_acc: 0.1000\n",
      "Epoch 1097/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4619 - acc: 0.9950 - val_loss: 11.3423 - val_acc: 0.1100\n",
      "Epoch 1098/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4584 - acc: 0.9900 - val_loss: 10.6261 - val_acc: 0.1400\n",
      "Epoch 1099/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4584 - acc: 0.9950 - val_loss: 9.2596 - val_acc: 0.1600\n",
      "Epoch 1100/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4497 - acc: 0.9950 - val_loss: 8.4072 - val_acc: 0.1800\n",
      "Epoch 1101/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4346 - acc: 1.0000 - val_loss: 6.2669 - val_acc: 0.2800\n",
      "Epoch 1102/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4967 - acc: 0.9900 - val_loss: 6.9856 - val_acc: 0.1500\n",
      "Epoch 1103/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4731 - acc: 0.9700 - val_loss: 7.2203 - val_acc: 0.1500\n",
      "Epoch 1104/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4705 - acc: 0.9950 - val_loss: 4.3720 - val_acc: 0.5000\n",
      "Epoch 1105/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4394 - acc: 0.9900 - val_loss: 4.1565 - val_acc: 0.5000\n",
      "Epoch 1106/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4387 - acc: 0.9950 - val_loss: 4.2806 - val_acc: 0.5000\n",
      "Epoch 1107/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4163 - acc: 1.0000 - val_loss: 4.3873 - val_acc: 0.5000\n",
      "Epoch 1108/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4393 - acc: 0.9950 - val_loss: 4.1412 - val_acc: 0.5000\n",
      "Epoch 1109/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4466 - acc: 0.9900 - val_loss: 4.9894 - val_acc: 0.4900\n",
      "Epoch 1110/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.5395 - acc: 0.9650 - val_loss: 4.6632 - val_acc: 0.5700\n",
      "Epoch 1111/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4298 - acc: 0.9950 - val_loss: 6.1043 - val_acc: 0.4600\n",
      "Epoch 1112/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4096 - acc: 1.0000 - val_loss: 6.5417 - val_acc: 0.4500\n",
      "Epoch 1113/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4108 - acc: 0.9900 - val_loss: 7.1313 - val_acc: 0.3900\n",
      "Epoch 1114/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3998 - acc: 1.0000 - val_loss: 6.2140 - val_acc: 0.4400\n",
      "Epoch 1115/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3984 - acc: 0.9950 - val_loss: 6.2637 - val_acc: 0.4500\n",
      "Epoch 1116/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3979 - acc: 0.9950 - val_loss: 5.3552 - val_acc: 0.4600\n",
      "Epoch 1117/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4082 - acc: 0.9850 - val_loss: 4.7530 - val_acc: 0.5100\n",
      "Epoch 1118/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4861 - acc: 0.9850 - val_loss: 3.9471 - val_acc: 0.5300\n",
      "Epoch 1119/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3861 - acc: 0.9950 - val_loss: 4.0477 - val_acc: 0.5300\n",
      "Epoch 1120/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4145 - acc: 0.9950 - val_loss: 4.2808 - val_acc: 0.5300\n",
      "Epoch 1121/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4095 - acc: 0.9900 - val_loss: 3.3027 - val_acc: 0.5400\n",
      "Epoch 1122/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4074 - acc: 0.9900 - val_loss: 4.0122 - val_acc: 0.5100\n",
      "Epoch 1123/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3759 - acc: 0.9950 - val_loss: 4.0619 - val_acc: 0.4900\n",
      "Epoch 1124/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3760 - acc: 0.9950 - val_loss: 4.1167 - val_acc: 0.5000\n",
      "Epoch 1125/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3651 - acc: 1.0000 - val_loss: 4.1764 - val_acc: 0.5300\n",
      "Epoch 1126/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3640 - acc: 1.0000 - val_loss: 3.8639 - val_acc: 0.5400\n",
      "Epoch 1127/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3607 - acc: 1.0000 - val_loss: 3.2936 - val_acc: 0.5400\n",
      "Epoch 1128/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3611 - acc: 0.9950 - val_loss: 3.1734 - val_acc: 0.5400\n",
      "Epoch 1129/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3876 - acc: 0.9950 - val_loss: 3.1932 - val_acc: 0.5500\n",
      "Epoch 1130/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4063 - acc: 0.9900 - val_loss: 3.0056 - val_acc: 0.5400\n",
      "Epoch 1131/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4050 - acc: 0.9750 - val_loss: 3.4896 - val_acc: 0.5200\n",
      "Epoch 1132/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3944 - acc: 0.9900 - val_loss: 4.0913 - val_acc: 0.5000\n",
      "Epoch 1133/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3640 - acc: 0.9950 - val_loss: 4.0246 - val_acc: 0.5000\n",
      "Epoch 1134/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3447 - acc: 1.0000 - val_loss: 3.9677 - val_acc: 0.5000\n",
      "Epoch 1135/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3382 - acc: 1.0000 - val_loss: 3.7674 - val_acc: 0.5000\n",
      "Epoch 1136/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3332 - acc: 1.0000 - val_loss: 3.7570 - val_acc: 0.5100\n",
      "Epoch 1137/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3325 - acc: 1.0000 - val_loss: 3.5897 - val_acc: 0.5300\n",
      "Epoch 1138/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3280 - acc: 1.0000 - val_loss: 3.6675 - val_acc: 0.5200\n",
      "Epoch 1139/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3243 - acc: 1.0000 - val_loss: 3.9568 - val_acc: 0.5100\n",
      "Epoch 1140/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3208 - acc: 1.0000 - val_loss: 3.6709 - val_acc: 0.5300\n",
      "Epoch 1141/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3203 - acc: 1.0000 - val_loss: 3.7473 - val_acc: 0.5200\n",
      "Epoch 1142/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3152 - acc: 1.0000 - val_loss: 3.7528 - val_acc: 0.5200\n",
      "Epoch 1143/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3127 - acc: 1.0000 - val_loss: 3.9998 - val_acc: 0.5100\n",
      "Epoch 1144/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3089 - acc: 1.0000 - val_loss: 3.9738 - val_acc: 0.5100\n",
      "Epoch 1145/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3054 - acc: 1.0000 - val_loss: 3.9772 - val_acc: 0.5200\n",
      "Epoch 1146/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3023 - acc: 1.0000 - val_loss: 4.0382 - val_acc: 0.5100\n",
      "Epoch 1147/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3007 - acc: 1.0000 - val_loss: 4.0127 - val_acc: 0.5200\n",
      "Epoch 1148/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2963 - acc: 1.0000 - val_loss: 4.1098 - val_acc: 0.5100\n",
      "Epoch 1149/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2937 - acc: 1.0000 - val_loss: 4.0792 - val_acc: 0.5100\n",
      "Epoch 1150/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2901 - acc: 1.0000 - val_loss: 4.0214 - val_acc: 0.5200\n",
      "Epoch 1151/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2882 - acc: 1.0000 - val_loss: 4.2446 - val_acc: 0.5200\n",
      "Epoch 1152/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2854 - acc: 1.0000 - val_loss: 3.7764 - val_acc: 0.5200\n",
      "Epoch 1153/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2810 - acc: 1.0000 - val_loss: 3.7799 - val_acc: 0.5300\n",
      "Epoch 1154/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2779 - acc: 1.0000 - val_loss: 3.9023 - val_acc: 0.5200\n",
      "Epoch 1155/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2773 - acc: 1.0000 - val_loss: 3.8960 - val_acc: 0.5200\n",
      "Epoch 1156/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2726 - acc: 1.0000 - val_loss: 3.7932 - val_acc: 0.5200\n",
      "Epoch 1157/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2691 - acc: 1.0000 - val_loss: 3.6707 - val_acc: 0.5200\n",
      "Epoch 1158/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2665 - acc: 1.0000 - val_loss: 3.4220 - val_acc: 0.5400\n",
      "Epoch 1159/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2636 - acc: 1.0000 - val_loss: 3.7668 - val_acc: 0.5200\n",
      "Epoch 1160/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.3294 - acc: 0.9800 - val_loss: 3.3718 - val_acc: 0.5500\n",
      "Epoch 1161/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2603 - acc: 1.0000 - val_loss: 2.9223 - val_acc: 0.5900\n",
      "Epoch 1162/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2585 - acc: 1.0000 - val_loss: 3.2739 - val_acc: 0.6000\n",
      "Epoch 1163/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2572 - acc: 1.0000 - val_loss: 3.2652 - val_acc: 0.5800\n",
      "Epoch 1164/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2752 - acc: 0.9850 - val_loss: 3.4881 - val_acc: 0.5400\n",
      "Epoch 1165/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2606 - acc: 0.9950 - val_loss: 3.5907 - val_acc: 0.5500\n",
      "Epoch 1166/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2474 - acc: 1.0000 - val_loss: 3.1246 - val_acc: 0.5600\n",
      "Epoch 1167/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2509 - acc: 1.0000 - val_loss: 3.3042 - val_acc: 0.5400\n",
      "Epoch 1168/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2400 - acc: 1.0000 - val_loss: 4.0593 - val_acc: 0.5100\n",
      "Epoch 1169/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2379 - acc: 1.0000 - val_loss: 4.0611 - val_acc: 0.5100\n",
      "Epoch 1170/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2360 - acc: 1.0000 - val_loss: 3.5480 - val_acc: 0.5400\n",
      "Epoch 1171/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2331 - acc: 1.0000 - val_loss: 3.6158 - val_acc: 0.5300\n",
      "Epoch 1172/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2385 - acc: 0.9950 - val_loss: 3.6801 - val_acc: 0.5300\n",
      "Epoch 1173/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2286 - acc: 1.0000 - val_loss: 4.0522 - val_acc: 0.5400\n",
      "Epoch 1174/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2409 - acc: 0.9900 - val_loss: 3.9599 - val_acc: 0.5300\n",
      "Epoch 1175/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2227 - acc: 1.0000 - val_loss: 3.2376 - val_acc: 0.5900\n",
      "Epoch 1176/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2231 - acc: 1.0000 - val_loss: 3.8528 - val_acc: 0.5500\n",
      "Epoch 1177/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2242 - acc: 0.9900 - val_loss: 3.3037 - val_acc: 0.5800\n",
      "Epoch 1178/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2187 - acc: 1.0000 - val_loss: 3.4178 - val_acc: 0.5800\n",
      "Epoch 1179/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2593 - acc: 0.9900 - val_loss: 3.3530 - val_acc: 0.6000\n",
      "Epoch 1180/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2169 - acc: 0.9950 - val_loss: 2.5514 - val_acc: 0.7100\n",
      "Epoch 1181/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.4143 - acc: 0.9750 - val_loss: 2.9853 - val_acc: 0.6400\n",
      "Epoch 1182/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2963 - acc: 0.9800 - val_loss: 3.0450 - val_acc: 0.6100\n",
      "Epoch 1183/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2318 - acc: 0.9900 - val_loss: 3.3773 - val_acc: 0.5900\n",
      "Epoch 1184/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2386 - acc: 0.9850 - val_loss: 2.9970 - val_acc: 0.6200\n",
      "Epoch 1185/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2076 - acc: 0.9950 - val_loss: 3.1949 - val_acc: 0.5900\n",
      "Epoch 1186/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2430 - acc: 0.9900 - val_loss: 3.0414 - val_acc: 0.6100\n",
      "Epoch 1187/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2036 - acc: 1.0000 - val_loss: 3.3467 - val_acc: 0.5900\n",
      "Epoch 1188/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2331 - acc: 0.9850 - val_loss: 3.6441 - val_acc: 0.6000\n",
      "Epoch 1189/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2046 - acc: 0.9900 - val_loss: 3.2247 - val_acc: 0.5800\n",
      "Epoch 1190/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2035 - acc: 0.9950 - val_loss: 4.0772 - val_acc: 0.5200\n",
      "Epoch 1191/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1913 - acc: 1.0000 - val_loss: 4.3168 - val_acc: 0.5100\n",
      "Epoch 1192/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2229 - acc: 0.9950 - val_loss: 3.7338 - val_acc: 0.5200\n",
      "Epoch 1193/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1880 - acc: 1.0000 - val_loss: 3.9436 - val_acc: 0.5300\n",
      "Epoch 1194/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1932 - acc: 0.9950 - val_loss: 3.8262 - val_acc: 0.5300\n",
      "Epoch 1195/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1858 - acc: 0.9950 - val_loss: 3.6420 - val_acc: 0.5300\n",
      "Epoch 1196/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1783 - acc: 1.0000 - val_loss: 3.6166 - val_acc: 0.5300\n",
      "Epoch 1197/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1737 - acc: 1.0000 - val_loss: 3.5428 - val_acc: 0.5300\n",
      "Epoch 1198/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1720 - acc: 1.0000 - val_loss: 3.2539 - val_acc: 0.5500\n",
      "Epoch 1199/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1682 - acc: 1.0000 - val_loss: 3.4306 - val_acc: 0.5200\n",
      "Epoch 1200/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1658 - acc: 1.0000 - val_loss: 3.3681 - val_acc: 0.5300\n",
      "Epoch 1201/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1616 - acc: 1.0000 - val_loss: 3.4231 - val_acc: 0.5200\n",
      "Epoch 1202/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2016 - acc: 0.9800 - val_loss: 3.4158 - val_acc: 0.5400\n",
      "Epoch 1203/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1567 - acc: 1.0000 - val_loss: 2.8547 - val_acc: 0.6000\n",
      "Epoch 1204/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1681 - acc: 0.9900 - val_loss: 2.7807 - val_acc: 0.5900\n",
      "Epoch 1205/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1541 - acc: 1.0000 - val_loss: 2.8994 - val_acc: 0.5700\n",
      "Epoch 1206/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1538 - acc: 1.0000 - val_loss: 3.0834 - val_acc: 0.5400\n",
      "Epoch 1207/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1658 - acc: 0.9950 - val_loss: 3.1998 - val_acc: 0.5400\n",
      "Epoch 1208/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1497 - acc: 1.0000 - val_loss: 3.2220 - val_acc: 0.5300\n",
      "Epoch 1209/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1907 - acc: 0.9850 - val_loss: 3.3226 - val_acc: 0.5300\n",
      "Epoch 1210/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1420 - acc: 1.0000 - val_loss: 3.1069 - val_acc: 0.5700\n",
      "Epoch 1211/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1423 - acc: 1.0000 - val_loss: 2.6490 - val_acc: 0.5900\n",
      "Epoch 1212/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.2281 - acc: 0.9900 - val_loss: 2.9684 - val_acc: 0.5600\n",
      "Epoch 1213/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1331 - acc: 1.0000 - val_loss: 2.5122 - val_acc: 0.5900\n",
      "Epoch 1214/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1366 - acc: 1.0000 - val_loss: 2.5227 - val_acc: 0.6000\n",
      "Epoch 1215/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1321 - acc: 1.0000 - val_loss: 2.8897 - val_acc: 0.5500\n",
      "Epoch 1216/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1357 - acc: 0.9950 - val_loss: 2.6707 - val_acc: 0.5600\n",
      "Epoch 1217/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1279 - acc: 1.0000 - val_loss: 3.1938 - val_acc: 0.5300\n",
      "Epoch 1218/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1257 - acc: 1.0000 - val_loss: 2.9458 - val_acc: 0.5600\n",
      "Epoch 1219/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1229 - acc: 1.0000 - val_loss: 3.2988 - val_acc: 0.5200\n",
      "Epoch 1220/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1188 - acc: 1.0000 - val_loss: 2.8834 - val_acc: 0.5500\n",
      "Epoch 1221/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1165 - acc: 1.0000 - val_loss: 3.5093 - val_acc: 0.5100\n",
      "Epoch 1222/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1129 - acc: 1.0000 - val_loss: 3.4213 - val_acc: 0.5100\n",
      "Epoch 1223/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1098 - acc: 1.0000 - val_loss: 3.4716 - val_acc: 0.5100\n",
      "Epoch 1224/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1076 - acc: 1.0000 - val_loss: 3.4524 - val_acc: 0.5100\n",
      "Epoch 1225/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1055 - acc: 1.0000 - val_loss: 3.1862 - val_acc: 0.5300\n",
      "Epoch 1226/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1113 - acc: 0.9950 - val_loss: 3.6507 - val_acc: 0.5100\n",
      "Epoch 1227/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0997 - acc: 1.0000 - val_loss: 3.7446 - val_acc: 0.5100\n",
      "Epoch 1228/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0978 - acc: 1.0000 - val_loss: 3.5083 - val_acc: 0.5100\n",
      "Epoch 1229/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0949 - acc: 1.0000 - val_loss: 3.4564 - val_acc: 0.5100\n",
      "Epoch 1230/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0919 - acc: 1.0000 - val_loss: 3.6237 - val_acc: 0.5100\n",
      "Epoch 1231/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0893 - acc: 1.0000 - val_loss: 3.6239 - val_acc: 0.5100\n",
      "Epoch 1232/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1044 - acc: 0.9950 - val_loss: 3.4516 - val_acc: 0.5100\n",
      "Epoch 1233/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0879 - acc: 1.0000 - val_loss: 3.5478 - val_acc: 0.5000\n",
      "Epoch 1234/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1762 - acc: 0.9650 - val_loss: 3.3882 - val_acc: 0.5000\n",
      "Epoch 1235/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0843 - acc: 1.0000 - val_loss: 4.0885 - val_acc: 0.4900\n",
      "Epoch 1236/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.1387 - acc: 0.9850 - val_loss: 3.9513 - val_acc: 0.5100\n",
      "Epoch 1237/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0806 - acc: 1.0000 - val_loss: 3.7385 - val_acc: 0.5100\n",
      "Epoch 1238/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0769 - acc: 1.0000 - val_loss: 3.6309 - val_acc: 0.5100\n",
      "Epoch 1239/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0913 - acc: 0.9950 - val_loss: 3.4536 - val_acc: 0.5100\n",
      "Epoch 1240/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0885 - acc: 0.9950 - val_loss: 3.2843 - val_acc: 0.5100\n",
      "Epoch 1241/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0715 - acc: 1.0000 - val_loss: 2.9767 - val_acc: 0.5500\n",
      "Epoch 1242/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0692 - acc: 1.0000 - val_loss: 2.8954 - val_acc: 0.5200\n",
      "Epoch 1243/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0697 - acc: 1.0000 - val_loss: 2.8623 - val_acc: 0.5200\n",
      "Epoch 1244/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0647 - acc: 1.0000 - val_loss: 2.7209 - val_acc: 0.5100\n",
      "Epoch 1245/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0608 - acc: 1.0000 - val_loss: 2.7275 - val_acc: 0.5200\n",
      "Epoch 1246/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0781 - acc: 0.9950 - val_loss: 2.3377 - val_acc: 0.5400\n",
      "Epoch 1247/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0729 - acc: 0.9900 - val_loss: 1.9828 - val_acc: 0.5900\n",
      "Epoch 1248/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0663 - acc: 1.0000 - val_loss: 1.9861 - val_acc: 0.6100\n",
      "Epoch 1249/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0569 - acc: 1.0000 - val_loss: 2.1216 - val_acc: 0.5400\n",
      "Epoch 1250/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0589 - acc: 1.0000 - val_loss: 2.1882 - val_acc: 0.5400\n",
      "Epoch 1251/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0652 - acc: 0.9950 - val_loss: 2.4526 - val_acc: 0.5300\n",
      "Epoch 1252/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0507 - acc: 1.0000 - val_loss: 2.6617 - val_acc: 0.5100\n",
      "Epoch 1253/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0482 - acc: 1.0000 - val_loss: 2.5780 - val_acc: 0.5300\n",
      "Epoch 1254/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0461 - acc: 1.0000 - val_loss: 2.7198 - val_acc: 0.5300\n",
      "Epoch 1255/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0443 - acc: 1.0000 - val_loss: 2.9492 - val_acc: 0.5000\n",
      "Epoch 1256/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0518 - acc: 0.9900 - val_loss: 2.8063 - val_acc: 0.5200\n",
      "Epoch 1257/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0387 - acc: 1.0000 - val_loss: 2.9193 - val_acc: 0.5000\n",
      "Epoch 1258/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0495 - acc: 0.9950 - val_loss: 2.7496 - val_acc: 0.5100\n",
      "Epoch 1259/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0354 - acc: 1.0000 - val_loss: 2.7641 - val_acc: 0.5000\n",
      "Epoch 1260/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0342 - acc: 1.0000 - val_loss: 2.8237 - val_acc: 0.5000\n",
      "Epoch 1261/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0302 - acc: 1.0000 - val_loss: 2.8700 - val_acc: 0.5000\n",
      "Epoch 1262/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0304 - acc: 1.0000 - val_loss: 2.7393 - val_acc: 0.5000\n",
      "Epoch 1263/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0239 - acc: 1.0000 - val_loss: 2.8134 - val_acc: 0.5000\n",
      "Epoch 1264/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0211 - acc: 1.0000 - val_loss: 2.7162 - val_acc: 0.5000\n",
      "Epoch 1265/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0195 - acc: 1.0000 - val_loss: 2.7422 - val_acc: 0.5000\n",
      "Epoch 1266/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0163 - acc: 1.0000 - val_loss: 2.7235 - val_acc: 0.5000\n",
      "Epoch 1267/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0144 - acc: 1.0000 - val_loss: 2.7481 - val_acc: 0.5000\n",
      "Epoch 1268/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0111 - acc: 1.0000 - val_loss: 2.6119 - val_acc: 0.5000\n",
      "Epoch 1269/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0091 - acc: 1.0000 - val_loss: 2.6096 - val_acc: 0.5000\n",
      "Epoch 1270/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0062 - acc: 1.0000 - val_loss: 2.6299 - val_acc: 0.5000\n",
      "Epoch 1271/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0046 - acc: 1.0000 - val_loss: 2.6106 - val_acc: 0.5000\n",
      "Epoch 1272/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0013 - acc: 1.0000 - val_loss: 2.5799 - val_acc: 0.5000\n",
      "Epoch 1273/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9988 - acc: 1.0000 - val_loss: 2.6248 - val_acc: 0.5000\n",
      "Epoch 1274/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9963 - acc: 1.0000 - val_loss: 2.5489 - val_acc: 0.5000\n",
      "Epoch 1275/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0357 - acc: 0.9950 - val_loss: 2.5063 - val_acc: 0.5000\n",
      "Epoch 1276/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9919 - acc: 1.0000 - val_loss: 2.2635 - val_acc: 0.5000\n",
      "Epoch 1277/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 1.0490 - acc: 0.9850 - val_loss: 2.5308 - val_acc: 0.5000\n",
      "Epoch 1278/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9952 - acc: 1.0000 - val_loss: 2.3876 - val_acc: 0.5000\n",
      "Epoch 1279/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9887 - acc: 1.0000 - val_loss: 2.3996 - val_acc: 0.5000\n",
      "Epoch 1280/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9866 - acc: 1.0000 - val_loss: 2.3666 - val_acc: 0.5000\n",
      "Epoch 1281/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9847 - acc: 1.0000 - val_loss: 2.4329 - val_acc: 0.5000\n",
      "Epoch 1282/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9857 - acc: 1.0000 - val_loss: 2.1546 - val_acc: 0.5200\n",
      "Epoch 1283/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9798 - acc: 1.0000 - val_loss: 2.2530 - val_acc: 0.5300\n",
      "Epoch 1284/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9782 - acc: 1.0000 - val_loss: 2.3309 - val_acc: 0.5400\n",
      "Epoch 1285/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9760 - acc: 1.0000 - val_loss: 2.3733 - val_acc: 0.5400\n",
      "Epoch 1286/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9744 - acc: 1.0000 - val_loss: 2.8198 - val_acc: 0.5200\n",
      "Epoch 1287/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9714 - acc: 1.0000 - val_loss: 2.4228 - val_acc: 0.5300\n",
      "Epoch 1288/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9684 - acc: 1.0000 - val_loss: 2.6174 - val_acc: 0.5200\n",
      "Epoch 1289/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9660 - acc: 1.0000 - val_loss: 2.6713 - val_acc: 0.5200\n",
      "Epoch 1290/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9633 - acc: 1.0000 - val_loss: 3.1583 - val_acc: 0.5100\n",
      "Epoch 1291/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9610 - acc: 1.0000 - val_loss: 2.6337 - val_acc: 0.5200\n",
      "Epoch 1292/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9599 - acc: 1.0000 - val_loss: 2.9562 - val_acc: 0.5100\n",
      "Epoch 1293/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9563 - acc: 1.0000 - val_loss: 2.9998 - val_acc: 0.5000\n",
      "Epoch 1294/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9539 - acc: 1.0000 - val_loss: 3.1067 - val_acc: 0.5000\n",
      "Epoch 1295/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9514 - acc: 1.0000 - val_loss: 2.8811 - val_acc: 0.5100\n",
      "Epoch 1296/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9500 - acc: 1.0000 - val_loss: 2.9927 - val_acc: 0.5000\n",
      "Epoch 1297/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9472 - acc: 1.0000 - val_loss: 2.8731 - val_acc: 0.5100\n",
      "Epoch 1298/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9444 - acc: 1.0000 - val_loss: 3.0473 - val_acc: 0.5000\n",
      "Epoch 1299/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9749 - acc: 0.9900 - val_loss: 3.2043 - val_acc: 0.5000\n",
      "Epoch 1300/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9476 - acc: 0.9950 - val_loss: 2.7310 - val_acc: 0.5400\n",
      "Epoch 1301/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9738 - acc: 0.9900 - val_loss: 3.6726 - val_acc: 0.5000\n",
      "Epoch 1302/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9456 - acc: 1.0000 - val_loss: 3.0057 - val_acc: 0.5300\n",
      "Epoch 1303/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9372 - acc: 1.0000 - val_loss: 3.0099 - val_acc: 0.5300\n",
      "Epoch 1304/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9357 - acc: 1.0000 - val_loss: 2.9347 - val_acc: 0.5400\n",
      "Epoch 1305/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9735 - acc: 0.9950 - val_loss: 3.6125 - val_acc: 0.5100\n",
      "Epoch 1306/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9569 - acc: 0.9950 - val_loss: 2.7474 - val_acc: 0.5500\n",
      "Epoch 1307/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9363 - acc: 0.9950 - val_loss: 3.1320 - val_acc: 0.5400\n",
      "Epoch 1308/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9304 - acc: 1.0000 - val_loss: 2.4657 - val_acc: 0.6100\n",
      "Epoch 1309/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9580 - acc: 0.9900 - val_loss: 3.1295 - val_acc: 0.5100\n",
      "Epoch 1310/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9294 - acc: 1.0000 - val_loss: 3.2679 - val_acc: 0.5100\n",
      "Epoch 1311/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9290 - acc: 0.9950 - val_loss: 3.0923 - val_acc: 0.5100\n",
      "Epoch 1312/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9218 - acc: 1.0000 - val_loss: 2.9049 - val_acc: 0.5000\n",
      "Epoch 1313/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9196 - acc: 1.0000 - val_loss: 2.7799 - val_acc: 0.5000\n",
      "Epoch 1314/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9186 - acc: 1.0000 - val_loss: 2.5054 - val_acc: 0.5100\n",
      "Epoch 1315/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9160 - acc: 1.0000 - val_loss: 2.6389 - val_acc: 0.5000\n",
      "Epoch 1316/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9136 - acc: 1.0000 - val_loss: 2.5587 - val_acc: 0.5000\n",
      "Epoch 1317/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9105 - acc: 1.0000 - val_loss: 2.6734 - val_acc: 0.5000\n",
      "Epoch 1318/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9085 - acc: 1.0000 - val_loss: 2.8335 - val_acc: 0.5000\n",
      "Epoch 1319/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9059 - acc: 1.0000 - val_loss: 2.6098 - val_acc: 0.5000\n",
      "Epoch 1320/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9036 - acc: 1.0000 - val_loss: 2.7246 - val_acc: 0.5000\n",
      "Epoch 1321/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9019 - acc: 1.0000 - val_loss: 2.5401 - val_acc: 0.5000\n",
      "Epoch 1322/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9183 - acc: 0.9950 - val_loss: 2.7250 - val_acc: 0.5000\n",
      "Epoch 1323/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8968 - acc: 1.0000 - val_loss: 2.6582 - val_acc: 0.5000\n",
      "Epoch 1324/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8950 - acc: 1.0000 - val_loss: 2.7527 - val_acc: 0.5000\n",
      "Epoch 1325/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8936 - acc: 1.0000 - val_loss: 2.5640 - val_acc: 0.5000\n",
      "Epoch 1326/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8902 - acc: 1.0000 - val_loss: 2.7453 - val_acc: 0.5000\n",
      "Epoch 1327/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8880 - acc: 1.0000 - val_loss: 2.6036 - val_acc: 0.5000\n",
      "Epoch 1328/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8870 - acc: 1.0000 - val_loss: 2.6338 - val_acc: 0.5000\n",
      "Epoch 1329/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.9048 - acc: 0.9850 - val_loss: 2.6271 - val_acc: 0.5000\n",
      "Epoch 1330/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8824 - acc: 1.0000 - val_loss: 2.8194 - val_acc: 0.5000\n",
      "Epoch 1331/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8809 - acc: 1.0000 - val_loss: 2.9020 - val_acc: 0.5000\n",
      "Epoch 1332/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8803 - acc: 1.0000 - val_loss: 2.7042 - val_acc: 0.5000\n",
      "Epoch 1333/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8893 - acc: 0.9950 - val_loss: 2.8198 - val_acc: 0.5000\n",
      "Epoch 1334/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8762 - acc: 1.0000 - val_loss: 3.2223 - val_acc: 0.5000\n",
      "Epoch 1335/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8749 - acc: 1.0000 - val_loss: 3.0487 - val_acc: 0.5000\n",
      "Epoch 1336/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8720 - acc: 1.0000 - val_loss: 3.2600 - val_acc: 0.5000\n",
      "Epoch 1337/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8698 - acc: 1.0000 - val_loss: 3.2758 - val_acc: 0.5000\n",
      "Epoch 1338/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8675 - acc: 1.0000 - val_loss: 3.2095 - val_acc: 0.5000\n",
      "Epoch 1339/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8662 - acc: 1.0000 - val_loss: 3.1772 - val_acc: 0.5000\n",
      "Epoch 1340/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8634 - acc: 1.0000 - val_loss: 3.0650 - val_acc: 0.5000\n",
      "Epoch 1341/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8609 - acc: 1.0000 - val_loss: 3.1815 - val_acc: 0.5000\n",
      "Epoch 1342/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8586 - acc: 1.0000 - val_loss: 3.1230 - val_acc: 0.5000\n",
      "Epoch 1343/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8565 - acc: 1.0000 - val_loss: 3.3378 - val_acc: 0.5000\n",
      "Epoch 1344/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8547 - acc: 1.0000 - val_loss: 3.0328 - val_acc: 0.5000\n",
      "Epoch 1345/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8522 - acc: 1.0000 - val_loss: 3.1404 - val_acc: 0.5000\n",
      "Epoch 1346/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8521 - acc: 1.0000 - val_loss: 3.1802 - val_acc: 0.5000\n",
      "Epoch 1347/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8478 - acc: 1.0000 - val_loss: 2.7178 - val_acc: 0.5000\n",
      "Epoch 1348/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8456 - acc: 1.0000 - val_loss: 2.7181 - val_acc: 0.5000\n",
      "Epoch 1349/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8436 - acc: 1.0000 - val_loss: 2.7742 - val_acc: 0.5000\n",
      "Epoch 1350/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8412 - acc: 1.0000 - val_loss: 2.8696 - val_acc: 0.5000\n",
      "Epoch 1351/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8392 - acc: 1.0000 - val_loss: 2.8046 - val_acc: 0.5000\n",
      "Epoch 1352/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8369 - acc: 1.0000 - val_loss: 2.9009 - val_acc: 0.5000\n",
      "Epoch 1353/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8347 - acc: 1.0000 - val_loss: 2.7011 - val_acc: 0.5000\n",
      "Epoch 1354/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8325 - acc: 1.0000 - val_loss: 2.8825 - val_acc: 0.5000\n",
      "Epoch 1355/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8304 - acc: 1.0000 - val_loss: 2.8850 - val_acc: 0.5000\n",
      "Epoch 1356/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8284 - acc: 1.0000 - val_loss: 2.7737 - val_acc: 0.5000\n",
      "Epoch 1357/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8327 - acc: 0.9950 - val_loss: 2.9915 - val_acc: 0.5000\n",
      "Epoch 1358/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8245 - acc: 1.0000 - val_loss: 2.3762 - val_acc: 0.5000\n",
      "Epoch 1359/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8498 - acc: 0.9900 - val_loss: 2.5918 - val_acc: 0.5000\n",
      "Epoch 1360/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8251 - acc: 1.0000 - val_loss: 2.6327 - val_acc: 0.5000\n",
      "Epoch 1361/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8227 - acc: 1.0000 - val_loss: 2.7762 - val_acc: 0.5000\n",
      "Epoch 1362/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8215 - acc: 1.0000 - val_loss: 2.7194 - val_acc: 0.5100\n",
      "Epoch 1363/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8194 - acc: 1.0000 - val_loss: 2.8209 - val_acc: 0.5000\n",
      "Epoch 1364/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8174 - acc: 1.0000 - val_loss: 2.4682 - val_acc: 0.5200\n",
      "Epoch 1365/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8156 - acc: 1.0000 - val_loss: 2.7673 - val_acc: 0.5000\n",
      "Epoch 1366/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8135 - acc: 1.0000 - val_loss: 2.5486 - val_acc: 0.5200\n",
      "Epoch 1367/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8112 - acc: 1.0000 - val_loss: 2.9720 - val_acc: 0.5000\n",
      "Epoch 1368/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8100 - acc: 1.0000 - val_loss: 3.0269 - val_acc: 0.5000\n",
      "Epoch 1369/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8069 - acc: 1.0000 - val_loss: 2.7376 - val_acc: 0.5100\n",
      "Epoch 1370/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8046 - acc: 1.0000 - val_loss: 2.7811 - val_acc: 0.5000\n",
      "Epoch 1371/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8220 - acc: 0.9950 - val_loss: 3.1312 - val_acc: 0.5100\n",
      "Epoch 1372/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8087 - acc: 0.9950 - val_loss: 2.7092 - val_acc: 0.5700\n",
      "Epoch 1373/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8112 - acc: 0.9950 - val_loss: 3.5715 - val_acc: 0.5300\n",
      "Epoch 1374/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8019 - acc: 1.0000 - val_loss: 4.2598 - val_acc: 0.5000\n",
      "Epoch 1375/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8013 - acc: 1.0000 - val_loss: 3.5390 - val_acc: 0.5100\n",
      "Epoch 1376/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8441 - acc: 0.9900 - val_loss: 3.7566 - val_acc: 0.5100\n",
      "Epoch 1377/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7981 - acc: 1.0000 - val_loss: 4.2339 - val_acc: 0.5000\n",
      "Epoch 1378/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7966 - acc: 1.0000 - val_loss: 3.7464 - val_acc: 0.5000\n",
      "Epoch 1379/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7955 - acc: 1.0000 - val_loss: 4.3101 - val_acc: 0.5000\n",
      "Epoch 1380/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8111 - acc: 0.9900 - val_loss: 4.1764 - val_acc: 0.5000\n",
      "Epoch 1381/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7910 - acc: 1.0000 - val_loss: 7.8024 - val_acc: 0.1700\n",
      "Epoch 1382/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8076 - acc: 0.9950 - val_loss: 6.5127 - val_acc: 0.2400\n",
      "Epoch 1383/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8335 - acc: 0.9900 - val_loss: 7.1939 - val_acc: 0.3600\n",
      "Epoch 1384/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8318 - acc: 0.9950 - val_loss: 13.4483 - val_acc: 0.0500\n",
      "Epoch 1385/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7931 - acc: 1.0000 - val_loss: 14.2665 - val_acc: 0.0400\n",
      "Epoch 1386/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7932 - acc: 1.0000 - val_loss: 13.6308 - val_acc: 0.0500\n",
      "Epoch 1387/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7890 - acc: 1.0000 - val_loss: 8.3821 - val_acc: 0.3700\n",
      "Epoch 1388/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7874 - acc: 1.0000 - val_loss: 6.4175 - val_acc: 0.4900\n",
      "Epoch 1389/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7873 - acc: 1.0000 - val_loss: 6.0292 - val_acc: 0.4900\n",
      "Epoch 1390/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7837 - acc: 1.0000 - val_loss: 5.7949 - val_acc: 0.5000\n",
      "Epoch 1391/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7817 - acc: 1.0000 - val_loss: 5.4229 - val_acc: 0.5000\n",
      "Epoch 1392/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7801 - acc: 1.0000 - val_loss: 5.0430 - val_acc: 0.5000\n",
      "Epoch 1393/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7779 - acc: 1.0000 - val_loss: 4.4381 - val_acc: 0.5000\n",
      "Epoch 1394/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7758 - acc: 1.0000 - val_loss: 4.3958 - val_acc: 0.5000\n",
      "Epoch 1395/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7739 - acc: 1.0000 - val_loss: 4.2533 - val_acc: 0.5000\n",
      "Epoch 1396/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7791 - acc: 0.9950 - val_loss: 4.0274 - val_acc: 0.5000\n",
      "Epoch 1397/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7701 - acc: 1.0000 - val_loss: 3.6178 - val_acc: 0.5000\n",
      "Epoch 1398/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7744 - acc: 0.9950 - val_loss: 3.6260 - val_acc: 0.5000\n",
      "Epoch 1399/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7692 - acc: 1.0000 - val_loss: 3.4708 - val_acc: 0.5000\n",
      "Epoch 1400/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7722 - acc: 1.0000 - val_loss: 3.7601 - val_acc: 0.5000\n",
      "Epoch 1401/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8025 - acc: 0.9950 - val_loss: 3.6180 - val_acc: 0.5000\n",
      "Epoch 1402/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8498 - acc: 0.9900 - val_loss: 3.4298 - val_acc: 0.4600\n",
      "Epoch 1403/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8838 - acc: 0.9800 - val_loss: 2.8555 - val_acc: 0.4600\n",
      "Epoch 1404/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7830 - acc: 0.9950 - val_loss: 2.7014 - val_acc: 0.4400\n",
      "Epoch 1405/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7673 - acc: 1.0000 - val_loss: 3.7573 - val_acc: 0.4500\n",
      "Epoch 1406/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7699 - acc: 1.0000 - val_loss: 3.6296 - val_acc: 0.3500\n",
      "Epoch 1407/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8479 - acc: 0.9900 - val_loss: 3.5014 - val_acc: 0.5000\n",
      "Epoch 1408/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7648 - acc: 1.0000 - val_loss: 3.7422 - val_acc: 0.5000\n",
      "Epoch 1409/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7676 - acc: 1.0000 - val_loss: 3.7579 - val_acc: 0.5000\n",
      "Epoch 1410/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7876 - acc: 0.9950 - val_loss: 3.6582 - val_acc: 0.5000\n",
      "Epoch 1411/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7808 - acc: 0.9950 - val_loss: 3.4466 - val_acc: 0.5000\n",
      "Epoch 1412/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7707 - acc: 0.9950 - val_loss: 2.7096 - val_acc: 0.5100\n",
      "Epoch 1413/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8041 - acc: 0.9800 - val_loss: 4.1414 - val_acc: 0.5000\n",
      "Epoch 1414/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7593 - acc: 1.0000 - val_loss: 6.4842 - val_acc: 0.3600\n",
      "Epoch 1415/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7579 - acc: 1.0000 - val_loss: 7.7513 - val_acc: 0.2300\n",
      "Epoch 1416/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7565 - acc: 1.0000 - val_loss: 12.7780 - val_acc: 0.0200\n",
      "Epoch 1417/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7553 - acc: 1.0000 - val_loss: 7.0578 - val_acc: 0.2700\n",
      "Epoch 1418/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7597 - acc: 0.9950 - val_loss: 10.5883 - val_acc: 0.0100\n",
      "Epoch 1419/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7840 - acc: 0.9950 - val_loss: 4.2530 - val_acc: 0.4800\n",
      "Epoch 1420/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7499 - acc: 1.0000 - val_loss: 3.4664 - val_acc: 0.5000\n",
      "Epoch 1421/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8420 - acc: 0.9900 - val_loss: 3.7815 - val_acc: 0.5000\n",
      "Epoch 1422/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7535 - acc: 1.0000 - val_loss: 4.6605 - val_acc: 0.5000\n",
      "Epoch 1423/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7553 - acc: 1.0000 - val_loss: 4.7568 - val_acc: 0.5000\n",
      "Epoch 1424/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7548 - acc: 1.0000 - val_loss: 4.0743 - val_acc: 0.5000\n",
      "Epoch 1425/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7542 - acc: 1.0000 - val_loss: 4.1556 - val_acc: 0.5000\n",
      "Epoch 1426/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7528 - acc: 1.0000 - val_loss: 3.8540 - val_acc: 0.5000\n",
      "Epoch 1427/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7511 - acc: 1.0000 - val_loss: 3.2528 - val_acc: 0.5000\n",
      "Epoch 1428/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7492 - acc: 1.0000 - val_loss: 2.9265 - val_acc: 0.5000\n",
      "Epoch 1429/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8114 - acc: 0.9900 - val_loss: 3.2282 - val_acc: 0.5000\n",
      "Epoch 1430/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7683 - acc: 0.9950 - val_loss: 4.0292 - val_acc: 0.5000\n",
      "Epoch 1431/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7478 - acc: 1.0000 - val_loss: 4.3388 - val_acc: 0.5000\n",
      "Epoch 1432/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7503 - acc: 0.9950 - val_loss: 4.1437 - val_acc: 0.5000\n",
      "Epoch 1433/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7490 - acc: 0.9950 - val_loss: 3.8085 - val_acc: 0.5000\n",
      "Epoch 1434/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7438 - acc: 1.0000 - val_loss: 3.5806 - val_acc: 0.5000\n",
      "Epoch 1435/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7451 - acc: 0.9950 - val_loss: 3.1152 - val_acc: 0.5000\n",
      "Epoch 1436/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7390 - acc: 1.0000 - val_loss: 2.3626 - val_acc: 0.5000\n",
      "Epoch 1437/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7375 - acc: 1.0000 - val_loss: 2.1616 - val_acc: 0.5000\n",
      "Epoch 1438/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7376 - acc: 1.0000 - val_loss: 2.3281 - val_acc: 0.5000\n",
      "Epoch 1439/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7347 - acc: 1.0000 - val_loss: 1.9291 - val_acc: 0.5000\n",
      "Epoch 1440/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7330 - acc: 1.0000 - val_loss: 2.1037 - val_acc: 0.5000\n",
      "Epoch 1441/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7310 - acc: 1.0000 - val_loss: 2.1436 - val_acc: 0.5000\n",
      "Epoch 1442/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7298 - acc: 1.0000 - val_loss: 2.1170 - val_acc: 0.5000\n",
      "Epoch 1443/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7276 - acc: 1.0000 - val_loss: 2.1135 - val_acc: 0.5000\n",
      "Epoch 1444/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7254 - acc: 1.0000 - val_loss: 2.2594 - val_acc: 0.5000\n",
      "Epoch 1445/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7235 - acc: 1.0000 - val_loss: 2.2190 - val_acc: 0.5000\n",
      "Epoch 1446/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7855 - acc: 0.9850 - val_loss: 2.3151 - val_acc: 0.5000\n",
      "Epoch 1447/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7209 - acc: 1.0000 - val_loss: 2.9957 - val_acc: 0.5000\n",
      "Epoch 1448/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7202 - acc: 1.0000 - val_loss: 3.5150 - val_acc: 0.5000\n",
      "Epoch 1449/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7191 - acc: 1.0000 - val_loss: 3.5989 - val_acc: 0.5000\n",
      "Epoch 1450/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7196 - acc: 1.0000 - val_loss: 3.4767 - val_acc: 0.5200\n",
      "Epoch 1451/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7156 - acc: 1.0000 - val_loss: 3.6554 - val_acc: 0.5200\n",
      "Epoch 1452/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7144 - acc: 1.0000 - val_loss: 3.6498 - val_acc: 0.5000\n",
      "Epoch 1453/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7138 - acc: 1.0000 - val_loss: 3.4847 - val_acc: 0.5000\n",
      "Epoch 1454/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7152 - acc: 0.9950 - val_loss: 3.7939 - val_acc: 0.5000\n",
      "Epoch 1455/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7083 - acc: 1.0000 - val_loss: 3.6182 - val_acc: 0.5000\n",
      "Epoch 1456/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7094 - acc: 1.0000 - val_loss: 3.6027 - val_acc: 0.5000\n",
      "Epoch 1457/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7065 - acc: 1.0000 - val_loss: 3.1087 - val_acc: 0.5000\n",
      "Epoch 1458/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7035 - acc: 1.0000 - val_loss: 3.0475 - val_acc: 0.5000\n",
      "Epoch 1459/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7016 - acc: 1.0000 - val_loss: 3.0335 - val_acc: 0.5000\n",
      "Epoch 1460/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6998 - acc: 1.0000 - val_loss: 2.9422 - val_acc: 0.5000\n",
      "Epoch 1461/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6980 - acc: 1.0000 - val_loss: 3.0784 - val_acc: 0.5000\n",
      "Epoch 1462/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7232 - acc: 0.9950 - val_loss: 3.2604 - val_acc: 0.5000\n",
      "Epoch 1463/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6958 - acc: 1.0000 - val_loss: 3.9833 - val_acc: 0.5000\n",
      "Epoch 1464/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7553 - acc: 0.9850 - val_loss: 3.9276 - val_acc: 0.5000\n",
      "Epoch 1465/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6945 - acc: 1.0000 - val_loss: 3.8423 - val_acc: 0.5000\n",
      "Epoch 1466/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6953 - acc: 1.0000 - val_loss: 3.3148 - val_acc: 0.5000\n",
      "Epoch 1467/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6917 - acc: 1.0000 - val_loss: 3.4871 - val_acc: 0.5000\n",
      "Epoch 1468/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6902 - acc: 1.0000 - val_loss: 3.7488 - val_acc: 0.5000\n",
      "Epoch 1469/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6885 - acc: 1.0000 - val_loss: 3.7959 - val_acc: 0.5000\n",
      "Epoch 1470/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6869 - acc: 1.0000 - val_loss: 3.3558 - val_acc: 0.5000\n",
      "Epoch 1471/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6853 - acc: 1.0000 - val_loss: 3.5265 - val_acc: 0.5000\n",
      "Epoch 1472/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6835 - acc: 1.0000 - val_loss: 3.6662 - val_acc: 0.5000\n",
      "Epoch 1473/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6820 - acc: 1.0000 - val_loss: 3.4918 - val_acc: 0.5000\n",
      "Epoch 1474/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6807 - acc: 1.0000 - val_loss: 3.3567 - val_acc: 0.5000\n",
      "Epoch 1475/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6784 - acc: 1.0000 - val_loss: 3.3850 - val_acc: 0.5000\n",
      "Epoch 1476/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6767 - acc: 1.0000 - val_loss: 3.3073 - val_acc: 0.5000\n",
      "Epoch 1477/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6752 - acc: 1.0000 - val_loss: 3.2202 - val_acc: 0.5000\n",
      "Epoch 1478/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6733 - acc: 1.0000 - val_loss: 3.4311 - val_acc: 0.5000\n",
      "Epoch 1479/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6717 - acc: 1.0000 - val_loss: 3.5099 - val_acc: 0.5000\n",
      "Epoch 1480/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6701 - acc: 1.0000 - val_loss: 3.3414 - val_acc: 0.5000\n",
      "Epoch 1481/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6684 - acc: 1.0000 - val_loss: 3.2444 - val_acc: 0.5000\n",
      "Epoch 1482/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6667 - acc: 1.0000 - val_loss: 3.1997 - val_acc: 0.5000\n",
      "Epoch 1483/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6651 - acc: 1.0000 - val_loss: 3.3347 - val_acc: 0.5000\n",
      "Epoch 1484/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6633 - acc: 1.0000 - val_loss: 3.2560 - val_acc: 0.5000\n",
      "Epoch 1485/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6623 - acc: 1.0000 - val_loss: 3.2850 - val_acc: 0.5000\n",
      "Epoch 1486/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6601 - acc: 1.0000 - val_loss: 3.0540 - val_acc: 0.5000\n",
      "Epoch 1487/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6584 - acc: 1.0000 - val_loss: 3.2715 - val_acc: 0.5000\n",
      "Epoch 1488/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6568 - acc: 1.0000 - val_loss: 3.0644 - val_acc: 0.5000\n",
      "Epoch 1489/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6633 - acc: 0.9950 - val_loss: 3.1663 - val_acc: 0.5000\n",
      "Epoch 1490/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7166 - acc: 0.9900 - val_loss: 2.8394 - val_acc: 0.5000\n",
      "Epoch 1491/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6533 - acc: 1.0000 - val_loss: 3.3585 - val_acc: 0.5100\n",
      "Epoch 1492/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7209 - acc: 0.9750 - val_loss: 3.6045 - val_acc: 0.5000\n",
      "Epoch 1493/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7022 - acc: 0.9800 - val_loss: 2.4851 - val_acc: 0.5000\n",
      "Epoch 1494/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.8111 - acc: 0.9650 - val_loss: 1.7294 - val_acc: 0.6600\n",
      "Epoch 1495/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6566 - acc: 1.0000 - val_loss: 2.8316 - val_acc: 0.5000\n",
      "Epoch 1496/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7021 - acc: 0.9700 - val_loss: 2.4456 - val_acc: 0.5200\n",
      "Epoch 1497/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6606 - acc: 1.0000 - val_loss: 1.7013 - val_acc: 0.5400\n",
      "Epoch 1498/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6655 - acc: 1.0000 - val_loss: 1.4703 - val_acc: 0.5700\n",
      "Epoch 1499/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6615 - acc: 1.0000 - val_loss: 1.3500 - val_acc: 0.6000\n",
      "Epoch 1500/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7585 - acc: 0.9800 - val_loss: 2.1093 - val_acc: 0.5000\n",
      "Epoch 1501/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6606 - acc: 1.0000 - val_loss: 2.7886 - val_acc: 0.5000\n",
      "Epoch 1502/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6695 - acc: 1.0000 - val_loss: 2.9703 - val_acc: 0.5000\n",
      "Epoch 1503/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6594 - acc: 1.0000 - val_loss: 3.2206 - val_acc: 0.5000\n",
      "Epoch 1504/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6580 - acc: 1.0000 - val_loss: 3.1542 - val_acc: 0.5000\n",
      "Epoch 1505/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6570 - acc: 1.0000 - val_loss: 3.0380 - val_acc: 0.5000\n",
      "Epoch 1506/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6642 - acc: 0.9950 - val_loss: 3.1396 - val_acc: 0.5000\n",
      "Epoch 1507/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6546 - acc: 1.0000 - val_loss: 2.9908 - val_acc: 0.5000\n",
      "Epoch 1508/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6504 - acc: 1.0000 - val_loss: 3.0075 - val_acc: 0.5000\n",
      "Epoch 1509/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6492 - acc: 1.0000 - val_loss: 3.1730 - val_acc: 0.5000\n",
      "Epoch 1510/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6470 - acc: 1.0000 - val_loss: 3.6125 - val_acc: 0.5000\n",
      "Epoch 1511/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6455 - acc: 1.0000 - val_loss: 3.7645 - val_acc: 0.5000\n",
      "Epoch 1512/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6442 - acc: 1.0000 - val_loss: 3.3608 - val_acc: 0.5000\n",
      "Epoch 1513/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6423 - acc: 1.0000 - val_loss: 3.8347 - val_acc: 0.5000\n",
      "Epoch 1514/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6407 - acc: 1.0000 - val_loss: 3.2127 - val_acc: 0.5000\n",
      "Epoch 1515/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6390 - acc: 1.0000 - val_loss: 3.5220 - val_acc: 0.5000\n",
      "Epoch 1516/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6377 - acc: 1.0000 - val_loss: 3.4526 - val_acc: 0.5000\n",
      "Epoch 1517/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6363 - acc: 1.0000 - val_loss: 3.5131 - val_acc: 0.5000\n",
      "Epoch 1518/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6558 - acc: 0.9900 - val_loss: 3.9276 - val_acc: 0.5000\n",
      "Epoch 1519/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6350 - acc: 1.0000 - val_loss: 4.1254 - val_acc: 0.5000\n",
      "Epoch 1520/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6358 - acc: 1.0000 - val_loss: 4.3873 - val_acc: 0.5000\n",
      "Epoch 1521/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6430 - acc: 1.0000 - val_loss: 3.9536 - val_acc: 0.5000\n",
      "Epoch 1522/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6399 - acc: 1.0000 - val_loss: 4.1359 - val_acc: 0.5000\n",
      "Epoch 1523/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6365 - acc: 1.0000 - val_loss: 4.1168 - val_acc: 0.5000\n",
      "Epoch 1524/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6459 - acc: 0.9900 - val_loss: 4.0630 - val_acc: 0.5000\n",
      "Epoch 1525/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6306 - acc: 1.0000 - val_loss: 3.9945 - val_acc: 0.5000\n",
      "Epoch 1526/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6298 - acc: 1.0000 - val_loss: 3.5331 - val_acc: 0.5000\n",
      "Epoch 1527/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6296 - acc: 1.0000 - val_loss: 3.8460 - val_acc: 0.5000\n",
      "Epoch 1528/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6287 - acc: 1.0000 - val_loss: 3.7850 - val_acc: 0.5000\n",
      "Epoch 1529/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6271 - acc: 1.0000 - val_loss: 3.7562 - val_acc: 0.5000\n",
      "Epoch 1530/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6258 - acc: 1.0000 - val_loss: 3.8851 - val_acc: 0.5000\n",
      "Epoch 1531/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6248 - acc: 1.0000 - val_loss: 3.9825 - val_acc: 0.5000\n",
      "Epoch 1532/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6223 - acc: 1.0000 - val_loss: 4.3891 - val_acc: 0.5000\n",
      "Epoch 1533/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6210 - acc: 1.0000 - val_loss: 4.1839 - val_acc: 0.5000\n",
      "Epoch 1534/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6192 - acc: 1.0000 - val_loss: 4.4001 - val_acc: 0.5000\n",
      "Epoch 1535/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6179 - acc: 1.0000 - val_loss: 4.2896 - val_acc: 0.5000\n",
      "Epoch 1536/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6166 - acc: 1.0000 - val_loss: 4.2009 - val_acc: 0.5000\n",
      "Epoch 1537/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6147 - acc: 1.0000 - val_loss: 4.3965 - val_acc: 0.5000\n",
      "Epoch 1538/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6135 - acc: 1.0000 - val_loss: 4.3738 - val_acc: 0.5000\n",
      "Epoch 1539/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6115 - acc: 1.0000 - val_loss: 4.4391 - val_acc: 0.5000\n",
      "Epoch 1540/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6100 - acc: 1.0000 - val_loss: 4.4042 - val_acc: 0.5000\n",
      "Epoch 1541/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6090 - acc: 1.0000 - val_loss: 4.2724 - val_acc: 0.5000\n",
      "Epoch 1542/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6070 - acc: 1.0000 - val_loss: 4.2963 - val_acc: 0.5000\n",
      "Epoch 1543/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6055 - acc: 1.0000 - val_loss: 4.2304 - val_acc: 0.5000\n",
      "Epoch 1544/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6040 - acc: 1.0000 - val_loss: 3.9687 - val_acc: 0.5000\n",
      "Epoch 1545/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6026 - acc: 1.0000 - val_loss: 4.1952 - val_acc: 0.5000\n",
      "Epoch 1546/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6011 - acc: 1.0000 - val_loss: 3.9322 - val_acc: 0.5000\n",
      "Epoch 1547/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5994 - acc: 1.0000 - val_loss: 4.0479 - val_acc: 0.5000\n",
      "Epoch 1548/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5981 - acc: 1.0000 - val_loss: 3.9738 - val_acc: 0.5000\n",
      "Epoch 1549/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5966 - acc: 1.0000 - val_loss: 4.1167 - val_acc: 0.5000\n",
      "Epoch 1550/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5950 - acc: 1.0000 - val_loss: 4.1201 - val_acc: 0.5000\n",
      "Epoch 1551/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5936 - acc: 1.0000 - val_loss: 3.8196 - val_acc: 0.5000\n",
      "Epoch 1552/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5921 - acc: 1.0000 - val_loss: 3.7772 - val_acc: 0.5000\n",
      "Epoch 1553/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5908 - acc: 1.0000 - val_loss: 3.8499 - val_acc: 0.5000\n",
      "Epoch 1554/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5892 - acc: 1.0000 - val_loss: 3.9935 - val_acc: 0.5000\n",
      "Epoch 1555/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5877 - acc: 1.0000 - val_loss: 3.8462 - val_acc: 0.5000\n",
      "Epoch 1556/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5863 - acc: 1.0000 - val_loss: 3.9765 - val_acc: 0.5000\n",
      "Epoch 1557/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5849 - acc: 1.0000 - val_loss: 4.0151 - val_acc: 0.5000\n",
      "Epoch 1558/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5834 - acc: 1.0000 - val_loss: 3.9339 - val_acc: 0.5000\n",
      "Epoch 1559/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5819 - acc: 1.0000 - val_loss: 3.7801 - val_acc: 0.5000\n",
      "Epoch 1560/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5879 - acc: 0.9950 - val_loss: 3.8199 - val_acc: 0.5000\n",
      "Epoch 1561/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5804 - acc: 1.0000 - val_loss: 2.8259 - val_acc: 0.5000\n",
      "Epoch 1562/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5808 - acc: 1.0000 - val_loss: 2.5560 - val_acc: 0.5000\n",
      "Epoch 1563/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6092 - acc: 0.9900 - val_loss: 2.3671 - val_acc: 0.5000\n",
      "Epoch 1564/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5795 - acc: 1.0000 - val_loss: 3.9768 - val_acc: 0.3500\n",
      "Epoch 1565/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5929 - acc: 0.9950 - val_loss: 2.2564 - val_acc: 0.5000\n",
      "Epoch 1566/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5834 - acc: 1.0000 - val_loss: 2.7418 - val_acc: 0.5000\n",
      "Epoch 1567/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5862 - acc: 1.0000 - val_loss: 2.7908 - val_acc: 0.5000\n",
      "Epoch 1568/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5853 - acc: 1.0000 - val_loss: 2.7241 - val_acc: 0.5000\n",
      "Epoch 1569/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5845 - acc: 1.0000 - val_loss: 2.6231 - val_acc: 0.5000\n",
      "Epoch 1570/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5844 - acc: 1.0000 - val_loss: 2.6326 - val_acc: 0.5000\n",
      "Epoch 1571/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5821 - acc: 1.0000 - val_loss: 2.6292 - val_acc: 0.5000\n",
      "Epoch 1572/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5808 - acc: 1.0000 - val_loss: 2.5742 - val_acc: 0.5000\n",
      "Epoch 1573/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6873 - acc: 0.9800 - val_loss: 2.4063 - val_acc: 0.5000\n",
      "Epoch 1574/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6117 - acc: 0.9900 - val_loss: 2.3093 - val_acc: 0.5000\n",
      "Epoch 1575/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5844 - acc: 1.0000 - val_loss: 2.8228 - val_acc: 0.5100\n",
      "Epoch 1576/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5877 - acc: 1.0000 - val_loss: 2.4121 - val_acc: 0.5300\n",
      "Epoch 1577/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5887 - acc: 1.0000 - val_loss: 2.4780 - val_acc: 0.5100\n",
      "Epoch 1578/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5883 - acc: 1.0000 - val_loss: 2.5387 - val_acc: 0.5100\n",
      "Epoch 1579/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5876 - acc: 1.0000 - val_loss: 2.9047 - val_acc: 0.5000\n",
      "Epoch 1580/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5861 - acc: 1.0000 - val_loss: 2.9291 - val_acc: 0.5000\n",
      "Epoch 1581/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5886 - acc: 0.9950 - val_loss: 3.1034 - val_acc: 0.5000\n",
      "Epoch 1582/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5832 - acc: 1.0000 - val_loss: 2.9151 - val_acc: 0.5100\n",
      "Epoch 1583/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5817 - acc: 1.0000 - val_loss: 3.0282 - val_acc: 0.5000\n",
      "Epoch 1584/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5801 - acc: 1.0000 - val_loss: 3.3606 - val_acc: 0.5000\n",
      "Epoch 1585/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5789 - acc: 1.0000 - val_loss: 3.3596 - val_acc: 0.5000\n",
      "Epoch 1586/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5771 - acc: 1.0000 - val_loss: 3.4089 - val_acc: 0.5000\n",
      "Epoch 1587/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5756 - acc: 1.0000 - val_loss: 3.2386 - val_acc: 0.5000\n",
      "Epoch 1588/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5740 - acc: 1.0000 - val_loss: 3.5593 - val_acc: 0.5000\n",
      "Epoch 1589/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5724 - acc: 1.0000 - val_loss: 3.5117 - val_acc: 0.5000\n",
      "Epoch 1590/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5709 - acc: 1.0000 - val_loss: 3.3446 - val_acc: 0.5000\n",
      "Epoch 1591/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5693 - acc: 1.0000 - val_loss: 3.6223 - val_acc: 0.5000\n",
      "Epoch 1592/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5678 - acc: 1.0000 - val_loss: 3.3537 - val_acc: 0.5000\n",
      "Epoch 1593/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5664 - acc: 1.0000 - val_loss: 3.6961 - val_acc: 0.5000\n",
      "Epoch 1594/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5648 - acc: 1.0000 - val_loss: 3.4080 - val_acc: 0.5000\n",
      "Epoch 1595/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5636 - acc: 1.0000 - val_loss: 3.4270 - val_acc: 0.5000\n",
      "Epoch 1596/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5619 - acc: 1.0000 - val_loss: 3.6225 - val_acc: 0.5000\n",
      "Epoch 1597/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5605 - acc: 1.0000 - val_loss: 3.3881 - val_acc: 0.5000\n",
      "Epoch 1598/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7061 - acc: 0.9800 - val_loss: 3.1607 - val_acc: 0.5000\n",
      "Epoch 1599/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5604 - acc: 1.0000 - val_loss: 2.1088 - val_acc: 0.5000\n",
      "Epoch 1600/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5647 - acc: 1.0000 - val_loss: 2.2456 - val_acc: 0.5000\n",
      "Epoch 1601/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5682 - acc: 1.0000 - val_loss: 2.3519 - val_acc: 0.5200\n",
      "Epoch 1602/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5700 - acc: 1.0000 - val_loss: 2.0807 - val_acc: 0.5300\n",
      "Epoch 1603/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5736 - acc: 0.9950 - val_loss: 2.0456 - val_acc: 0.5300\n",
      "Epoch 1604/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5679 - acc: 1.0000 - val_loss: 1.7530 - val_acc: 0.5300\n",
      "Epoch 1605/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5669 - acc: 1.0000 - val_loss: 1.8907 - val_acc: 0.5300\n",
      "Epoch 1606/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5658 - acc: 1.0000 - val_loss: 1.9440 - val_acc: 0.5300\n",
      "Epoch 1607/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5644 - acc: 1.0000 - val_loss: 1.7752 - val_acc: 0.5300\n",
      "Epoch 1608/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5629 - acc: 1.0000 - val_loss: 1.8721 - val_acc: 0.5300\n",
      "Epoch 1609/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5614 - acc: 1.0000 - val_loss: 1.7686 - val_acc: 0.5300\n",
      "Epoch 1610/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5599 - acc: 1.0000 - val_loss: 1.5146 - val_acc: 0.5400\n",
      "Epoch 1611/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5583 - acc: 1.0000 - val_loss: 1.6346 - val_acc: 0.5300\n",
      "Epoch 1612/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6186 - acc: 0.9850 - val_loss: 1.5878 - val_acc: 0.5600\n",
      "Epoch 1613/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5987 - acc: 0.9850 - val_loss: 2.3594 - val_acc: 0.5300\n",
      "Epoch 1614/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5759 - acc: 0.9900 - val_loss: 2.5652 - val_acc: 0.5100\n",
      "Epoch 1615/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5707 - acc: 0.9950 - val_loss: 1.7151 - val_acc: 0.5800\n",
      "Epoch 1616/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5982 - acc: 0.9900 - val_loss: 1.8228 - val_acc: 0.5400\n",
      "Epoch 1617/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5669 - acc: 1.0000 - val_loss: 1.6932 - val_acc: 0.5300\n",
      "Epoch 1618/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5672 - acc: 1.0000 - val_loss: 1.7101 - val_acc: 0.5000\n",
      "Epoch 1619/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5809 - acc: 0.9900 - val_loss: 1.7509 - val_acc: 0.5000\n",
      "Epoch 1620/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5658 - acc: 1.0000 - val_loss: 2.4790 - val_acc: 0.5000\n",
      "Epoch 1621/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5656 - acc: 1.0000 - val_loss: 2.7255 - val_acc: 0.4500\n",
      "Epoch 1622/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5651 - acc: 1.0000 - val_loss: 2.7041 - val_acc: 0.5000\n",
      "Epoch 1623/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5651 - acc: 1.0000 - val_loss: 2.4970 - val_acc: 0.5000\n",
      "Epoch 1624/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5643 - acc: 1.0000 - val_loss: 2.6730 - val_acc: 0.5000\n",
      "Epoch 1625/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5618 - acc: 1.0000 - val_loss: 2.4221 - val_acc: 0.4900\n",
      "Epoch 1626/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5605 - acc: 1.0000 - val_loss: 2.3905 - val_acc: 0.5000\n",
      "Epoch 1627/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5586 - acc: 1.0000 - val_loss: 2.4521 - val_acc: 0.5000\n",
      "Epoch 1628/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5652 - acc: 1.0000 - val_loss: 2.4534 - val_acc: 0.5000\n",
      "Epoch 1629/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5556 - acc: 1.0000 - val_loss: 2.2864 - val_acc: 0.5000\n",
      "Epoch 1630/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5543 - acc: 1.0000 - val_loss: 2.1527 - val_acc: 0.5000\n",
      "Epoch 1631/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5531 - acc: 1.0000 - val_loss: 2.0993 - val_acc: 0.5000\n",
      "Epoch 1632/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5515 - acc: 1.0000 - val_loss: 2.2054 - val_acc: 0.5000\n",
      "Epoch 1633/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5497 - acc: 1.0000 - val_loss: 2.0829 - val_acc: 0.5000\n",
      "Epoch 1634/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5482 - acc: 1.0000 - val_loss: 2.1359 - val_acc: 0.5000\n",
      "Epoch 1635/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5468 - acc: 1.0000 - val_loss: 1.9951 - val_acc: 0.5100\n",
      "Epoch 1636/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5452 - acc: 1.0000 - val_loss: 2.1000 - val_acc: 0.5100\n",
      "Epoch 1637/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5438 - acc: 1.0000 - val_loss: 2.0671 - val_acc: 0.5000\n",
      "Epoch 1638/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5422 - acc: 1.0000 - val_loss: 2.1834 - val_acc: 0.5000\n",
      "Epoch 1639/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5407 - acc: 1.0000 - val_loss: 1.9682 - val_acc: 0.5200\n",
      "Epoch 1640/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5393 - acc: 1.0000 - val_loss: 2.1191 - val_acc: 0.5000\n",
      "Epoch 1641/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5377 - acc: 1.0000 - val_loss: 2.1046 - val_acc: 0.5000\n",
      "Epoch 1642/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5367 - acc: 1.0000 - val_loss: 2.0378 - val_acc: 0.5000\n",
      "Epoch 1643/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5349 - acc: 1.0000 - val_loss: 1.9711 - val_acc: 0.5200\n",
      "Epoch 1644/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5334 - acc: 1.0000 - val_loss: 1.9690 - val_acc: 0.5100\n",
      "Epoch 1645/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5319 - acc: 1.0000 - val_loss: 1.9356 - val_acc: 0.5200\n",
      "Epoch 1646/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5306 - acc: 1.0000 - val_loss: 1.9430 - val_acc: 0.5200\n",
      "Epoch 1647/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5291 - acc: 1.0000 - val_loss: 1.9994 - val_acc: 0.5000\n",
      "Epoch 1648/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5396 - acc: 0.9900 - val_loss: 1.9805 - val_acc: 0.5000\n",
      "Epoch 1649/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5276 - acc: 1.0000 - val_loss: 1.6051 - val_acc: 0.5600\n",
      "Epoch 1650/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5309 - acc: 1.0000 - val_loss: 1.5167 - val_acc: 0.5700\n",
      "Epoch 1651/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5294 - acc: 0.9950 - val_loss: 1.6136 - val_acc: 0.5700\n",
      "Epoch 1652/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5246 - acc: 1.0000 - val_loss: 1.5730 - val_acc: 0.5600\n",
      "Epoch 1653/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5246 - acc: 1.0000 - val_loss: 1.5995 - val_acc: 0.5700\n",
      "Epoch 1654/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5223 - acc: 1.0000 - val_loss: 1.5751 - val_acc: 0.5400\n",
      "Epoch 1655/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5284 - acc: 0.9950 - val_loss: 1.9220 - val_acc: 0.5300\n",
      "Epoch 1656/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5504 - acc: 0.9950 - val_loss: 1.9450 - val_acc: 0.4100\n",
      "Epoch 1657/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5177 - acc: 1.0000 - val_loss: 1.6381 - val_acc: 0.5200\n",
      "Epoch 1658/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6022 - acc: 0.9900 - val_loss: 1.6436 - val_acc: 0.5000\n",
      "Epoch 1659/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5362 - acc: 0.9900 - val_loss: 2.1349 - val_acc: 0.5000\n",
      "Epoch 1660/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6355 - acc: 0.9800 - val_loss: 2.9494 - val_acc: 0.5000\n",
      "Epoch 1661/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5295 - acc: 0.9950 - val_loss: 3.0269 - val_acc: 0.5000\n",
      "Epoch 1662/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5535 - acc: 0.9800 - val_loss: 2.9580 - val_acc: 0.5000\n",
      "Epoch 1663/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5386 - acc: 0.9900 - val_loss: 2.8142 - val_acc: 0.5000\n",
      "Epoch 1664/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5241 - acc: 1.0000 - val_loss: 2.5640 - val_acc: 0.5300\n",
      "Epoch 1665/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5236 - acc: 1.0000 - val_loss: 2.6467 - val_acc: 0.5000\n",
      "Epoch 1666/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5223 - acc: 1.0000 - val_loss: 2.6678 - val_acc: 0.5000\n",
      "Epoch 1667/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5209 - acc: 1.0000 - val_loss: 2.6874 - val_acc: 0.5000\n",
      "Epoch 1668/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5195 - acc: 1.0000 - val_loss: 2.6637 - val_acc: 0.5000\n",
      "Epoch 1669/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5184 - acc: 1.0000 - val_loss: 2.8290 - val_acc: 0.5000\n",
      "Epoch 1670/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5169 - acc: 1.0000 - val_loss: 2.4326 - val_acc: 0.5000\n",
      "Epoch 1671/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5159 - acc: 1.0000 - val_loss: 2.7948 - val_acc: 0.5000\n",
      "Epoch 1672/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5139 - acc: 1.0000 - val_loss: 2.7478 - val_acc: 0.5000\n",
      "Epoch 1673/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5128 - acc: 1.0000 - val_loss: 2.5590 - val_acc: 0.5000\n",
      "Epoch 1674/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5112 - acc: 1.0000 - val_loss: 2.6429 - val_acc: 0.5000\n",
      "Epoch 1675/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5239 - acc: 0.9950 - val_loss: 2.7042 - val_acc: 0.5000\n",
      "Epoch 1676/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5085 - acc: 1.0000 - val_loss: 2.6049 - val_acc: 0.5000\n",
      "Epoch 1677/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5071 - acc: 1.0000 - val_loss: 2.8517 - val_acc: 0.5000\n",
      "Epoch 1678/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5062 - acc: 1.0000 - val_loss: 2.7805 - val_acc: 0.5000\n",
      "Epoch 1679/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5045 - acc: 1.0000 - val_loss: 2.7351 - val_acc: 0.5000\n",
      "Epoch 1680/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5038 - acc: 1.0000 - val_loss: 2.4962 - val_acc: 0.5000\n",
      "Epoch 1681/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5027 - acc: 1.0000 - val_loss: 2.6718 - val_acc: 0.5000\n",
      "Epoch 1682/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5004 - acc: 1.0000 - val_loss: 2.4298 - val_acc: 0.5000\n",
      "Epoch 1683/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4992 - acc: 1.0000 - val_loss: 2.5419 - val_acc: 0.5000\n",
      "Epoch 1684/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4978 - acc: 1.0000 - val_loss: 2.4499 - val_acc: 0.5000\n",
      "Epoch 1685/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4993 - acc: 1.0000 - val_loss: 2.2733 - val_acc: 0.5000\n",
      "Epoch 1686/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4950 - acc: 1.0000 - val_loss: 2.2216 - val_acc: 0.5000\n",
      "Epoch 1687/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4937 - acc: 1.0000 - val_loss: 1.9637 - val_acc: 0.5000\n",
      "Epoch 1688/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4925 - acc: 1.0000 - val_loss: 2.0107 - val_acc: 0.5000\n",
      "Epoch 1689/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4912 - acc: 1.0000 - val_loss: 1.8263 - val_acc: 0.5000\n",
      "Epoch 1690/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4910 - acc: 1.0000 - val_loss: 1.7258 - val_acc: 0.5000\n",
      "Epoch 1691/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4887 - acc: 1.0000 - val_loss: 1.6226 - val_acc: 0.5000\n",
      "Epoch 1692/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4874 - acc: 1.0000 - val_loss: 1.6289 - val_acc: 0.5000\n",
      "Epoch 1693/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5331 - acc: 0.9850 - val_loss: 1.5558 - val_acc: 0.5000\n",
      "Epoch 1694/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4859 - acc: 1.0000 - val_loss: 2.7826 - val_acc: 0.5000\n",
      "Epoch 1695/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5016 - acc: 0.9950 - val_loss: 2.9739 - val_acc: 0.5000\n",
      "Epoch 1696/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5392 - acc: 0.9800 - val_loss: 3.3104 - val_acc: 0.3700\n",
      "Epoch 1697/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5631 - acc: 0.9750 - val_loss: 3.5148 - val_acc: 0.5000\n",
      "Epoch 1698/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5061 - acc: 0.9950 - val_loss: 5.0350 - val_acc: 0.5000\n",
      "Epoch 1699/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4995 - acc: 0.9950 - val_loss: 5.9916 - val_acc: 0.5000\n",
      "Epoch 1700/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4995 - acc: 1.0000 - val_loss: 5.9901 - val_acc: 0.5000\n",
      "Epoch 1701/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4980 - acc: 1.0000 - val_loss: 5.7158 - val_acc: 0.5000\n",
      "Epoch 1702/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4972 - acc: 1.0000 - val_loss: 5.2444 - val_acc: 0.5000\n",
      "Epoch 1703/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4963 - acc: 1.0000 - val_loss: 5.0278 - val_acc: 0.5000\n",
      "Epoch 1704/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5060 - acc: 0.9950 - val_loss: 4.5009 - val_acc: 0.5000\n",
      "Epoch 1705/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4943 - acc: 1.0000 - val_loss: 4.1954 - val_acc: 0.5000\n",
      "Epoch 1706/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4980 - acc: 1.0000 - val_loss: 3.7625 - val_acc: 0.5000\n",
      "Epoch 1707/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4938 - acc: 1.0000 - val_loss: 3.9083 - val_acc: 0.5000\n",
      "Epoch 1708/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4930 - acc: 1.0000 - val_loss: 3.8680 - val_acc: 0.5000\n",
      "Epoch 1709/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4909 - acc: 1.0000 - val_loss: 3.6110 - val_acc: 0.5000\n",
      "Epoch 1710/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4894 - acc: 1.0000 - val_loss: 3.5846 - val_acc: 0.5000\n",
      "Epoch 1711/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4898 - acc: 1.0000 - val_loss: 3.5817 - val_acc: 0.5000\n",
      "Epoch 1712/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4859 - acc: 1.0000 - val_loss: 3.3246 - val_acc: 0.5000\n",
      "Epoch 1713/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4846 - acc: 1.0000 - val_loss: 3.3566 - val_acc: 0.5000\n",
      "Epoch 1714/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4838 - acc: 1.0000 - val_loss: 3.1525 - val_acc: 0.5000\n",
      "Epoch 1715/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4818 - acc: 1.0000 - val_loss: 3.0976 - val_acc: 0.5000\n",
      "Epoch 1716/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4805 - acc: 1.0000 - val_loss: 3.0778 - val_acc: 0.5000\n",
      "Epoch 1717/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4792 - acc: 1.0000 - val_loss: 2.9809 - val_acc: 0.5000\n",
      "Epoch 1718/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4779 - acc: 1.0000 - val_loss: 2.8876 - val_acc: 0.5000\n",
      "Epoch 1719/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4764 - acc: 1.0000 - val_loss: 3.1106 - val_acc: 0.5000\n",
      "Epoch 1720/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4823 - acc: 0.9950 - val_loss: 2.9775 - val_acc: 0.5000\n",
      "Epoch 1721/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4738 - acc: 1.0000 - val_loss: 2.8328 - val_acc: 0.5000\n",
      "Epoch 1722/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4724 - acc: 1.0000 - val_loss: 2.9590 - val_acc: 0.5000\n",
      "Epoch 1723/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4713 - acc: 1.0000 - val_loss: 2.8785 - val_acc: 0.5000\n",
      "Epoch 1724/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4698 - acc: 1.0000 - val_loss: 2.8937 - val_acc: 0.5000\n",
      "Epoch 1725/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4685 - acc: 1.0000 - val_loss: 3.0277 - val_acc: 0.5000\n",
      "Epoch 1726/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4671 - acc: 1.0000 - val_loss: 2.8454 - val_acc: 0.5000\n",
      "Epoch 1727/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5642 - acc: 0.9850 - val_loss: 3.5415 - val_acc: 0.5000\n",
      "Epoch 1728/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4671 - acc: 1.0000 - val_loss: 9.5779 - val_acc: 0.3700\n",
      "Epoch 1729/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4937 - acc: 0.9950 - val_loss: 9.3459 - val_acc: 0.4200\n",
      "Epoch 1730/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5897 - acc: 0.9750 - val_loss: 8.9458 - val_acc: 0.4400\n",
      "Epoch 1731/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.7329 - acc: 0.9700 - val_loss: 9.5930 - val_acc: 0.3900\n",
      "Epoch 1732/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5403 - acc: 0.9950 - val_loss: 1.4397 - val_acc: 0.6000\n",
      "Epoch 1733/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5772 - acc: 0.9800 - val_loss: 8.6331 - val_acc: 0.4000\n",
      "Epoch 1734/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5097 - acc: 0.9950 - val_loss: 2.4345 - val_acc: 0.6300\n",
      "Epoch 1735/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4987 - acc: 1.0000 - val_loss: 1.4858 - val_acc: 0.8100\n",
      "Epoch 1736/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5003 - acc: 1.0000 - val_loss: 1.7484 - val_acc: 0.7600\n",
      "Epoch 1737/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5101 - acc: 0.9950 - val_loss: 4.3693 - val_acc: 0.5100\n",
      "Epoch 1738/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4998 - acc: 1.0000 - val_loss: 4.0267 - val_acc: 0.5100\n",
      "Epoch 1739/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4987 - acc: 1.0000 - val_loss: 3.7864 - val_acc: 0.5000\n",
      "Epoch 1740/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4977 - acc: 1.0000 - val_loss: 3.3877 - val_acc: 0.5000\n",
      "Epoch 1741/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4966 - acc: 1.0000 - val_loss: 3.2552 - val_acc: 0.5000\n",
      "Epoch 1742/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4955 - acc: 1.0000 - val_loss: 2.7714 - val_acc: 0.5000\n",
      "Epoch 1743/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4941 - acc: 1.0000 - val_loss: 2.6598 - val_acc: 0.5000\n",
      "Epoch 1744/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4945 - acc: 1.0000 - val_loss: 2.2902 - val_acc: 0.5000\n",
      "Epoch 1745/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5263 - acc: 0.9900 - val_loss: 2.3069 - val_acc: 0.5000\n",
      "Epoch 1746/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4906 - acc: 1.0000 - val_loss: 2.5279 - val_acc: 0.5000\n",
      "Epoch 1747/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5045 - acc: 0.9900 - val_loss: 3.4134 - val_acc: 0.5000\n",
      "Epoch 1748/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4927 - acc: 1.0000 - val_loss: 2.4832 - val_acc: 0.5100\n",
      "Epoch 1749/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4944 - acc: 1.0000 - val_loss: 1.8096 - val_acc: 0.5100\n",
      "Epoch 1750/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4936 - acc: 1.0000 - val_loss: 1.8686 - val_acc: 0.5100\n",
      "Epoch 1751/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4928 - acc: 1.0000 - val_loss: 2.0146 - val_acc: 0.5000\n",
      "Epoch 1752/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4912 - acc: 1.0000 - val_loss: 2.2125 - val_acc: 0.5000\n",
      "Epoch 1753/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4906 - acc: 1.0000 - val_loss: 2.2344 - val_acc: 0.5000\n",
      "Epoch 1754/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4911 - acc: 1.0000 - val_loss: 1.9842 - val_acc: 0.5100\n",
      "Epoch 1755/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4877 - acc: 1.0000 - val_loss: 1.9531 - val_acc: 0.5000\n",
      "Epoch 1756/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4864 - acc: 1.0000 - val_loss: 1.9570 - val_acc: 0.5000\n",
      "Epoch 1757/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4854 - acc: 1.0000 - val_loss: 1.9374 - val_acc: 0.5000\n",
      "Epoch 1758/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4837 - acc: 1.0000 - val_loss: 1.9382 - val_acc: 0.5000\n",
      "Epoch 1759/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5059 - acc: 0.9950 - val_loss: 2.0946 - val_acc: 0.5000\n",
      "Epoch 1760/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4824 - acc: 1.0000 - val_loss: 2.5533 - val_acc: 0.5000\n",
      "Epoch 1761/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5868 - acc: 0.9800 - val_loss: 2.4357 - val_acc: 0.5000\n",
      "Epoch 1762/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5139 - acc: 0.9950 - val_loss: 1.9567 - val_acc: 0.5000\n",
      "Epoch 1763/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4809 - acc: 1.0000 - val_loss: 1.8721 - val_acc: 0.5000\n",
      "Epoch 1764/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4804 - acc: 1.0000 - val_loss: 1.8659 - val_acc: 0.5000\n",
      "Epoch 1765/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4797 - acc: 1.0000 - val_loss: 1.7692 - val_acc: 0.5000\n",
      "Epoch 1766/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4788 - acc: 1.0000 - val_loss: 1.7810 - val_acc: 0.5000\n",
      "Epoch 1767/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4775 - acc: 1.0000 - val_loss: 1.7035 - val_acc: 0.5000\n",
      "Epoch 1768/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4766 - acc: 1.0000 - val_loss: 1.7555 - val_acc: 0.5000\n",
      "Epoch 1769/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4752 - acc: 1.0000 - val_loss: 1.6914 - val_acc: 0.5000\n",
      "Epoch 1770/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4742 - acc: 1.0000 - val_loss: 1.7007 - val_acc: 0.5000\n",
      "Epoch 1771/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4728 - acc: 1.0000 - val_loss: 1.7039 - val_acc: 0.5000\n",
      "Epoch 1772/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4716 - acc: 1.0000 - val_loss: 1.6806 - val_acc: 0.5000\n",
      "Epoch 1773/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5326 - acc: 0.9850 - val_loss: 1.5983 - val_acc: 0.5000\n",
      "Epoch 1774/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4695 - acc: 1.0000 - val_loss: 1.4481 - val_acc: 0.5000\n",
      "Epoch 1775/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5069 - acc: 0.9850 - val_loss: 1.7773 - val_acc: 0.5000\n",
      "Epoch 1776/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4768 - acc: 0.9950 - val_loss: 2.1794 - val_acc: 0.5000\n",
      "Epoch 1777/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4727 - acc: 1.0000 - val_loss: 3.3212 - val_acc: 0.1500\n",
      "Epoch 1778/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4951 - acc: 0.9950 - val_loss: 2.2088 - val_acc: 0.5000\n",
      "Epoch 1779/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4746 - acc: 1.0000 - val_loss: 2.2779 - val_acc: 0.5000\n",
      "Epoch 1780/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4743 - acc: 1.0000 - val_loss: 2.2280 - val_acc: 0.5000\n",
      "Epoch 1781/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4736 - acc: 1.0000 - val_loss: 2.2779 - val_acc: 0.5000\n",
      "Epoch 1782/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4726 - acc: 1.0000 - val_loss: 2.2677 - val_acc: 0.5000\n",
      "Epoch 1783/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4717 - acc: 1.0000 - val_loss: 2.3403 - val_acc: 0.5000\n",
      "Epoch 1784/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4704 - acc: 1.0000 - val_loss: 2.3655 - val_acc: 0.5000\n",
      "Epoch 1785/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4697 - acc: 1.0000 - val_loss: 2.3356 - val_acc: 0.5000\n",
      "Epoch 1786/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4682 - acc: 1.0000 - val_loss: 2.4033 - val_acc: 0.5000\n",
      "Epoch 1787/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4669 - acc: 1.0000 - val_loss: 2.3707 - val_acc: 0.5000\n",
      "Epoch 1788/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4660 - acc: 1.0000 - val_loss: 2.3437 - val_acc: 0.5000\n",
      "Epoch 1789/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4648 - acc: 1.0000 - val_loss: 2.2125 - val_acc: 0.5000\n",
      "Epoch 1790/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4635 - acc: 1.0000 - val_loss: 2.3217 - val_acc: 0.5000\n",
      "Epoch 1791/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4625 - acc: 1.0000 - val_loss: 2.2561 - val_acc: 0.5000\n",
      "Epoch 1792/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4613 - acc: 1.0000 - val_loss: 2.2622 - val_acc: 0.5000\n",
      "Epoch 1793/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4601 - acc: 1.0000 - val_loss: 2.2970 - val_acc: 0.5000\n",
      "Epoch 1794/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4590 - acc: 1.0000 - val_loss: 2.1867 - val_acc: 0.5000\n",
      "Epoch 1795/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4580 - acc: 1.0000 - val_loss: 2.3055 - val_acc: 0.5000\n",
      "Epoch 1796/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4577 - acc: 1.0000 - val_loss: 2.2179 - val_acc: 0.5000\n",
      "Epoch 1797/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4646 - acc: 0.9950 - val_loss: 1.9817 - val_acc: 0.5000\n",
      "Epoch 1798/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4547 - acc: 1.0000 - val_loss: 2.0413 - val_acc: 0.5000\n",
      "Epoch 1799/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4540 - acc: 1.0000 - val_loss: 1.7901 - val_acc: 0.5000\n",
      "Epoch 1800/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4531 - acc: 1.0000 - val_loss: 2.0378 - val_acc: 0.5000\n",
      "Epoch 1801/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4931 - acc: 0.9900 - val_loss: 2.0883 - val_acc: 0.5000\n",
      "Epoch 1802/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4514 - acc: 1.0000 - val_loss: 2.5273 - val_acc: 0.5000\n",
      "Epoch 1803/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4530 - acc: 1.0000 - val_loss: 2.6213 - val_acc: 0.5000\n",
      "Epoch 1804/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4541 - acc: 1.0000 - val_loss: 2.7268 - val_acc: 0.4600\n",
      "Epoch 1805/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4498 - acc: 1.0000 - val_loss: 2.4863 - val_acc: 0.4900\n",
      "Epoch 1806/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4518 - acc: 1.0000 - val_loss: 2.5905 - val_acc: 0.5000\n",
      "Epoch 1807/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4495 - acc: 1.0000 - val_loss: 2.5077 - val_acc: 0.4900\n",
      "Epoch 1808/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4466 - acc: 1.0000 - val_loss: 2.6567 - val_acc: 0.5000\n",
      "Epoch 1809/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4458 - acc: 1.0000 - val_loss: 2.5351 - val_acc: 0.5000\n",
      "Epoch 1810/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4463 - acc: 1.0000 - val_loss: 2.5997 - val_acc: 0.5000\n",
      "Epoch 1811/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4435 - acc: 1.0000 - val_loss: 2.6252 - val_acc: 0.5000\n",
      "Epoch 1812/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4424 - acc: 1.0000 - val_loss: 2.5114 - val_acc: 0.5000\n",
      "Epoch 1813/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5129 - acc: 0.9850 - val_loss: 2.3644 - val_acc: 0.5000\n",
      "Epoch 1814/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6207 - acc: 0.9750 - val_loss: 2.3144 - val_acc: 0.5000\n",
      "Epoch 1815/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4848 - acc: 0.9850 - val_loss: 2.2647 - val_acc: 0.5000\n",
      "Epoch 1816/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4830 - acc: 0.9950 - val_loss: 2.7117 - val_acc: 0.5000\n",
      "Epoch 1817/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4516 - acc: 0.9950 - val_loss: 3.2524 - val_acc: 0.5000\n",
      "Epoch 1818/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4460 - acc: 1.0000 - val_loss: 3.3378 - val_acc: 0.5000\n",
      "Epoch 1819/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4469 - acc: 1.0000 - val_loss: 3.4024 - val_acc: 0.5000\n",
      "Epoch 1820/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4457 - acc: 1.0000 - val_loss: 3.2921 - val_acc: 0.5000\n",
      "Epoch 1821/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4447 - acc: 1.0000 - val_loss: 3.3265 - val_acc: 0.5000\n",
      "Epoch 1822/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4434 - acc: 1.0000 - val_loss: 3.2712 - val_acc: 0.5000\n",
      "Epoch 1823/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4423 - acc: 1.0000 - val_loss: 3.3386 - val_acc: 0.5000\n",
      "Epoch 1824/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4512 - acc: 0.9950 - val_loss: 3.3642 - val_acc: 0.5000\n",
      "Epoch 1825/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4422 - acc: 1.0000 - val_loss: 3.4684 - val_acc: 0.5000\n",
      "Epoch 1826/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4418 - acc: 1.0000 - val_loss: 3.5010 - val_acc: 0.5000\n",
      "Epoch 1827/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4391 - acc: 1.0000 - val_loss: 3.3622 - val_acc: 0.5000\n",
      "Epoch 1828/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4557 - acc: 0.9950 - val_loss: 3.4675 - val_acc: 0.5000\n",
      "Epoch 1829/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4406 - acc: 1.0000 - val_loss: 2.5896 - val_acc: 0.5000\n",
      "Epoch 1830/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4500 - acc: 0.9950 - val_loss: 1.8576 - val_acc: 0.5000\n",
      "Epoch 1831/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4365 - acc: 1.0000 - val_loss: 1.7465 - val_acc: 0.5000\n",
      "Epoch 1832/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4360 - acc: 1.0000 - val_loss: 1.7139 - val_acc: 0.5000\n",
      "Epoch 1833/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4394 - acc: 1.0000 - val_loss: 1.6655 - val_acc: 0.5000\n",
      "Epoch 1834/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4346 - acc: 1.0000 - val_loss: 1.6572 - val_acc: 0.5000\n",
      "Epoch 1835/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5827 - acc: 0.9900 - val_loss: 1.7107 - val_acc: 0.5000\n",
      "Epoch 1836/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4345 - acc: 1.0000 - val_loss: 2.4205 - val_acc: 0.5000\n",
      "Epoch 1837/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4399 - acc: 1.0000 - val_loss: 2.6506 - val_acc: 0.5000\n",
      "Epoch 1838/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4433 - acc: 1.0000 - val_loss: 2.5597 - val_acc: 0.5000\n",
      "Epoch 1839/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4450 - acc: 1.0000 - val_loss: 2.5448 - val_acc: 0.5000\n",
      "Epoch 1840/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4447 - acc: 1.0000 - val_loss: 2.3873 - val_acc: 0.5000\n",
      "Epoch 1841/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4444 - acc: 1.0000 - val_loss: 2.3196 - val_acc: 0.5000\n",
      "Epoch 1842/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4468 - acc: 1.0000 - val_loss: 2.2753 - val_acc: 0.5000\n",
      "Epoch 1843/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4430 - acc: 1.0000 - val_loss: 2.2780 - val_acc: 0.5000\n",
      "Epoch 1844/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4416 - acc: 1.0000 - val_loss: 2.3181 - val_acc: 0.5000\n",
      "Epoch 1845/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4406 - acc: 1.0000 - val_loss: 2.2663 - val_acc: 0.5000\n",
      "Epoch 1846/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4396 - acc: 1.0000 - val_loss: 2.3244 - val_acc: 0.5000\n",
      "Epoch 1847/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6058 - acc: 0.9750 - val_loss: 2.3106 - val_acc: 0.5000\n",
      "Epoch 1848/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4380 - acc: 1.0000 - val_loss: 2.3103 - val_acc: 0.5000\n",
      "Epoch 1849/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4402 - acc: 1.0000 - val_loss: 2.3395 - val_acc: 0.5000\n",
      "Epoch 1850/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4386 - acc: 1.0000 - val_loss: 2.2133 - val_acc: 0.5000\n",
      "Epoch 1851/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4383 - acc: 1.0000 - val_loss: 2.3664 - val_acc: 0.5000\n",
      "Epoch 1852/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4380 - acc: 1.0000 - val_loss: 2.2413 - val_acc: 0.5000\n",
      "Epoch 1853/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4824 - acc: 0.9850 - val_loss: 2.3214 - val_acc: 0.5000\n",
      "Epoch 1854/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4359 - acc: 1.0000 - val_loss: 2.2986 - val_acc: 0.5000\n",
      "Epoch 1855/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4377 - acc: 1.0000 - val_loss: 2.3143 - val_acc: 0.5000\n",
      "Epoch 1856/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4410 - acc: 1.0000 - val_loss: 2.3758 - val_acc: 0.5000\n",
      "Epoch 1857/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4381 - acc: 1.0000 - val_loss: 2.1438 - val_acc: 0.5000\n",
      "Epoch 1858/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4467 - acc: 0.9950 - val_loss: 2.0380 - val_acc: 0.5000\n",
      "Epoch 1859/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4323 - acc: 1.0000 - val_loss: 1.8970 - val_acc: 0.5000\n",
      "Epoch 1860/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4314 - acc: 1.0000 - val_loss: 1.9383 - val_acc: 0.5000\n",
      "Epoch 1861/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4306 - acc: 1.0000 - val_loss: 1.7925 - val_acc: 0.5000\n",
      "Epoch 1862/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4296 - acc: 1.0000 - val_loss: 1.9110 - val_acc: 0.5000\n",
      "Epoch 1863/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4286 - acc: 1.0000 - val_loss: 1.9324 - val_acc: 0.5000\n",
      "Epoch 1864/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4277 - acc: 1.0000 - val_loss: 1.7358 - val_acc: 0.5000\n",
      "Epoch 1865/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4266 - acc: 1.0000 - val_loss: 1.8204 - val_acc: 0.5000\n",
      "Epoch 1866/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4260 - acc: 1.0000 - val_loss: 1.7001 - val_acc: 0.5000\n",
      "Epoch 1867/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4247 - acc: 1.0000 - val_loss: 1.7825 - val_acc: 0.5000\n",
      "Epoch 1868/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4236 - acc: 1.0000 - val_loss: 1.6944 - val_acc: 0.5000\n",
      "Epoch 1869/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4960 - acc: 0.9850 - val_loss: 1.7592 - val_acc: 0.5000\n",
      "Epoch 1870/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4225 - acc: 1.0000 - val_loss: 1.8679 - val_acc: 0.5000\n",
      "Epoch 1871/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4575 - acc: 0.9900 - val_loss: 1.9386 - val_acc: 0.5000\n",
      "Epoch 1872/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5496 - acc: 0.9850 - val_loss: 2.4355 - val_acc: 0.5000\n",
      "Epoch 1873/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5970 - acc: 0.9750 - val_loss: 3.1822 - val_acc: 0.5000\n",
      "Epoch 1874/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4256 - acc: 1.0000 - val_loss: 3.3408 - val_acc: 0.5000\n",
      "Epoch 1875/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4412 - acc: 0.9950 - val_loss: 3.2739 - val_acc: 0.5000\n",
      "Epoch 1876/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4455 - acc: 0.9950 - val_loss: 3.8257 - val_acc: 0.5100\n",
      "Epoch 1877/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4298 - acc: 1.0000 - val_loss: 3.1032 - val_acc: 0.5000\n",
      "Epoch 1878/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4291 - acc: 1.0000 - val_loss: 3.6695 - val_acc: 0.5000\n",
      "Epoch 1879/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4285 - acc: 1.0000 - val_loss: 3.4085 - val_acc: 0.5100\n",
      "Epoch 1880/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4278 - acc: 1.0000 - val_loss: 3.4809 - val_acc: 0.5000\n",
      "Epoch 1881/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4270 - acc: 1.0000 - val_loss: 3.3914 - val_acc: 0.5000\n",
      "Epoch 1882/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4259 - acc: 1.0000 - val_loss: 3.1303 - val_acc: 0.5000\n",
      "Epoch 1883/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4247 - acc: 1.0000 - val_loss: 3.2399 - val_acc: 0.5000\n",
      "Epoch 1884/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4290 - acc: 0.9950 - val_loss: 3.2952 - val_acc: 0.5000\n",
      "Epoch 1885/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4224 - acc: 1.0000 - val_loss: 3.9065 - val_acc: 0.5000\n",
      "Epoch 1886/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4372 - acc: 0.9950 - val_loss: 3.8587 - val_acc: 0.5000\n",
      "Epoch 1887/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4226 - acc: 1.0000 - val_loss: 2.7043 - val_acc: 0.5000\n",
      "Epoch 1888/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4469 - acc: 0.9950 - val_loss: 2.7542 - val_acc: 0.5100\n",
      "Epoch 1889/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4234 - acc: 1.0000 - val_loss: 2.6079 - val_acc: 0.5100\n",
      "Epoch 1890/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4332 - acc: 0.9950 - val_loss: 2.4421 - val_acc: 0.5100\n",
      "Epoch 1891/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4234 - acc: 1.0000 - val_loss: 2.8002 - val_acc: 0.5000\n",
      "Epoch 1892/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4199 - acc: 1.0000 - val_loss: 2.8821 - val_acc: 0.5000\n",
      "Epoch 1893/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4186 - acc: 1.0000 - val_loss: 2.8189 - val_acc: 0.5000\n",
      "Epoch 1894/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4182 - acc: 1.0000 - val_loss: 2.9657 - val_acc: 0.5000\n",
      "Epoch 1895/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4164 - acc: 1.0000 - val_loss: 3.0063 - val_acc: 0.5000\n",
      "Epoch 1896/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4154 - acc: 1.0000 - val_loss: 3.0522 - val_acc: 0.5000\n",
      "Epoch 1897/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.6479 - acc: 0.9700 - val_loss: 2.6741 - val_acc: 0.5000\n",
      "Epoch 1898/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4155 - acc: 1.0000 - val_loss: 1.5215 - val_acc: 0.5000\n",
      "Epoch 1899/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4153 - acc: 1.0000 - val_loss: 1.3029 - val_acc: 0.5000\n",
      "Epoch 1900/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4158 - acc: 1.0000 - val_loss: 1.2694 - val_acc: 0.5000\n",
      "Epoch 1901/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4481 - acc: 0.9900 - val_loss: 1.2654 - val_acc: 0.5000\n",
      "Epoch 1902/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4158 - acc: 1.0000 - val_loss: 1.3745 - val_acc: 0.5000\n",
      "Epoch 1903/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4292 - acc: 0.9950 - val_loss: 1.5225 - val_acc: 0.5000\n",
      "Epoch 1904/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4263 - acc: 0.9950 - val_loss: 1.6540 - val_acc: 0.5000\n",
      "Epoch 1905/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4396 - acc: 0.9950 - val_loss: 1.6671 - val_acc: 0.5000\n",
      "Epoch 1906/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4175 - acc: 1.0000 - val_loss: 1.6476 - val_acc: 0.5000\n",
      "Epoch 1907/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4165 - acc: 1.0000 - val_loss: 1.5837 - val_acc: 0.5000\n",
      "Epoch 1908/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4177 - acc: 1.0000 - val_loss: 1.6715 - val_acc: 0.5000\n",
      "Epoch 1909/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4151 - acc: 1.0000 - val_loss: 1.6411 - val_acc: 0.5000\n",
      "Epoch 1910/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4204 - acc: 0.9950 - val_loss: 1.6481 - val_acc: 0.5000\n",
      "Epoch 1911/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4133 - acc: 1.0000 - val_loss: 1.6762 - val_acc: 0.5000\n",
      "Epoch 1912/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4123 - acc: 1.0000 - val_loss: 1.6788 - val_acc: 0.5000\n",
      "Epoch 1913/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4128 - acc: 1.0000 - val_loss: 1.6880 - val_acc: 0.5000\n",
      "Epoch 1914/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4114 - acc: 1.0000 - val_loss: 1.7220 - val_acc: 0.5000\n",
      "Epoch 1915/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4095 - acc: 1.0000 - val_loss: 1.7426 - val_acc: 0.5000\n",
      "Epoch 1916/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4084 - acc: 1.0000 - val_loss: 1.7871 - val_acc: 0.5000\n",
      "Epoch 1917/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4075 - acc: 1.0000 - val_loss: 1.7849 - val_acc: 0.5000\n",
      "Epoch 1918/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4063 - acc: 1.0000 - val_loss: 1.7629 - val_acc: 0.5000\n",
      "Epoch 1919/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4055 - acc: 1.0000 - val_loss: 1.8136 - val_acc: 0.5000\n",
      "Epoch 1920/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4044 - acc: 1.0000 - val_loss: 1.7218 - val_acc: 0.5000\n",
      "Epoch 1921/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4049 - acc: 1.0000 - val_loss: 1.7940 - val_acc: 0.5000\n",
      "Epoch 1922/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4025 - acc: 1.0000 - val_loss: 1.7468 - val_acc: 0.5000\n",
      "Epoch 1923/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4015 - acc: 1.0000 - val_loss: 1.7739 - val_acc: 0.5000\n",
      "Epoch 1924/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4006 - acc: 1.0000 - val_loss: 1.6738 - val_acc: 0.5000\n",
      "Epoch 1925/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4263 - acc: 0.9950 - val_loss: 1.6425 - val_acc: 0.5000\n",
      "Epoch 1926/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3992 - acc: 1.0000 - val_loss: 1.3481 - val_acc: 0.5000\n",
      "Epoch 1927/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5637 - acc: 0.9800 - val_loss: 1.3539 - val_acc: 0.5000\n",
      "Epoch 1928/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4088 - acc: 0.9950 - val_loss: 1.3463 - val_acc: 0.5000\n",
      "Epoch 1929/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4723 - acc: 0.9850 - val_loss: 1.3927 - val_acc: 0.5000\n",
      "Epoch 1930/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3976 - acc: 1.0000 - val_loss: 1.2933 - val_acc: 0.5000\n",
      "Epoch 1931/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3978 - acc: 1.0000 - val_loss: 1.3536 - val_acc: 0.5000\n",
      "Epoch 1932/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3977 - acc: 1.0000 - val_loss: 1.2999 - val_acc: 0.5000\n",
      "Epoch 1933/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3976 - acc: 1.0000 - val_loss: 1.3775 - val_acc: 0.5000\n",
      "Epoch 1934/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3974 - acc: 1.0000 - val_loss: 1.4019 - val_acc: 0.5000\n",
      "Epoch 1935/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3957 - acc: 1.0000 - val_loss: 1.4953 - val_acc: 0.5000\n",
      "Epoch 1936/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3951 - acc: 1.0000 - val_loss: 1.4570 - val_acc: 0.5000\n",
      "Epoch 1937/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3943 - acc: 1.0000 - val_loss: 1.3927 - val_acc: 0.5000\n",
      "Epoch 1938/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4534 - acc: 0.9800 - val_loss: 1.4238 - val_acc: 0.5000\n",
      "Epoch 1939/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3977 - acc: 0.9950 - val_loss: 1.5315 - val_acc: 0.5000\n",
      "Epoch 1940/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4307 - acc: 0.9950 - val_loss: 1.6604 - val_acc: 0.5000\n",
      "Epoch 1941/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3969 - acc: 1.0000 - val_loss: 1.7249 - val_acc: 0.5000\n",
      "Epoch 1942/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3976 - acc: 1.0000 - val_loss: 1.7756 - val_acc: 0.5000\n",
      "Epoch 1943/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3954 - acc: 1.0000 - val_loss: 1.6736 - val_acc: 0.5000\n",
      "Epoch 1944/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3948 - acc: 1.0000 - val_loss: 1.7896 - val_acc: 0.5000\n",
      "Epoch 1945/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3947 - acc: 1.0000 - val_loss: 1.7119 - val_acc: 0.5000\n",
      "Epoch 1946/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3931 - acc: 1.0000 - val_loss: 1.8089 - val_acc: 0.5000\n",
      "Epoch 1947/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3922 - acc: 1.0000 - val_loss: 1.7324 - val_acc: 0.5000\n",
      "Epoch 1948/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3912 - acc: 1.0000 - val_loss: 1.7894 - val_acc: 0.5000\n",
      "Epoch 1949/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3901 - acc: 1.0000 - val_loss: 1.8477 - val_acc: 0.5000\n",
      "Epoch 1950/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3892 - acc: 1.0000 - val_loss: 1.8281 - val_acc: 0.5000\n",
      "Epoch 1951/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3884 - acc: 1.0000 - val_loss: 1.8267 - val_acc: 0.5000\n",
      "Epoch 1952/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3884 - acc: 1.0000 - val_loss: 1.8308 - val_acc: 0.5000\n",
      "Epoch 1953/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3863 - acc: 1.0000 - val_loss: 1.7993 - val_acc: 0.5000\n",
      "Epoch 1954/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3855 - acc: 1.0000 - val_loss: 1.8830 - val_acc: 0.5000\n",
      "Epoch 1955/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3858 - acc: 1.0000 - val_loss: 1.8856 - val_acc: 0.5000\n",
      "Epoch 1956/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3836 - acc: 1.0000 - val_loss: 1.8759 - val_acc: 0.5000\n",
      "Epoch 1957/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3947 - acc: 0.9950 - val_loss: 1.8381 - val_acc: 0.5000\n",
      "Epoch 1958/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3828 - acc: 1.0000 - val_loss: 1.6809 - val_acc: 0.5000\n",
      "Epoch 1959/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4777 - acc: 0.9800 - val_loss: 1.7283 - val_acc: 0.5000\n",
      "Epoch 1960/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3828 - acc: 1.0000 - val_loss: 2.1833 - val_acc: 0.5000\n",
      "Epoch 1961/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3887 - acc: 1.0000 - val_loss: 2.1573 - val_acc: 0.5000\n",
      "Epoch 1962/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3936 - acc: 1.0000 - val_loss: 2.2460 - val_acc: 0.5000\n",
      "Epoch 1963/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4320 - acc: 0.9850 - val_loss: 2.0947 - val_acc: 0.5000\n",
      "Epoch 1964/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3994 - acc: 0.9900 - val_loss: 2.2722 - val_acc: 0.5000\n",
      "Epoch 1965/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3856 - acc: 1.0000 - val_loss: 2.1082 - val_acc: 0.5000\n",
      "Epoch 1966/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3847 - acc: 1.0000 - val_loss: 2.2319 - val_acc: 0.5000\n",
      "Epoch 1967/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3836 - acc: 1.0000 - val_loss: 2.3522 - val_acc: 0.5000\n",
      "Epoch 1968/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3828 - acc: 1.0000 - val_loss: 2.3694 - val_acc: 0.5000\n",
      "Epoch 1969/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3819 - acc: 1.0000 - val_loss: 2.4151 - val_acc: 0.5000\n",
      "Epoch 1970/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3815 - acc: 1.0000 - val_loss: 2.3681 - val_acc: 0.5000\n",
      "Epoch 1971/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3797 - acc: 1.0000 - val_loss: 2.4617 - val_acc: 0.5000\n",
      "Epoch 1972/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3788 - acc: 1.0000 - val_loss: 2.4973 - val_acc: 0.5000\n",
      "Epoch 1973/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3779 - acc: 1.0000 - val_loss: 2.4610 - val_acc: 0.5000\n",
      "Epoch 1974/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3771 - acc: 1.0000 - val_loss: 2.6108 - val_acc: 0.5000\n",
      "Epoch 1975/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3759 - acc: 1.0000 - val_loss: 2.5951 - val_acc: 0.5000\n",
      "Epoch 1976/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4640 - acc: 0.9900 - val_loss: 2.7708 - val_acc: 0.5000\n",
      "Epoch 1977/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3751 - acc: 1.0000 - val_loss: 2.7920 - val_acc: 0.4500\n",
      "Epoch 1978/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4470 - acc: 0.9750 - val_loss: 2.8552 - val_acc: 0.5300\n",
      "Epoch 1979/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3790 - acc: 1.0000 - val_loss: 2.5249 - val_acc: 0.5000\n",
      "Epoch 1980/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3777 - acc: 1.0000 - val_loss: 2.3675 - val_acc: 0.5000\n",
      "Epoch 1981/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3795 - acc: 1.0000 - val_loss: 2.2030 - val_acc: 0.5000\n",
      "Epoch 1982/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3885 - acc: 0.9950 - val_loss: 2.2151 - val_acc: 0.5000\n",
      "Epoch 1983/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3766 - acc: 1.0000 - val_loss: 2.0400 - val_acc: 0.5000\n",
      "Epoch 1984/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3759 - acc: 1.0000 - val_loss: 2.1261 - val_acc: 0.5000\n",
      "Epoch 1985/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3751 - acc: 1.0000 - val_loss: 2.1767 - val_acc: 0.5000\n",
      "Epoch 1986/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3763 - acc: 1.0000 - val_loss: 2.0509 - val_acc: 0.5000\n",
      "Epoch 1987/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3733 - acc: 1.0000 - val_loss: 2.1441 - val_acc: 0.5000\n",
      "Epoch 1988/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3725 - acc: 1.0000 - val_loss: 2.2107 - val_acc: 0.5000\n",
      "Epoch 1989/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3744 - acc: 1.0000 - val_loss: 2.0999 - val_acc: 0.5000\n",
      "Epoch 1990/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3712 - acc: 1.0000 - val_loss: 2.0828 - val_acc: 0.5000\n",
      "Epoch 1991/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3695 - acc: 1.0000 - val_loss: 2.1500 - val_acc: 0.5000\n",
      "Epoch 1992/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3687 - acc: 1.0000 - val_loss: 2.1687 - val_acc: 0.5000\n",
      "Epoch 1993/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3698 - acc: 1.0000 - val_loss: 2.1240 - val_acc: 0.5000\n",
      "Epoch 1994/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3668 - acc: 1.0000 - val_loss: 2.1548 - val_acc: 0.5000\n",
      "Epoch 1995/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3657 - acc: 1.0000 - val_loss: 2.0752 - val_acc: 0.5000\n",
      "Epoch 1996/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3649 - acc: 1.0000 - val_loss: 2.1236 - val_acc: 0.5000\n",
      "Epoch 1997/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3704 - acc: 0.9950 - val_loss: 2.1240 - val_acc: 0.5000\n",
      "Epoch 1998/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3632 - acc: 1.0000 - val_loss: 2.1213 - val_acc: 0.5000\n",
      "Epoch 1999/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3629 - acc: 1.0000 - val_loss: 2.3403 - val_acc: 0.5000\n",
      "Epoch 2000/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3623 - acc: 1.0000 - val_loss: 2.3539 - val_acc: 0.5000\n",
      "Epoch 2001/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3616 - acc: 1.0000 - val_loss: 2.4099 - val_acc: 0.5000\n",
      "Epoch 2002/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3610 - acc: 1.0000 - val_loss: 2.1878 - val_acc: 0.5000\n",
      "Epoch 2003/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3599 - acc: 1.0000 - val_loss: 2.2129 - val_acc: 0.5000\n",
      "Epoch 2004/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3591 - acc: 1.0000 - val_loss: 2.2789 - val_acc: 0.5000\n",
      "Epoch 2005/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3582 - acc: 1.0000 - val_loss: 2.3623 - val_acc: 0.5000\n",
      "Epoch 2006/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3573 - acc: 1.0000 - val_loss: 2.2744 - val_acc: 0.5000\n",
      "Epoch 2007/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3573 - acc: 1.0000 - val_loss: 2.4899 - val_acc: 0.5000\n",
      "Epoch 2008/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3556 - acc: 1.0000 - val_loss: 2.3495 - val_acc: 0.5000\n",
      "Epoch 2009/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3546 - acc: 1.0000 - val_loss: 2.4273 - val_acc: 0.5000\n",
      "Epoch 2010/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3606 - acc: 0.9950 - val_loss: 2.5056 - val_acc: 0.5000\n",
      "Epoch 2011/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3533 - acc: 1.0000 - val_loss: 2.3414 - val_acc: 0.5000\n",
      "Epoch 2012/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3538 - acc: 1.0000 - val_loss: 2.1890 - val_acc: 0.5000\n",
      "Epoch 2013/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3523 - acc: 1.0000 - val_loss: 2.0532 - val_acc: 0.5000\n",
      "Epoch 2014/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3523 - acc: 1.0000 - val_loss: 2.0500 - val_acc: 0.5000\n",
      "Epoch 2015/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3512 - acc: 1.0000 - val_loss: 2.1837 - val_acc: 0.5000\n",
      "Epoch 2016/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3516 - acc: 1.0000 - val_loss: 2.0511 - val_acc: 0.5000\n",
      "Epoch 2017/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3529 - acc: 1.0000 - val_loss: 2.0790 - val_acc: 0.5000\n",
      "Epoch 2018/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3490 - acc: 1.0000 - val_loss: 1.8731 - val_acc: 0.5000\n",
      "Epoch 2019/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3479 - acc: 1.0000 - val_loss: 1.7088 - val_acc: 0.5000\n",
      "Epoch 2020/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3473 - acc: 1.0000 - val_loss: 1.7292 - val_acc: 0.5000\n",
      "Epoch 2021/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3465 - acc: 1.0000 - val_loss: 1.7413 - val_acc: 0.5000\n",
      "Epoch 2022/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3457 - acc: 1.0000 - val_loss: 1.6585 - val_acc: 0.5000\n",
      "Epoch 2023/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3449 - acc: 1.0000 - val_loss: 1.6218 - val_acc: 0.5000\n",
      "Epoch 2024/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3440 - acc: 1.0000 - val_loss: 1.6037 - val_acc: 0.5000\n",
      "Epoch 2025/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3434 - acc: 1.0000 - val_loss: 1.5968 - val_acc: 0.5000\n",
      "Epoch 2026/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3424 - acc: 1.0000 - val_loss: 1.7147 - val_acc: 0.5000\n",
      "Epoch 2027/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3416 - acc: 1.0000 - val_loss: 1.7218 - val_acc: 0.5000\n",
      "Epoch 2028/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3408 - acc: 1.0000 - val_loss: 1.6988 - val_acc: 0.5000\n",
      "Epoch 2029/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3399 - acc: 1.0000 - val_loss: 1.5739 - val_acc: 0.5000\n",
      "Epoch 2030/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3392 - acc: 1.0000 - val_loss: 1.6741 - val_acc: 0.5000\n",
      "Epoch 2031/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3387 - acc: 1.0000 - val_loss: 1.5902 - val_acc: 0.5000\n",
      "Epoch 2032/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3375 - acc: 1.0000 - val_loss: 1.5232 - val_acc: 0.5000\n",
      "Epoch 2033/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3366 - acc: 1.0000 - val_loss: 1.5278 - val_acc: 0.5000\n",
      "Epoch 2034/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3360 - acc: 1.0000 - val_loss: 1.4494 - val_acc: 0.5000\n",
      "Epoch 2035/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4344 - acc: 0.9800 - val_loss: 1.6158 - val_acc: 0.5000\n",
      "Epoch 2036/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3362 - acc: 1.0000 - val_loss: 2.1737 - val_acc: 0.5000\n",
      "Epoch 2037/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3408 - acc: 1.0000 - val_loss: 1.5127 - val_acc: 0.6400\n",
      "Epoch 2038/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3441 - acc: 1.0000 - val_loss: 0.9139 - val_acc: 0.8400\n",
      "Epoch 2039/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4239 - acc: 0.9900 - val_loss: 7.8268 - val_acc: 0.0200\n",
      "Epoch 2040/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5860 - acc: 0.9700 - val_loss: 2.5161 - val_acc: 0.5000\n",
      "Epoch 2041/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3519 - acc: 1.0000 - val_loss: 2.1280 - val_acc: 0.5000\n",
      "Epoch 2042/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3554 - acc: 1.0000 - val_loss: 2.2628 - val_acc: 0.5000\n",
      "Epoch 2043/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3607 - acc: 0.9950 - val_loss: 2.0289 - val_acc: 0.5000\n",
      "Epoch 2044/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3583 - acc: 1.0000 - val_loss: 2.0744 - val_acc: 0.5000\n",
      "Epoch 2045/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3602 - acc: 1.0000 - val_loss: 2.0145 - val_acc: 0.5000\n",
      "Epoch 2046/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3571 - acc: 1.0000 - val_loss: 2.1381 - val_acc: 0.5000\n",
      "Epoch 2047/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3563 - acc: 1.0000 - val_loss: 1.9940 - val_acc: 0.5000\n",
      "Epoch 2048/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3554 - acc: 1.0000 - val_loss: 2.0026 - val_acc: 0.5000\n",
      "Epoch 2049/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3544 - acc: 1.0000 - val_loss: 1.8978 - val_acc: 0.5000\n",
      "Epoch 2050/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3535 - acc: 1.0000 - val_loss: 1.9787 - val_acc: 0.5000\n",
      "Epoch 2051/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3525 - acc: 1.0000 - val_loss: 2.0023 - val_acc: 0.5000\n",
      "Epoch 2052/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3516 - acc: 1.0000 - val_loss: 2.0311 - val_acc: 0.5000\n",
      "Epoch 2053/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3506 - acc: 1.0000 - val_loss: 1.9314 - val_acc: 0.5000\n",
      "Epoch 2054/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3499 - acc: 1.0000 - val_loss: 2.1024 - val_acc: 0.5000\n",
      "Epoch 2055/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3488 - acc: 1.0000 - val_loss: 1.9930 - val_acc: 0.5000\n",
      "Epoch 2056/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3482 - acc: 1.0000 - val_loss: 1.9122 - val_acc: 0.5000\n",
      "Epoch 2057/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3470 - acc: 1.0000 - val_loss: 2.0257 - val_acc: 0.5000\n",
      "Epoch 2058/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3459 - acc: 1.0000 - val_loss: 1.9891 - val_acc: 0.5000\n",
      "Epoch 2059/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3555 - acc: 0.9950 - val_loss: 2.0668 - val_acc: 0.5000\n",
      "Epoch 2060/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3772 - acc: 0.9900 - val_loss: 1.8794 - val_acc: 0.5000\n",
      "Epoch 2061/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4002 - acc: 0.9750 - val_loss: 1.5990 - val_acc: 0.5000\n",
      "Epoch 2062/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3452 - acc: 1.0000 - val_loss: 1.4749 - val_acc: 0.5000\n",
      "Epoch 2063/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3458 - acc: 1.0000 - val_loss: 1.4817 - val_acc: 0.5000\n",
      "Epoch 2064/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3583 - acc: 0.9950 - val_loss: 1.6257 - val_acc: 0.5000\n",
      "Epoch 2065/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3560 - acc: 0.9900 - val_loss: 1.6520 - val_acc: 0.5000\n",
      "Epoch 2066/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3435 - acc: 1.0000 - val_loss: 1.5695 - val_acc: 0.5000\n",
      "Epoch 2067/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3427 - acc: 1.0000 - val_loss: 1.6431 - val_acc: 0.5000\n",
      "Epoch 2068/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3418 - acc: 1.0000 - val_loss: 1.7351 - val_acc: 0.5000\n",
      "Epoch 2069/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3409 - acc: 1.0000 - val_loss: 1.7447 - val_acc: 0.5000\n",
      "Epoch 2070/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3399 - acc: 1.0000 - val_loss: 1.6804 - val_acc: 0.5000\n",
      "Epoch 2071/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3390 - acc: 1.0000 - val_loss: 1.8405 - val_acc: 0.5000\n",
      "Epoch 2072/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3381 - acc: 1.0000 - val_loss: 1.7588 - val_acc: 0.5000\n",
      "Epoch 2073/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3372 - acc: 1.0000 - val_loss: 1.8452 - val_acc: 0.5000\n",
      "Epoch 2074/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3363 - acc: 1.0000 - val_loss: 1.8974 - val_acc: 0.5000\n",
      "Epoch 2075/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3355 - acc: 1.0000 - val_loss: 1.6892 - val_acc: 0.5000\n",
      "Epoch 2076/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3347 - acc: 1.0000 - val_loss: 1.8923 - val_acc: 0.5000\n",
      "Epoch 2077/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3336 - acc: 1.0000 - val_loss: 1.9297 - val_acc: 0.5000\n",
      "Epoch 2078/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3328 - acc: 1.0000 - val_loss: 1.8546 - val_acc: 0.5000\n",
      "Epoch 2079/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3319 - acc: 1.0000 - val_loss: 1.9294 - val_acc: 0.5000\n",
      "Epoch 2080/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3318 - acc: 1.0000 - val_loss: 1.9196 - val_acc: 0.5000\n",
      "Epoch 2081/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3301 - acc: 1.0000 - val_loss: 1.7666 - val_acc: 0.5000\n",
      "Epoch 2082/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3296 - acc: 1.0000 - val_loss: 1.7771 - val_acc: 0.5000\n",
      "Epoch 2083/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3285 - acc: 1.0000 - val_loss: 1.7898 - val_acc: 0.5000\n",
      "Epoch 2084/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3277 - acc: 1.0000 - val_loss: 1.9147 - val_acc: 0.5000\n",
      "Epoch 2085/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3267 - acc: 1.0000 - val_loss: 1.8212 - val_acc: 0.5000\n",
      "Epoch 2086/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3259 - acc: 1.0000 - val_loss: 1.7292 - val_acc: 0.5000\n",
      "Epoch 2087/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3254 - acc: 1.0000 - val_loss: 1.9092 - val_acc: 0.5000\n",
      "Epoch 2088/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3242 - acc: 1.0000 - val_loss: 1.8225 - val_acc: 0.5000\n",
      "Epoch 2089/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3233 - acc: 1.0000 - val_loss: 1.8679 - val_acc: 0.5000\n",
      "Epoch 2090/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3227 - acc: 1.0000 - val_loss: 1.9289 - val_acc: 0.5000\n",
      "Epoch 2091/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3218 - acc: 1.0000 - val_loss: 1.9916 - val_acc: 0.5000\n",
      "Epoch 2092/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3211 - acc: 1.0000 - val_loss: 2.0224 - val_acc: 0.5000\n",
      "Epoch 2093/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3201 - acc: 1.0000 - val_loss: 1.9138 - val_acc: 0.5000\n",
      "Epoch 2094/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3193 - acc: 1.0000 - val_loss: 1.9746 - val_acc: 0.5000\n",
      "Epoch 2095/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3185 - acc: 1.0000 - val_loss: 1.7551 - val_acc: 0.5000\n",
      "Epoch 2096/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3176 - acc: 1.0000 - val_loss: 2.1060 - val_acc: 0.5000\n",
      "Epoch 2097/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3168 - acc: 1.0000 - val_loss: 1.8038 - val_acc: 0.5000\n",
      "Epoch 2098/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3161 - acc: 1.0000 - val_loss: 1.9033 - val_acc: 0.5000\n",
      "Epoch 2099/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3152 - acc: 1.0000 - val_loss: 1.8542 - val_acc: 0.5000\n",
      "Epoch 2100/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3144 - acc: 1.0000 - val_loss: 2.0984 - val_acc: 0.5000\n",
      "Epoch 2101/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3137 - acc: 1.0000 - val_loss: 1.9412 - val_acc: 0.5000\n",
      "Epoch 2102/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3129 - acc: 1.0000 - val_loss: 1.8883 - val_acc: 0.5000\n",
      "Epoch 2103/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3121 - acc: 1.0000 - val_loss: 1.9709 - val_acc: 0.5000\n",
      "Epoch 2104/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3113 - acc: 1.0000 - val_loss: 1.8882 - val_acc: 0.5000\n",
      "Epoch 2105/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3105 - acc: 1.0000 - val_loss: 2.0966 - val_acc: 0.5000\n",
      "Epoch 2106/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3097 - acc: 1.0000 - val_loss: 1.9564 - val_acc: 0.5000\n",
      "Epoch 2107/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3090 - acc: 1.0000 - val_loss: 1.9789 - val_acc: 0.5000\n",
      "Epoch 2108/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4363 - acc: 0.9850 - val_loss: 1.6481 - val_acc: 0.5000\n",
      "Epoch 2109/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3087 - acc: 1.0000 - val_loss: 1.6514 - val_acc: 0.5000\n",
      "Epoch 2110/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3243 - acc: 0.9950 - val_loss: 1.5637 - val_acc: 0.5000\n",
      "Epoch 2111/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3142 - acc: 1.0000 - val_loss: 1.4266 - val_acc: 0.5000\n",
      "Epoch 2112/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3164 - acc: 1.0000 - val_loss: 1.4127 - val_acc: 0.5000\n",
      "Epoch 2113/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3640 - acc: 0.9850 - val_loss: 1.3585 - val_acc: 0.5000\n",
      "Epoch 2114/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3182 - acc: 1.0000 - val_loss: 1.2988 - val_acc: 0.5000\n",
      "Epoch 2115/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3282 - acc: 0.9950 - val_loss: 1.2859 - val_acc: 0.5000\n",
      "Epoch 2116/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3715 - acc: 0.9850 - val_loss: 1.2617 - val_acc: 0.5000\n",
      "Epoch 2117/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3168 - acc: 1.0000 - val_loss: 1.3154 - val_acc: 0.5000\n",
      "Epoch 2118/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3150 - acc: 1.0000 - val_loss: 1.2528 - val_acc: 0.5000\n",
      "Epoch 2119/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3143 - acc: 1.0000 - val_loss: 1.2806 - val_acc: 0.5000\n",
      "Epoch 2120/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3137 - acc: 1.0000 - val_loss: 1.2537 - val_acc: 0.5000\n",
      "Epoch 2121/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3126 - acc: 1.0000 - val_loss: 1.2248 - val_acc: 0.5000\n",
      "Epoch 2122/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3119 - acc: 1.0000 - val_loss: 1.2601 - val_acc: 0.5000\n",
      "Epoch 2123/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3111 - acc: 1.0000 - val_loss: 1.1717 - val_acc: 0.5000\n",
      "Epoch 2124/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3101 - acc: 1.0000 - val_loss: 1.2277 - val_acc: 0.5000\n",
      "Epoch 2125/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3093 - acc: 1.0000 - val_loss: 1.2677 - val_acc: 0.5000\n",
      "Epoch 2126/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3086 - acc: 1.0000 - val_loss: 1.2410 - val_acc: 0.5000\n",
      "Epoch 2127/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3106 - acc: 1.0000 - val_loss: 1.1672 - val_acc: 0.5000\n",
      "Epoch 2128/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3068 - acc: 1.0000 - val_loss: 1.2186 - val_acc: 0.5000\n",
      "Epoch 2129/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3061 - acc: 1.0000 - val_loss: 1.1360 - val_acc: 0.5000\n",
      "Epoch 2130/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3053 - acc: 1.0000 - val_loss: 1.1543 - val_acc: 0.5000\n",
      "Epoch 2131/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3045 - acc: 1.0000 - val_loss: 1.1090 - val_acc: 0.5000\n",
      "Epoch 2132/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3042 - acc: 1.0000 - val_loss: 1.1018 - val_acc: 0.5000\n",
      "Epoch 2133/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3029 - acc: 1.0000 - val_loss: 1.1272 - val_acc: 0.5000\n",
      "Epoch 2134/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3022 - acc: 1.0000 - val_loss: 1.1359 - val_acc: 0.5000\n",
      "Epoch 2135/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3015 - acc: 1.0000 - val_loss: 1.0617 - val_acc: 0.5000\n",
      "Epoch 2136/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3007 - acc: 1.0000 - val_loss: 1.1578 - val_acc: 0.5000\n",
      "Epoch 2137/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3000 - acc: 1.0000 - val_loss: 1.0520 - val_acc: 0.5000\n",
      "Epoch 2138/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2991 - acc: 1.0000 - val_loss: 1.1229 - val_acc: 0.5000\n",
      "Epoch 2139/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2984 - acc: 1.0000 - val_loss: 1.1165 - val_acc: 0.5000\n",
      "Epoch 2140/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2976 - acc: 1.0000 - val_loss: 1.1348 - val_acc: 0.5000\n",
      "Epoch 2141/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2969 - acc: 1.0000 - val_loss: 1.1080 - val_acc: 0.5000\n",
      "Epoch 2142/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2961 - acc: 1.0000 - val_loss: 0.9811 - val_acc: 0.5000\n",
      "Epoch 2143/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2953 - acc: 1.0000 - val_loss: 1.0866 - val_acc: 0.5000\n",
      "Epoch 2144/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2947 - acc: 1.0000 - val_loss: 1.0906 - val_acc: 0.5000\n",
      "Epoch 2145/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3850 - acc: 0.9800 - val_loss: 1.0502 - val_acc: 0.5000\n",
      "Epoch 2146/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2966 - acc: 1.0000 - val_loss: 0.6721 - val_acc: 0.6700\n",
      "Epoch 2147/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5070 - acc: 0.9700 - val_loss: 0.5424 - val_acc: 0.8500\n",
      "Epoch 2148/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3884 - acc: 0.9850 - val_loss: 2.1871 - val_acc: 0.5500\n",
      "Epoch 2149/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3364 - acc: 1.0000 - val_loss: 9.0177 - val_acc: 0.2400\n",
      "Epoch 2150/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3466 - acc: 1.0000 - val_loss: 9.2622 - val_acc: 0.2600\n",
      "Epoch 2151/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3517 - acc: 1.0000 - val_loss: 9.1995 - val_acc: 0.2000\n",
      "Epoch 2152/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3544 - acc: 1.0000 - val_loss: 9.4048 - val_acc: 0.2300\n",
      "Epoch 2153/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3599 - acc: 1.0000 - val_loss: 6.4968 - val_acc: 0.2400\n",
      "Epoch 2154/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3542 - acc: 1.0000 - val_loss: 2.1119 - val_acc: 0.5200\n",
      "Epoch 2155/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3538 - acc: 1.0000 - val_loss: 3.6598 - val_acc: 0.1800\n",
      "Epoch 2156/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4244 - acc: 0.9800 - val_loss: 1.9272 - val_acc: 0.5000\n",
      "Epoch 2157/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4028 - acc: 0.9850 - val_loss: 3.9799 - val_acc: 0.5000\n",
      "Epoch 2158/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3612 - acc: 1.0000 - val_loss: 4.5842 - val_acc: 0.5000\n",
      "Epoch 2159/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3669 - acc: 1.0000 - val_loss: 4.6171 - val_acc: 0.5000\n",
      "Epoch 2160/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3701 - acc: 1.0000 - val_loss: 4.6296 - val_acc: 0.5000\n",
      "Epoch 2161/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3713 - acc: 1.0000 - val_loss: 4.1625 - val_acc: 0.5000\n",
      "Epoch 2162/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3719 - acc: 1.0000 - val_loss: 3.3669 - val_acc: 0.5000\n",
      "Epoch 2163/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3714 - acc: 1.0000 - val_loss: 2.7108 - val_acc: 0.5000\n",
      "Epoch 2164/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3700 - acc: 1.0000 - val_loss: 2.6466 - val_acc: 0.5000\n",
      "Epoch 2165/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3693 - acc: 1.0000 - val_loss: 2.4013 - val_acc: 0.5000\n",
      "Epoch 2166/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3682 - acc: 1.0000 - val_loss: 1.9906 - val_acc: 0.5000\n",
      "Epoch 2167/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3680 - acc: 1.0000 - val_loss: 1.8059 - val_acc: 0.5000\n",
      "Epoch 2168/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3664 - acc: 1.0000 - val_loss: 1.8341 - val_acc: 0.5000\n",
      "Epoch 2169/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3654 - acc: 1.0000 - val_loss: 1.6260 - val_acc: 0.5000\n",
      "Epoch 2170/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3645 - acc: 1.0000 - val_loss: 1.6705 - val_acc: 0.5000\n",
      "Epoch 2171/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3635 - acc: 1.0000 - val_loss: 1.5879 - val_acc: 0.5000\n",
      "Epoch 2172/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3636 - acc: 1.0000 - val_loss: 1.4614 - val_acc: 0.5000\n",
      "Epoch 2173/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4108 - acc: 0.9900 - val_loss: 1.3529 - val_acc: 0.5000\n",
      "Epoch 2174/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3634 - acc: 1.0000 - val_loss: 1.2431 - val_acc: 0.5000\n",
      "Epoch 2175/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5170 - acc: 0.9800 - val_loss: 1.2768 - val_acc: 0.5000\n",
      "Epoch 2176/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3613 - acc: 1.0000 - val_loss: 1.9183 - val_acc: 0.5000\n",
      "Epoch 2177/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3645 - acc: 1.0000 - val_loss: 2.0013 - val_acc: 0.5000\n",
      "Epoch 2178/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4681 - acc: 0.9900 - val_loss: 1.9231 - val_acc: 0.5000\n",
      "Epoch 2179/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3663 - acc: 1.0000 - val_loss: 1.9027 - val_acc: 0.5000\n",
      "Epoch 2180/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3818 - acc: 0.9950 - val_loss: 1.9908 - val_acc: 0.5100\n",
      "Epoch 2181/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3648 - acc: 1.0000 - val_loss: 2.3939 - val_acc: 0.5200\n",
      "Epoch 2182/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4109 - acc: 0.9850 - val_loss: 2.4440 - val_acc: 0.5300\n",
      "Epoch 2183/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3648 - acc: 1.0000 - val_loss: 2.2045 - val_acc: 0.5400\n",
      "Epoch 2184/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3655 - acc: 1.0000 - val_loss: 2.4407 - val_acc: 0.5400\n",
      "Epoch 2185/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3640 - acc: 1.0000 - val_loss: 2.5945 - val_acc: 0.5100\n",
      "Epoch 2186/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3618 - acc: 1.0000 - val_loss: 2.4454 - val_acc: 0.5300\n",
      "Epoch 2187/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3607 - acc: 1.0000 - val_loss: 2.4622 - val_acc: 0.5300\n",
      "Epoch 2188/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3600 - acc: 1.0000 - val_loss: 2.5514 - val_acc: 0.5200\n",
      "Epoch 2189/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3593 - acc: 1.0000 - val_loss: 2.8763 - val_acc: 0.5100\n",
      "Epoch 2190/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3589 - acc: 1.0000 - val_loss: 2.9041 - val_acc: 0.5100\n",
      "Epoch 2191/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3571 - acc: 1.0000 - val_loss: 2.9694 - val_acc: 0.5100\n",
      "Epoch 2192/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3565 - acc: 1.0000 - val_loss: 2.9600 - val_acc: 0.5100\n",
      "Epoch 2193/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3583 - acc: 1.0000 - val_loss: 2.8472 - val_acc: 0.5100\n",
      "Epoch 2194/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3543 - acc: 1.0000 - val_loss: 2.8579 - val_acc: 0.5100\n",
      "Epoch 2195/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3545 - acc: 1.0000 - val_loss: 2.7655 - val_acc: 0.5100\n",
      "Epoch 2196/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3525 - acc: 1.0000 - val_loss: 3.0022 - val_acc: 0.5100\n",
      "Epoch 2197/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3524 - acc: 1.0000 - val_loss: 3.0688 - val_acc: 0.5100\n",
      "Epoch 2198/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3509 - acc: 1.0000 - val_loss: 2.8804 - val_acc: 0.5100\n",
      "Epoch 2199/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3500 - acc: 1.0000 - val_loss: 2.7892 - val_acc: 0.5100\n",
      "Epoch 2200/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3490 - acc: 1.0000 - val_loss: 2.7182 - val_acc: 0.5100\n",
      "Epoch 2201/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3489 - acc: 1.0000 - val_loss: 2.9737 - val_acc: 0.5100\n",
      "Epoch 2202/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3472 - acc: 1.0000 - val_loss: 2.7963 - val_acc: 0.5100\n",
      "Epoch 2203/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3464 - acc: 1.0000 - val_loss: 2.9562 - val_acc: 0.5100\n",
      "Epoch 2204/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3460 - acc: 1.0000 - val_loss: 2.7208 - val_acc: 0.5100\n",
      "Epoch 2205/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3812 - acc: 0.9900 - val_loss: 3.0189 - val_acc: 0.5100\n",
      "Epoch 2206/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3447 - acc: 1.0000 - val_loss: 3.0549 - val_acc: 0.5600\n",
      "Epoch 2207/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4735 - acc: 0.9850 - val_loss: 3.7929 - val_acc: 0.4900\n",
      "Epoch 2208/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3973 - acc: 0.9950 - val_loss: 3.3251 - val_acc: 0.5000\n",
      "Epoch 2209/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3468 - acc: 1.0000 - val_loss: 3.1915 - val_acc: 0.5000\n",
      "Epoch 2210/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3479 - acc: 1.0000 - val_loss: 3.0120 - val_acc: 0.5000\n",
      "Epoch 2211/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3463 - acc: 1.0000 - val_loss: 2.8433 - val_acc: 0.5000\n",
      "Epoch 2212/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3505 - acc: 0.9950 - val_loss: 2.6008 - val_acc: 0.5000\n",
      "Epoch 2213/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3449 - acc: 1.0000 - val_loss: 2.6065 - val_acc: 0.5000\n",
      "Epoch 2214/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3441 - acc: 1.0000 - val_loss: 2.4496 - val_acc: 0.5000\n",
      "Epoch 2215/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3434 - acc: 1.0000 - val_loss: 2.1975 - val_acc: 0.5000\n",
      "Epoch 2216/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3425 - acc: 1.0000 - val_loss: 2.2359 - val_acc: 0.5000\n",
      "Epoch 2217/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3416 - acc: 1.0000 - val_loss: 2.2258 - val_acc: 0.5000\n",
      "Epoch 2218/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3408 - acc: 1.0000 - val_loss: 2.0916 - val_acc: 0.5000\n",
      "Epoch 2219/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3400 - acc: 1.0000 - val_loss: 1.9918 - val_acc: 0.5000\n",
      "Epoch 2220/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3391 - acc: 1.0000 - val_loss: 1.9588 - val_acc: 0.5000\n",
      "Epoch 2221/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3445 - acc: 0.9950 - val_loss: 1.8471 - val_acc: 0.5000\n",
      "Epoch 2222/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3376 - acc: 1.0000 - val_loss: 1.7103 - val_acc: 0.5000\n",
      "Epoch 2223/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3367 - acc: 1.0000 - val_loss: 1.5795 - val_acc: 0.5000\n",
      "Epoch 2224/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3364 - acc: 1.0000 - val_loss: 1.5876 - val_acc: 0.5000\n",
      "Epoch 2225/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3352 - acc: 1.0000 - val_loss: 1.5733 - val_acc: 0.5000\n",
      "Epoch 2226/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3345 - acc: 1.0000 - val_loss: 1.5414 - val_acc: 0.5000\n",
      "Epoch 2227/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3339 - acc: 1.0000 - val_loss: 1.5369 - val_acc: 0.5000\n",
      "Epoch 2228/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3334 - acc: 1.0000 - val_loss: 1.5703 - val_acc: 0.5000\n",
      "Epoch 2229/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3318 - acc: 1.0000 - val_loss: 1.5696 - val_acc: 0.5000\n",
      "Epoch 2230/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3310 - acc: 1.0000 - val_loss: 1.6365 - val_acc: 0.5000\n",
      "Epoch 2231/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3302 - acc: 1.0000 - val_loss: 1.5688 - val_acc: 0.5000\n",
      "Epoch 2232/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3295 - acc: 1.0000 - val_loss: 1.5444 - val_acc: 0.5000\n",
      "Epoch 2233/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3313 - acc: 1.0000 - val_loss: 1.4665 - val_acc: 0.5000\n",
      "Epoch 2234/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3281 - acc: 1.0000 - val_loss: 1.6768 - val_acc: 0.5000\n",
      "Epoch 2235/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3271 - acc: 1.0000 - val_loss: 1.7448 - val_acc: 0.5000\n",
      "Epoch 2236/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3266 - acc: 1.0000 - val_loss: 1.6918 - val_acc: 0.5000\n",
      "Epoch 2237/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3256 - acc: 1.0000 - val_loss: 1.7569 - val_acc: 0.5000\n",
      "Epoch 2238/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3248 - acc: 1.0000 - val_loss: 1.7936 - val_acc: 0.5000\n",
      "Epoch 2239/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3239 - acc: 1.0000 - val_loss: 1.8341 - val_acc: 0.5000\n",
      "Epoch 2240/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3232 - acc: 1.0000 - val_loss: 1.7222 - val_acc: 0.5000\n",
      "Epoch 2241/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3225 - acc: 1.0000 - val_loss: 1.8504 - val_acc: 0.5000\n",
      "Epoch 2242/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3220 - acc: 1.0000 - val_loss: 1.7674 - val_acc: 0.5000\n",
      "Epoch 2243/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3208 - acc: 1.0000 - val_loss: 1.9395 - val_acc: 0.5000\n",
      "Epoch 2244/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3202 - acc: 1.0000 - val_loss: 1.8592 - val_acc: 0.5000\n",
      "Epoch 2245/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3193 - acc: 1.0000 - val_loss: 1.8802 - val_acc: 0.5000\n",
      "Epoch 2246/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3186 - acc: 1.0000 - val_loss: 1.8710 - val_acc: 0.5000\n",
      "Epoch 2247/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3178 - acc: 1.0000 - val_loss: 1.8457 - val_acc: 0.5000\n",
      "Epoch 2248/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3171 - acc: 1.0000 - val_loss: 1.8773 - val_acc: 0.5000\n",
      "Epoch 2249/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3166 - acc: 1.0000 - val_loss: 1.8975 - val_acc: 0.5000\n",
      "Epoch 2250/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3155 - acc: 1.0000 - val_loss: 1.7976 - val_acc: 0.5000\n",
      "Epoch 2251/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3149 - acc: 1.0000 - val_loss: 1.8773 - val_acc: 0.5000\n",
      "Epoch 2252/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4810 - acc: 0.9900 - val_loss: 1.8854 - val_acc: 0.5000\n",
      "Epoch 2253/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3134 - acc: 1.0000 - val_loss: 1.7690 - val_acc: 0.5000\n",
      "Epoch 2254/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3128 - acc: 1.0000 - val_loss: 1.7257 - val_acc: 0.5000\n",
      "Epoch 2255/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3974 - acc: 0.9900 - val_loss: 1.7633 - val_acc: 0.5000\n",
      "Epoch 2256/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3168 - acc: 0.9950 - val_loss: 2.0488 - val_acc: 0.5000\n",
      "Epoch 2257/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3109 - acc: 1.0000 - val_loss: 2.2384 - val_acc: 0.5000\n",
      "Epoch 2258/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3218 - acc: 0.9950 - val_loss: 2.1744 - val_acc: 0.5000\n",
      "Epoch 2259/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3106 - acc: 1.0000 - val_loss: 2.0263 - val_acc: 0.5000\n",
      "Epoch 2260/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3090 - acc: 1.0000 - val_loss: 1.9608 - val_acc: 0.5000\n",
      "Epoch 2261/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3086 - acc: 1.0000 - val_loss: 1.7214 - val_acc: 0.5000\n",
      "Epoch 2262/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3153 - acc: 0.9950 - val_loss: 1.7853 - val_acc: 0.5000\n",
      "Epoch 2263/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3068 - acc: 1.0000 - val_loss: 1.9273 - val_acc: 0.5000\n",
      "Epoch 2264/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3077 - acc: 1.0000 - val_loss: 1.9306 - val_acc: 0.5000\n",
      "Epoch 2265/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3115 - acc: 1.0000 - val_loss: 2.1625 - val_acc: 0.5000\n",
      "Epoch 2266/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3062 - acc: 1.0000 - val_loss: 2.2028 - val_acc: 0.5000\n",
      "Epoch 2267/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3042 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 0.5000\n",
      "Epoch 2268/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3034 - acc: 1.0000 - val_loss: 2.2166 - val_acc: 0.5000\n",
      "Epoch 2269/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3029 - acc: 1.0000 - val_loss: 2.3922 - val_acc: 0.5000\n",
      "Epoch 2270/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3024 - acc: 1.0000 - val_loss: 2.4243 - val_acc: 0.5000\n",
      "Epoch 2271/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3014 - acc: 1.0000 - val_loss: 2.3517 - val_acc: 0.5000\n",
      "Epoch 2272/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3006 - acc: 1.0000 - val_loss: 2.3236 - val_acc: 0.5000\n",
      "Epoch 2273/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2999 - acc: 1.0000 - val_loss: 2.2800 - val_acc: 0.5000\n",
      "Epoch 2274/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2994 - acc: 1.0000 - val_loss: 2.2370 - val_acc: 0.5000\n",
      "Epoch 2275/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2985 - acc: 1.0000 - val_loss: 2.2678 - val_acc: 0.5000\n",
      "Epoch 2276/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2979 - acc: 1.0000 - val_loss: 2.2191 - val_acc: 0.5000\n",
      "Epoch 2277/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2971 - acc: 1.0000 - val_loss: 2.3603 - val_acc: 0.5000\n",
      "Epoch 2278/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2966 - acc: 1.0000 - val_loss: 2.1127 - val_acc: 0.5000\n",
      "Epoch 2279/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2960 - acc: 1.0000 - val_loss: 2.2639 - val_acc: 0.5000\n",
      "Epoch 2280/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2951 - acc: 1.0000 - val_loss: 2.2398 - val_acc: 0.5000\n",
      "Epoch 2281/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2979 - acc: 1.0000 - val_loss: 2.2411 - val_acc: 0.5000\n",
      "Epoch 2282/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2938 - acc: 1.0000 - val_loss: 2.2592 - val_acc: 0.5000\n",
      "Epoch 2283/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2935 - acc: 1.0000 - val_loss: 2.4110 - val_acc: 0.5000\n",
      "Epoch 2284/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2925 - acc: 1.0000 - val_loss: 2.2371 - val_acc: 0.5000\n",
      "Epoch 2285/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2919 - acc: 1.0000 - val_loss: 2.2389 - val_acc: 0.5000\n",
      "Epoch 2286/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2914 - acc: 1.0000 - val_loss: 2.3553 - val_acc: 0.5000\n",
      "Epoch 2287/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2907 - acc: 1.0000 - val_loss: 2.2665 - val_acc: 0.5000\n",
      "Epoch 2288/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2962 - acc: 0.9950 - val_loss: 2.3029 - val_acc: 0.5000\n",
      "Epoch 2289/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2893 - acc: 1.0000 - val_loss: 2.1121 - val_acc: 0.5000\n",
      "Epoch 2290/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2887 - acc: 1.0000 - val_loss: 2.2253 - val_acc: 0.5000\n",
      "Epoch 2291/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2918 - acc: 0.9950 - val_loss: 2.2428 - val_acc: 0.5000\n",
      "Epoch 2292/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2875 - acc: 1.0000 - val_loss: 2.4125 - val_acc: 0.5000\n",
      "Epoch 2293/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2871 - acc: 1.0000 - val_loss: 2.7233 - val_acc: 0.5000\n",
      "Epoch 2294/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2866 - acc: 1.0000 - val_loss: 2.7078 - val_acc: 0.5000\n",
      "Epoch 2295/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2867 - acc: 1.0000 - val_loss: 2.6428 - val_acc: 0.5000\n",
      "Epoch 2296/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2855 - acc: 1.0000 - val_loss: 2.5426 - val_acc: 0.5000\n",
      "Epoch 2297/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2848 - acc: 1.0000 - val_loss: 2.5511 - val_acc: 0.5000\n",
      "Epoch 2298/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2842 - acc: 1.0000 - val_loss: 2.5333 - val_acc: 0.5000\n",
      "Epoch 2299/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2836 - acc: 1.0000 - val_loss: 2.4709 - val_acc: 0.5000\n",
      "Epoch 2300/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2830 - acc: 1.0000 - val_loss: 2.5880 - val_acc: 0.5000\n",
      "Epoch 2301/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2823 - acc: 1.0000 - val_loss: 2.5215 - val_acc: 0.5000\n",
      "Epoch 2302/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2833 - acc: 1.0000 - val_loss: 2.5044 - val_acc: 0.5000\n",
      "Epoch 2303/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2811 - acc: 1.0000 - val_loss: 2.3760 - val_acc: 0.5000\n",
      "Epoch 2304/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2805 - acc: 1.0000 - val_loss: 2.3945 - val_acc: 0.5000\n",
      "Epoch 2305/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2799 - acc: 1.0000 - val_loss: 2.3441 - val_acc: 0.5000\n",
      "Epoch 2306/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2793 - acc: 1.0000 - val_loss: 2.5541 - val_acc: 0.5000\n",
      "Epoch 2307/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2787 - acc: 1.0000 - val_loss: 2.3735 - val_acc: 0.5000\n",
      "Epoch 2308/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2781 - acc: 1.0000 - val_loss: 2.3777 - val_acc: 0.5000\n",
      "Epoch 2309/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2775 - acc: 1.0000 - val_loss: 2.4835 - val_acc: 0.5000\n",
      "Epoch 2310/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2769 - acc: 1.0000 - val_loss: 2.4070 - val_acc: 0.5000\n",
      "Epoch 2311/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2765 - acc: 1.0000 - val_loss: 2.4360 - val_acc: 0.5000\n",
      "Epoch 2312/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2757 - acc: 1.0000 - val_loss: 2.3712 - val_acc: 0.5000\n",
      "Epoch 2313/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2751 - acc: 1.0000 - val_loss: 2.3377 - val_acc: 0.5000\n",
      "Epoch 2314/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2745 - acc: 1.0000 - val_loss: 2.3729 - val_acc: 0.5000\n",
      "Epoch 2315/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2739 - acc: 1.0000 - val_loss: 2.5526 - val_acc: 0.5000\n",
      "Epoch 2316/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2746 - acc: 1.0000 - val_loss: 2.4396 - val_acc: 0.5000\n",
      "Epoch 2317/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2727 - acc: 1.0000 - val_loss: 2.2003 - val_acc: 0.5000\n",
      "Epoch 2318/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2722 - acc: 1.0000 - val_loss: 2.3672 - val_acc: 0.5000\n",
      "Epoch 2319/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2716 - acc: 1.0000 - val_loss: 2.3645 - val_acc: 0.5000\n",
      "Epoch 2320/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2710 - acc: 1.0000 - val_loss: 2.3789 - val_acc: 0.5000\n",
      "Epoch 2321/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2830 - acc: 0.9950 - val_loss: 2.1025 - val_acc: 0.5000\n",
      "Epoch 2322/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2699 - acc: 1.0000 - val_loss: 2.2298 - val_acc: 0.5000\n",
      "Epoch 2323/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2846 - acc: 0.9900 - val_loss: 2.3619 - val_acc: 0.5000\n",
      "Epoch 2324/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2690 - acc: 1.0000 - val_loss: 2.0239 - val_acc: 0.5000\n",
      "Epoch 2325/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2688 - acc: 1.0000 - val_loss: 1.9066 - val_acc: 0.5000\n",
      "Epoch 2326/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2685 - acc: 1.0000 - val_loss: 2.0359 - val_acc: 0.5000\n",
      "Epoch 2327/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2681 - acc: 1.0000 - val_loss: 2.1285 - val_acc: 0.5000\n",
      "Epoch 2328/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2792 - acc: 0.9950 - val_loss: 1.9920 - val_acc: 0.5000\n",
      "Epoch 2329/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2671 - acc: 1.0000 - val_loss: 1.7158 - val_acc: 0.5000\n",
      "Epoch 2330/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5035 - acc: 0.9750 - val_loss: 0.9696 - val_acc: 0.5000\n",
      "Epoch 2331/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2672 - acc: 1.0000 - val_loss: 0.9461 - val_acc: 0.5000\n",
      "Epoch 2332/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3061 - acc: 0.9950 - val_loss: 0.8640 - val_acc: 0.5000\n",
      "Epoch 2333/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2743 - acc: 1.0000 - val_loss: 0.4779 - val_acc: 0.9300\n",
      "Epoch 2334/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2757 - acc: 1.0000 - val_loss: 0.6870 - val_acc: 0.5500\n",
      "Epoch 2335/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4165 - acc: 0.9900 - val_loss: 0.8748 - val_acc: 0.5000\n",
      "Epoch 2336/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2800 - acc: 1.0000 - val_loss: 1.3017 - val_acc: 0.8100\n",
      "Epoch 2337/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3243 - acc: 0.9950 - val_loss: 1.4636 - val_acc: 0.5000\n",
      "Epoch 2338/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2941 - acc: 1.0000 - val_loss: 1.0758 - val_acc: 0.5000\n",
      "Epoch 2339/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2978 - acc: 1.0000 - val_loss: 1.0615 - val_acc: 0.5000\n",
      "Epoch 2340/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2982 - acc: 1.0000 - val_loss: 1.0078 - val_acc: 0.5000\n",
      "Epoch 2341/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2985 - acc: 1.0000 - val_loss: 0.9605 - val_acc: 0.5000\n",
      "Epoch 2342/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4846 - acc: 0.9750 - val_loss: 0.9779 - val_acc: 0.5000\n",
      "Epoch 2343/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2993 - acc: 1.0000 - val_loss: 1.1529 - val_acc: 0.5000\n",
      "Epoch 2344/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3044 - acc: 1.0000 - val_loss: 0.8325 - val_acc: 0.6000\n",
      "Epoch 2345/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4692 - acc: 0.9850 - val_loss: 0.7879 - val_acc: 0.6300\n",
      "Epoch 2346/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4475 - acc: 0.9900 - val_loss: 1.1013 - val_acc: 0.5000\n",
      "Epoch 2347/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3113 - acc: 1.0000 - val_loss: 1.0797 - val_acc: 0.5000\n",
      "Epoch 2348/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4219 - acc: 0.9800 - val_loss: 1.0362 - val_acc: 0.5000\n",
      "Epoch 2349/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3154 - acc: 1.0000 - val_loss: 1.0898 - val_acc: 0.5200\n",
      "Epoch 2350/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3187 - acc: 1.0000 - val_loss: 1.4095 - val_acc: 0.5000\n",
      "Epoch 2351/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3373 - acc: 0.9900 - val_loss: 1.2339 - val_acc: 0.5100\n",
      "Epoch 2352/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4108 - acc: 0.9900 - val_loss: 1.1414 - val_acc: 0.5700\n",
      "Epoch 2353/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4126 - acc: 0.9900 - val_loss: 1.7351 - val_acc: 0.5000\n",
      "Epoch 2354/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3243 - acc: 1.0000 - val_loss: 1.7152 - val_acc: 0.5000\n",
      "Epoch 2355/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3265 - acc: 1.0000 - val_loss: 1.7654 - val_acc: 0.5100\n",
      "Epoch 2356/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3459 - acc: 0.9900 - val_loss: 1.9227 - val_acc: 0.5100\n",
      "Epoch 2357/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4588 - acc: 0.9700 - val_loss: 1.7424 - val_acc: 0.5100\n",
      "Epoch 2358/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3277 - acc: 1.0000 - val_loss: 1.8772 - val_acc: 0.5200\n",
      "Epoch 2359/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3294 - acc: 1.0000 - val_loss: 1.9194 - val_acc: 0.5000\n",
      "Epoch 2360/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3297 - acc: 1.0000 - val_loss: 1.7883 - val_acc: 0.5000\n",
      "Epoch 2361/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3298 - acc: 1.0000 - val_loss: 1.9007 - val_acc: 0.5000\n",
      "Epoch 2362/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3293 - acc: 1.0000 - val_loss: 1.9548 - val_acc: 0.5000\n",
      "Epoch 2363/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3354 - acc: 0.9950 - val_loss: 1.9915 - val_acc: 0.5000\n",
      "Epoch 2364/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3278 - acc: 1.0000 - val_loss: 1.8247 - val_acc: 0.5000\n",
      "Epoch 2365/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3270 - acc: 1.0000 - val_loss: 1.8194 - val_acc: 0.5000\n",
      "Epoch 2366/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3368 - acc: 0.9950 - val_loss: 1.7217 - val_acc: 0.5000\n",
      "Epoch 2367/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3319 - acc: 0.9950 - val_loss: 1.9184 - val_acc: 0.5000\n",
      "Epoch 2368/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3325 - acc: 0.9950 - val_loss: 1.8918 - val_acc: 0.5000\n",
      "Epoch 2369/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3236 - acc: 1.0000 - val_loss: 1.8746 - val_acc: 0.5000\n",
      "Epoch 2370/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3232 - acc: 1.0000 - val_loss: 2.0140 - val_acc: 0.5000\n",
      "Epoch 2371/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3224 - acc: 1.0000 - val_loss: 2.1386 - val_acc: 0.5000\n",
      "Epoch 2372/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3974 - acc: 0.9800 - val_loss: 2.0271 - val_acc: 0.5000\n",
      "Epoch 2373/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3204 - acc: 1.0000 - val_loss: 2.1674 - val_acc: 0.5000\n",
      "Epoch 2374/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3197 - acc: 1.0000 - val_loss: 2.1387 - val_acc: 0.5000\n",
      "Epoch 2375/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3191 - acc: 1.0000 - val_loss: 2.0762 - val_acc: 0.5000\n",
      "Epoch 2376/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3201 - acc: 1.0000 - val_loss: 2.1685 - val_acc: 0.5000\n",
      "Epoch 2377/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3179 - acc: 1.0000 - val_loss: 2.3179 - val_acc: 0.5000\n",
      "Epoch 2378/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3164 - acc: 1.0000 - val_loss: 2.4865 - val_acc: 0.5000\n",
      "Epoch 2379/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3159 - acc: 1.0000 - val_loss: 2.1643 - val_acc: 0.5000\n",
      "Epoch 2380/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3148 - acc: 1.0000 - val_loss: 2.1958 - val_acc: 0.5000\n",
      "Epoch 2381/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3146 - acc: 1.0000 - val_loss: 2.3493 - val_acc: 0.5000\n",
      "Epoch 2382/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3184 - acc: 0.9950 - val_loss: 2.5042 - val_acc: 0.5000\n",
      "Epoch 2383/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3346 - acc: 0.9900 - val_loss: 2.2927 - val_acc: 0.5000\n",
      "Epoch 2384/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3115 - acc: 1.0000 - val_loss: 1.7849 - val_acc: 0.5000\n",
      "Epoch 2385/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3115 - acc: 1.0000 - val_loss: 1.6337 - val_acc: 0.5000\n",
      "Epoch 2386/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3111 - acc: 1.0000 - val_loss: 1.8354 - val_acc: 0.5000\n",
      "Epoch 2387/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3184 - acc: 0.9950 - val_loss: 1.7295 - val_acc: 0.5000\n",
      "Epoch 2388/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3129 - acc: 1.0000 - val_loss: 1.5915 - val_acc: 0.5000\n",
      "Epoch 2389/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3089 - acc: 1.0000 - val_loss: 1.7727 - val_acc: 0.5000\n",
      "Epoch 2390/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3139 - acc: 0.9950 - val_loss: 1.6287 - val_acc: 0.5000\n",
      "Epoch 2391/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3082 - acc: 1.0000 - val_loss: 1.4505 - val_acc: 0.5000\n",
      "Epoch 2392/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3059 - acc: 1.0000 - val_loss: 1.6354 - val_acc: 0.5000\n",
      "Epoch 2393/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3067 - acc: 1.0000 - val_loss: 1.3628 - val_acc: 0.5000\n",
      "Epoch 2394/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3045 - acc: 1.0000 - val_loss: 1.3817 - val_acc: 0.5000\n",
      "Epoch 2395/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3031 - acc: 1.0000 - val_loss: 1.5080 - val_acc: 0.5000\n",
      "Epoch 2396/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3689 - acc: 0.9850 - val_loss: 1.1924 - val_acc: 0.5000\n",
      "Epoch 2397/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3026 - acc: 1.0000 - val_loss: 1.2998 - val_acc: 0.5000\n",
      "Epoch 2398/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3828 - acc: 0.9900 - val_loss: 0.9071 - val_acc: 0.5500\n",
      "Epoch 2399/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3057 - acc: 1.0000 - val_loss: 0.6975 - val_acc: 0.6200\n",
      "Epoch 2400/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3065 - acc: 1.0000 - val_loss: 0.6132 - val_acc: 0.7000\n",
      "Epoch 2401/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3077 - acc: 1.0000 - val_loss: 0.6813 - val_acc: 0.6600\n",
      "Epoch 2402/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3072 - acc: 1.0000 - val_loss: 0.7908 - val_acc: 0.5800\n",
      "Epoch 2403/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3101 - acc: 1.0000 - val_loss: 1.2426 - val_acc: 0.5000\n",
      "Epoch 2404/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3090 - acc: 1.0000 - val_loss: 1.0659 - val_acc: 0.5200\n",
      "Epoch 2405/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3047 - acc: 1.0000 - val_loss: 1.2595 - val_acc: 0.5200\n",
      "Epoch 2406/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3039 - acc: 1.0000 - val_loss: 1.4484 - val_acc: 0.5000\n",
      "Epoch 2407/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3032 - acc: 1.0000 - val_loss: 1.7002 - val_acc: 0.5000\n",
      "Epoch 2408/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3024 - acc: 1.0000 - val_loss: 1.8425 - val_acc: 0.5000\n",
      "Epoch 2409/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3018 - acc: 1.0000 - val_loss: 1.8908 - val_acc: 0.5000\n",
      "Epoch 2410/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3008 - acc: 1.0000 - val_loss: 2.4249 - val_acc: 0.5000\n",
      "Epoch 2411/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3001 - acc: 1.0000 - val_loss: 2.3348 - val_acc: 0.5000\n",
      "Epoch 2412/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2993 - acc: 1.0000 - val_loss: 2.9898 - val_acc: 0.5000\n",
      "Epoch 2413/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3004 - acc: 1.0000 - val_loss: 2.8866 - val_acc: 0.5000\n",
      "Epoch 2414/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2978 - acc: 1.0000 - val_loss: 2.8851 - val_acc: 0.5000\n",
      "Epoch 2415/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2972 - acc: 1.0000 - val_loss: 3.2699 - val_acc: 0.5000\n",
      "Epoch 2416/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2964 - acc: 1.0000 - val_loss: 3.2722 - val_acc: 0.5000\n",
      "Epoch 2417/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2956 - acc: 1.0000 - val_loss: 3.4995 - val_acc: 0.5000\n",
      "Epoch 2418/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2954 - acc: 1.0000 - val_loss: 3.6248 - val_acc: 0.5000\n",
      "Epoch 2419/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3196 - acc: 0.9900 - val_loss: 3.7229 - val_acc: 0.5000\n",
      "Epoch 2420/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2939 - acc: 1.0000 - val_loss: 3.2462 - val_acc: 0.5000\n",
      "Epoch 2421/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2946 - acc: 1.0000 - val_loss: 3.2968 - val_acc: 0.5000\n",
      "Epoch 2422/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2954 - acc: 1.0000 - val_loss: 2.9026 - val_acc: 0.5000\n",
      "Epoch 2423/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2946 - acc: 1.0000 - val_loss: 3.0613 - val_acc: 0.5000\n",
      "Epoch 2424/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2946 - acc: 1.0000 - val_loss: 2.9048 - val_acc: 0.5000\n",
      "Epoch 2425/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2985 - acc: 0.9950 - val_loss: 3.1107 - val_acc: 0.5000\n",
      "Epoch 2426/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3103 - acc: 0.9900 - val_loss: 3.2300 - val_acc: 0.5000\n",
      "Epoch 2427/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2997 - acc: 0.9950 - val_loss: 3.5576 - val_acc: 0.5000\n",
      "Epoch 2428/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3392 - acc: 0.9900 - val_loss: 3.3135 - val_acc: 0.5000\n",
      "Epoch 2429/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2931 - acc: 1.0000 - val_loss: 4.3157 - val_acc: 0.5000\n",
      "Epoch 2430/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3149 - acc: 0.9950 - val_loss: 4.6160 - val_acc: 0.5000\n",
      "Epoch 2431/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3066 - acc: 0.9950 - val_loss: 4.6858 - val_acc: 0.5000\n",
      "Epoch 2432/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2924 - acc: 1.0000 - val_loss: 4.4869 - val_acc: 0.5000\n",
      "Epoch 2433/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2910 - acc: 1.0000 - val_loss: 4.3991 - val_acc: 0.5000\n",
      "Epoch 2434/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2905 - acc: 1.0000 - val_loss: 4.5187 - val_acc: 0.5000\n",
      "Epoch 2435/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2896 - acc: 1.0000 - val_loss: 4.4839 - val_acc: 0.5000\n",
      "Epoch 2436/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2945 - acc: 0.9950 - val_loss: 4.5139 - val_acc: 0.5000\n",
      "Epoch 2437/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3538 - acc: 0.9800 - val_loss: 4.4679 - val_acc: 0.5000\n",
      "Epoch 2438/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3011 - acc: 0.9950 - val_loss: 3.9619 - val_acc: 0.5000\n",
      "Epoch 2439/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3137 - acc: 0.9950 - val_loss: 3.5358 - val_acc: 0.4800\n",
      "Epoch 2440/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2975 - acc: 0.9900 - val_loss: 3.4543 - val_acc: 0.5000\n",
      "Epoch 2441/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2885 - acc: 1.0000 - val_loss: 3.4890 - val_acc: 0.5000\n",
      "Epoch 2442/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2870 - acc: 1.0000 - val_loss: 3.4726 - val_acc: 0.5000\n",
      "Epoch 2443/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2864 - acc: 1.0000 - val_loss: 3.5330 - val_acc: 0.5000\n",
      "Epoch 2444/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2858 - acc: 1.0000 - val_loss: 3.3425 - val_acc: 0.5000\n",
      "Epoch 2445/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2851 - acc: 1.0000 - val_loss: 3.4445 - val_acc: 0.5000\n",
      "Epoch 2446/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2844 - acc: 1.0000 - val_loss: 3.9414 - val_acc: 0.5000\n",
      "Epoch 2447/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2837 - acc: 1.0000 - val_loss: 3.8174 - val_acc: 0.5000\n",
      "Epoch 2448/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3853 - acc: 0.9850 - val_loss: 3.8842 - val_acc: 0.5000\n",
      "Epoch 2449/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2831 - acc: 1.0000 - val_loss: 4.2361 - val_acc: 0.5000\n",
      "Epoch 2450/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2856 - acc: 1.0000 - val_loss: 4.0425 - val_acc: 0.5000\n",
      "Epoch 2451/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2907 - acc: 0.9950 - val_loss: 4.4021 - val_acc: 0.5000\n",
      "Epoch 2452/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2860 - acc: 1.0000 - val_loss: 4.9010 - val_acc: 0.5000\n",
      "Epoch 2453/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2859 - acc: 1.0000 - val_loss: 4.3704 - val_acc: 0.5000\n",
      "Epoch 2454/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2873 - acc: 1.0000 - val_loss: 4.7946 - val_acc: 0.5000\n",
      "Epoch 2455/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2849 - acc: 1.0000 - val_loss: 4.6656 - val_acc: 0.5000\n",
      "Epoch 2456/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2848 - acc: 1.0000 - val_loss: 5.1011 - val_acc: 0.5000\n",
      "Epoch 2457/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2833 - acc: 1.0000 - val_loss: 4.8575 - val_acc: 0.5000\n",
      "Epoch 2458/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2833 - acc: 1.0000 - val_loss: 4.9362 - val_acc: 0.5000\n",
      "Epoch 2459/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2820 - acc: 1.0000 - val_loss: 5.1890 - val_acc: 0.5000\n",
      "Epoch 2460/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2815 - acc: 1.0000 - val_loss: 5.4253 - val_acc: 0.5000\n",
      "Epoch 2461/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2805 - acc: 1.0000 - val_loss: 5.2019 - val_acc: 0.5000\n",
      "Epoch 2462/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2821 - acc: 1.0000 - val_loss: 5.1428 - val_acc: 0.5000\n",
      "Epoch 2463/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2791 - acc: 1.0000 - val_loss: 5.3438 - val_acc: 0.5000\n",
      "Epoch 2464/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2785 - acc: 1.0000 - val_loss: 5.6733 - val_acc: 0.5000\n",
      "Epoch 2465/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2816 - acc: 1.0000 - val_loss: 5.4181 - val_acc: 0.5000\n",
      "Epoch 2466/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2779 - acc: 1.0000 - val_loss: 5.4638 - val_acc: 0.5000\n",
      "Epoch 2467/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2768 - acc: 1.0000 - val_loss: 5.3547 - val_acc: 0.5000\n",
      "Epoch 2468/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2760 - acc: 1.0000 - val_loss: 5.2184 - val_acc: 0.5000\n",
      "Epoch 2469/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2755 - acc: 1.0000 - val_loss: 5.0918 - val_acc: 0.5000\n",
      "Epoch 2470/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2749 - acc: 1.0000 - val_loss: 5.0332 - val_acc: 0.5000\n",
      "Epoch 2471/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2742 - acc: 1.0000 - val_loss: 5.0570 - val_acc: 0.5000\n",
      "Epoch 2472/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2734 - acc: 1.0000 - val_loss: 5.1443 - val_acc: 0.5000\n",
      "Epoch 2473/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2729 - acc: 1.0000 - val_loss: 4.7518 - val_acc: 0.5000\n",
      "Epoch 2474/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2722 - acc: 1.0000 - val_loss: 5.0576 - val_acc: 0.5000\n",
      "Epoch 2475/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2718 - acc: 1.0000 - val_loss: 5.1714 - val_acc: 0.5000\n",
      "Epoch 2476/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2708 - acc: 1.0000 - val_loss: 4.7552 - val_acc: 0.5000\n",
      "Epoch 2477/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2701 - acc: 1.0000 - val_loss: 4.9834 - val_acc: 0.5000\n",
      "Epoch 2478/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2696 - acc: 1.0000 - val_loss: 4.8814 - val_acc: 0.5000\n",
      "Epoch 2479/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2698 - acc: 1.0000 - val_loss: 4.8851 - val_acc: 0.5000\n",
      "Epoch 2480/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2681 - acc: 1.0000 - val_loss: 4.8851 - val_acc: 0.5000\n",
      "Epoch 2481/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2675 - acc: 1.0000 - val_loss: 4.8862 - val_acc: 0.5000\n",
      "Epoch 2482/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2669 - acc: 1.0000 - val_loss: 4.9319 - val_acc: 0.5000\n",
      "Epoch 2483/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2677 - acc: 1.0000 - val_loss: 4.9802 - val_acc: 0.5000\n",
      "Epoch 2484/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2673 - acc: 1.0000 - val_loss: 5.0007 - val_acc: 0.5000\n",
      "Epoch 2485/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2650 - acc: 1.0000 - val_loss: 4.7950 - val_acc: 0.5000\n",
      "Epoch 2486/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2644 - acc: 1.0000 - val_loss: 4.9747 - val_acc: 0.5000\n",
      "Epoch 2487/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2720 - acc: 0.9950 - val_loss: 4.7644 - val_acc: 0.5000\n",
      "Epoch 2488/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2633 - acc: 1.0000 - val_loss: 4.3521 - val_acc: 0.5000\n",
      "Epoch 2489/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2632 - acc: 1.0000 - val_loss: 4.3674 - val_acc: 0.5000\n",
      "Epoch 2490/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2627 - acc: 1.0000 - val_loss: 3.8413 - val_acc: 0.5000\n",
      "Epoch 2491/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2627 - acc: 1.0000 - val_loss: 4.0817 - val_acc: 0.5000\n",
      "Epoch 2492/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2622 - acc: 1.0000 - val_loss: 3.6258 - val_acc: 0.5100\n",
      "Epoch 2493/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2612 - acc: 1.0000 - val_loss: 3.6565 - val_acc: 0.5100\n",
      "Epoch 2494/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2607 - acc: 1.0000 - val_loss: 3.8530 - val_acc: 0.5000\n",
      "Epoch 2495/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2600 - acc: 1.0000 - val_loss: 3.5815 - val_acc: 0.5000\n",
      "Epoch 2496/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2604 - acc: 1.0000 - val_loss: 4.3248 - val_acc: 0.5000\n",
      "Epoch 2497/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2596 - acc: 1.0000 - val_loss: 4.4592 - val_acc: 0.5000\n",
      "Epoch 2498/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2582 - acc: 1.0000 - val_loss: 3.6288 - val_acc: 0.5000\n",
      "Epoch 2499/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2590 - acc: 1.0000 - val_loss: 4.2032 - val_acc: 0.5000\n",
      "Epoch 2500/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2570 - acc: 1.0000 - val_loss: 4.4262 - val_acc: 0.5000\n",
      "Epoch 2501/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2565 - acc: 1.0000 - val_loss: 4.1006 - val_acc: 0.5000\n",
      "Epoch 2502/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2559 - acc: 1.0000 - val_loss: 4.0433 - val_acc: 0.5000\n",
      "Epoch 2503/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2553 - acc: 1.0000 - val_loss: 3.5440 - val_acc: 0.5000\n",
      "Epoch 2504/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2547 - acc: 1.0000 - val_loss: 4.0919 - val_acc: 0.5000\n",
      "Epoch 2505/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2542 - acc: 1.0000 - val_loss: 4.0488 - val_acc: 0.5000\n",
      "Epoch 2506/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2536 - acc: 1.0000 - val_loss: 3.7859 - val_acc: 0.5000\n",
      "Epoch 2507/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2535 - acc: 1.0000 - val_loss: 3.7040 - val_acc: 0.5000\n",
      "Epoch 2508/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2524 - acc: 1.0000 - val_loss: 4.1937 - val_acc: 0.5000\n",
      "Epoch 2509/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2519 - acc: 1.0000 - val_loss: 3.8953 - val_acc: 0.5000\n",
      "Epoch 2510/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2513 - acc: 1.0000 - val_loss: 4.1658 - val_acc: 0.5000\n",
      "Epoch 2511/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2512 - acc: 1.0000 - val_loss: 3.4256 - val_acc: 0.5000\n",
      "Epoch 2512/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2506 - acc: 1.0000 - val_loss: 3.8670 - val_acc: 0.5000\n",
      "Epoch 2513/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2497 - acc: 1.0000 - val_loss: 3.4941 - val_acc: 0.5000\n",
      "Epoch 2514/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2495 - acc: 1.0000 - val_loss: 4.0055 - val_acc: 0.5000\n",
      "Epoch 2515/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2486 - acc: 1.0000 - val_loss: 3.8818 - val_acc: 0.5000\n",
      "Epoch 2516/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2481 - acc: 1.0000 - val_loss: 3.6635 - val_acc: 0.5000\n",
      "Epoch 2517/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2475 - acc: 1.0000 - val_loss: 3.4408 - val_acc: 0.5000\n",
      "Epoch 2518/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2470 - acc: 1.0000 - val_loss: 3.0659 - val_acc: 0.5000\n",
      "Epoch 2519/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2648 - acc: 0.9950 - val_loss: 4.2727 - val_acc: 0.5000\n",
      "Epoch 2520/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2462 - acc: 1.0000 - val_loss: 4.9942 - val_acc: 0.5000\n",
      "Epoch 2521/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2728 - acc: 0.9950 - val_loss: 4.8958 - val_acc: 0.5000\n",
      "Epoch 2522/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2477 - acc: 1.0000 - val_loss: 5.9971 - val_acc: 0.5000\n",
      "Epoch 2523/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3003 - acc: 0.9900 - val_loss: 5.1394 - val_acc: 0.5000\n",
      "Epoch 2524/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2495 - acc: 1.0000 - val_loss: 4.9152 - val_acc: 0.5000\n",
      "Epoch 2525/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2503 - acc: 1.0000 - val_loss: 5.3342 - val_acc: 0.5000\n",
      "Epoch 2526/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2504 - acc: 1.0000 - val_loss: 5.3197 - val_acc: 0.5000\n",
      "Epoch 2527/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2502 - acc: 1.0000 - val_loss: 4.8727 - val_acc: 0.5000\n",
      "Epoch 2528/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2528 - acc: 1.0000 - val_loss: 4.8479 - val_acc: 0.5000\n",
      "Epoch 2529/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2495 - acc: 1.0000 - val_loss: 4.5665 - val_acc: 0.5000\n",
      "Epoch 2530/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2488 - acc: 1.0000 - val_loss: 4.6231 - val_acc: 0.5000\n",
      "Epoch 2531/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2483 - acc: 1.0000 - val_loss: 4.2584 - val_acc: 0.5000\n",
      "Epoch 2532/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2476 - acc: 1.0000 - val_loss: 4.2797 - val_acc: 0.5000\n",
      "Epoch 2533/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2470 - acc: 1.0000 - val_loss: 3.9678 - val_acc: 0.5000\n",
      "Epoch 2534/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2467 - acc: 1.0000 - val_loss: 4.1816 - val_acc: 0.5000\n",
      "Epoch 2535/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2460 - acc: 1.0000 - val_loss: 3.9481 - val_acc: 0.5000\n",
      "Epoch 2536/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2453 - acc: 1.0000 - val_loss: 3.6801 - val_acc: 0.5000\n",
      "Epoch 2537/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2447 - acc: 1.0000 - val_loss: 3.7735 - val_acc: 0.5000\n",
      "Epoch 2538/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2915 - acc: 0.9900 - val_loss: 3.7421 - val_acc: 0.5000\n",
      "Epoch 2539/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2444 - acc: 1.0000 - val_loss: 3.1268 - val_acc: 0.5000\n",
      "Epoch 2540/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2447 - acc: 1.0000 - val_loss: 1.6552 - val_acc: 0.7300\n",
      "Epoch 2541/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2451 - acc: 1.0000 - val_loss: 1.4088 - val_acc: 0.7300\n",
      "Epoch 2542/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2641 - acc: 0.9950 - val_loss: 3.0644 - val_acc: 0.5000\n",
      "Epoch 2543/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2451 - acc: 1.0000 - val_loss: 3.1996 - val_acc: 0.5000\n",
      "Epoch 2544/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2447 - acc: 1.0000 - val_loss: 3.0939 - val_acc: 0.5000\n",
      "Epoch 2545/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3268 - acc: 0.9850 - val_loss: 2.7634 - val_acc: 0.5000\n",
      "Epoch 2546/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3548 - acc: 0.9900 - val_loss: 1.4863 - val_acc: 0.5000\n",
      "Epoch 2547/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4046 - acc: 0.9850 - val_loss: 1.9530 - val_acc: 0.5000\n",
      "Epoch 2548/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2590 - acc: 1.0000 - val_loss: 3.1278 - val_acc: 0.5000\n",
      "Epoch 2549/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2683 - acc: 1.0000 - val_loss: 3.4181 - val_acc: 0.5000\n",
      "Epoch 2550/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2720 - acc: 1.0000 - val_loss: 3.3920 - val_acc: 0.5000\n",
      "Epoch 2551/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2765 - acc: 1.0000 - val_loss: 3.1402 - val_acc: 0.5000\n",
      "Epoch 2552/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2746 - acc: 1.0000 - val_loss: 3.0279 - val_acc: 0.5000\n",
      "Epoch 2553/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2745 - acc: 1.0000 - val_loss: 3.2258 - val_acc: 0.5000\n",
      "Epoch 2554/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2740 - acc: 1.0000 - val_loss: 2.9934 - val_acc: 0.5000\n",
      "Epoch 2555/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2734 - acc: 1.0000 - val_loss: 2.7877 - val_acc: 0.5000\n",
      "Epoch 2556/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2727 - acc: 1.0000 - val_loss: 3.1038 - val_acc: 0.5000\n",
      "Epoch 2557/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2719 - acc: 1.0000 - val_loss: 2.8565 - val_acc: 0.5000\n",
      "Epoch 2558/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2713 - acc: 1.0000 - val_loss: 2.8782 - val_acc: 0.5000\n",
      "Epoch 2559/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2705 - acc: 1.0000 - val_loss: 2.9987 - val_acc: 0.5000\n",
      "Epoch 2560/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2700 - acc: 1.0000 - val_loss: 2.3635 - val_acc: 0.5000\n",
      "Epoch 2561/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2689 - acc: 1.0000 - val_loss: 2.8096 - val_acc: 0.5000\n",
      "Epoch 2562/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2684 - acc: 1.0000 - val_loss: 2.6722 - val_acc: 0.5000\n",
      "Epoch 2563/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2674 - acc: 1.0000 - val_loss: 2.5260 - val_acc: 0.5000\n",
      "Epoch 2564/3000\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2666 - acc: 1.0000 - val_loss: 2.6785 - val_acc: 0.5000\n",
      "Epoch 2565/3000\n",
      " 64/200 [========>.....................] - ETA: 0s - loss: 0.2661 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-d27419bf0708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_r\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the discriminator\n",
    "d_loss = model.fit([X_l, X_r],labels,validation_split=0.33,nb_epoch=3000,verbose=1,shuffle=True)\n",
    "\n",
    "print( d_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDX-7pYMDEGH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIeezj5mDEbw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "stzybbTL-fr0",
    "outputId": "4bd7dcde-2f9b-4382-e02c-8eb9e8a99070"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA1lAV7a4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(d_loss.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(d_loss.history['acc'])\n",
    "plt.plot(d_loss.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(d_loss.history['loss'])\n",
    "plt.plot(d_loss.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "pXyPUUhR1qHo",
    "outputId": "46e8242e-de1e-42d2-c43f-99b4cf7eb601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1 \n",
      "\n",
      "1 2 \n",
      "\n",
      "1 2 3 \n",
      "\n",
      "1 2 3 4 \n",
      "\n",
      "1 2 3 4 5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,7):\n",
    "    j=1\n",
    "    while(j<i):\n",
    "      print(j,end =\" \")\n",
    "      j+=1\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KTgMUYKE-hdH",
    "outputId": "3b682b24-c4d3-4f44-dbbf-7999b015d420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n"
     ]
    }
   ],
   "source": [
    "print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rh6pFLpJ_dsf"
   },
   "outputs": [],
   "source": [
    "p=[1.0,2.0,3.0]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nHzsjlIdiy5D",
    "outputId": "a445eab9-c0cc-4126-9654-da6f5db2c386"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000.0"
      ]
     },
     "execution_count": 258,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-T4034teizlp",
    "outputId": "19dc3c4f-f3a1-4722-f0db-61e6ea8a6a5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 287,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xW0q0vr6ynYR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.06"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 10.3\n",
    "c= a/5\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, int(c)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_NORM = 1\n",
    "    model.add(Conv2D(8, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', input_shape=(128,94,1), name='block1_conv1'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(8, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block1_conv2'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block2_conv1'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block2_conv2'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block3_conv1'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block3_conv2'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block3_conv3'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block3_conv4'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block4_conv1'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block4_conv2'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block4_conv3'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block4_conv4'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), kernel_regularizer=regularizers.l2(0.01),padding='same', name='block5_conv1'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', name='block5_conv2'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', name='block5_conv3'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', name='block5_conv4'))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(2048))\n",
    "    model.add(BatchNormalization()) if BATCH_NORM else None\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(2048, name='fc2'))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SiameseVGGish.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
